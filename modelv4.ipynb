{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, _pickle as pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "ckpt_dir = 'checkpoint'\n",
    "word_embd_dir = 'checkpoint/word_embd'\n",
    "model_dir = 'checkpoint/modelv4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embd_dim = 100\n",
    "pos_embd_dim = 25\n",
    "dep_embd_dim = 25\n",
    "word_vocab_size = 400001\n",
    "pos_vocab_size = 10\n",
    "dep_vocab_size = 21\n",
    "relation_classes = 19\n",
    "word_state_size = 100\n",
    "other_state_size = 100\n",
    "batch_size = 10\n",
    "channels = 3\n",
    "lambda_l2 = 0.0001\n",
    "max_len_path = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"input\"):\n",
    "    path_length = tf.placeholder(tf.int32, shape=[2, batch_size], name=\"path1_length\")\n",
    "    word_ids = tf.placeholder(tf.int32, shape=[2, batch_size, max_len_path], name=\"word_ids\")\n",
    "    pos_ids = tf.placeholder(tf.int32, [2, batch_size, max_len_path], name=\"pos_ids\")\n",
    "    dep_ids = tf.placeholder(tf.int32, [2, batch_size, max_len_path], name=\"dep_ids\")\n",
    "    y = tf.placeholder(tf.int32, [batch_size], name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"word_embedding\"):\n",
    "    W = tf.Variable(tf.constant(0.0, shape=[word_vocab_size, word_embd_dim]), name=\"W\")\n",
    "    embedding_placeholder = tf.placeholder(tf.float32,[word_vocab_size, word_embd_dim])\n",
    "    embedding_init = W.assign(embedding_placeholder)\n",
    "    embedded_word = tf.nn.embedding_lookup(W, word_ids)\n",
    "    word_embedding_saver = tf.train.Saver({\"word_embedding/W\": W})\n",
    "\n",
    "with tf.name_scope(\"pos_embedding\"):\n",
    "    W = tf.Variable(tf.random_uniform([pos_vocab_size, pos_embd_dim]), name=\"W\")\n",
    "    embedded_pos = tf.nn.embedding_lookup(W, pos_ids)\n",
    "    pos_embedding_saver = tf.train.Saver({\"pos_embedding/W\": W})\n",
    "\n",
    "with tf.name_scope(\"dep_embedding\"):\n",
    "    W = tf.Variable(tf.random_uniform([dep_vocab_size, dep_embd_dim]), name=\"W\")\n",
    "    embedded_dep = tf.nn.embedding_lookup(W, dep_ids)\n",
    "    dep_embedding_saver = tf.train.Saver({\"dep_embedding/W\": W})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dropout\"):\n",
    "    h_drop = tf.nn.dropout(embedded_word, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_hidden_state = tf.zeros([batch_size, word_state_size], name='word_hidden_state')\n",
    "word_cell_state = tf.zeros([batch_size, word_state_size], name='word_cell_state')\n",
    "word_init_state = tf.contrib.rnn.LSTMStateTuple(word_hidden_state, word_cell_state)\n",
    "\n",
    "other_hidden_states = tf.zeros([channels-1, batch_size, other_state_size], name=\"hidden_state\")\n",
    "other_cell_states = tf.zeros([channels-1, batch_size, other_state_size], name=\"cell_state\")\n",
    "\n",
    "other_init_states = [tf.contrib.rnn.LSTMStateTuple(other_hidden_states[i], other_cell_states[i]) for i in range(channels-1)]\n",
    "\n",
    "with tf.variable_scope(\"word_lstm1\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(word_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_word[0], sequence_length=path_length[0], initial_state=word_init_state)\n",
    "    state_series_word1 = tf.reduce_max(state_series, axis=1)\n",
    "\n",
    "with tf.variable_scope(\"word_lstm2\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(word_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_word[1], sequence_length=path_length[1], initial_state=word_init_state)\n",
    "    state_series_word2 = tf.reduce_max(state_series, axis=1)\n",
    "\n",
    "with tf.variable_scope(\"pos_lstm1\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_pos[0], sequence_length=path_length[0],initial_state=other_init_states[0])\n",
    "    state_series_pos1 = tf.reduce_max(state_series, axis=1)\n",
    "\n",
    "with tf.variable_scope(\"pos_lstm2\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_pos[1], sequence_length=path_length[1],initial_state=other_init_states[0])\n",
    "    state_series_pos2 = tf.reduce_max(state_series, axis=1)\n",
    "\n",
    "with tf.variable_scope(\"dep_lstm1\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_dep[0], sequence_length=path_length[0], initial_state=other_init_states[1])\n",
    "    state_series_dep1 = tf.reduce_max(state_series, axis=1)\n",
    "\n",
    "with tf.variable_scope(\"dep_lstm2\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_dep[1], sequence_length=path_length[1], initial_state=other_init_states[1])\n",
    "    state_series_dep2 = tf.reduce_max(state_series, axis=1)\n",
    "\n",
    "state_series1 = tf.concat([state_series_word1, state_series_pos1, state_series_dep1], 1)\n",
    "state_series2 = tf.concat([state_series_word2, state_series_pos2, state_series_dep2], 1)\n",
    "\n",
    "state_series = tf.concat([state_series1, state_series2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"hidden_layer\"):\n",
    "    W = tf.Variable(tf.truncated_normal([600, 100], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([100]), name=\"b\")\n",
    "    y_hidden_layer = tf.matmul(state_series, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dropout\"):\n",
    "    h_drop = tf.nn.dropout(y_hidden_layer, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"softmax_layer\"):\n",
    "    W = tf.Variable(tf.truncated_normal([100, relation_classes], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([relation_classes]), name=\"b\")\n",
    "    logits = tf.matmul(y_hidden_layer, W) + b\n",
    "    predictions = tf.argmax(logits, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tv_all = tf.trainable_variables()\n",
    "tv_regu = []\n",
    "non_reg = [\"word_embedding/W:0\",\"pos_embedding/W:0\",'dep_embedding/W:0',\"global_step:0\",'hidden_layer/b:0','softmax_layer/b:0']\n",
    "for t in tv_all:\n",
    "    if t.name not in non_reg:\n",
    "        if(t.name.find('biases')==-1):\n",
    "            tv_regu.append(t)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    l2_loss = lambda_l2 * tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv_regu ])\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    total_loss = loss + l2_loss\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(total_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('data/vocab.pkl', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "word2id[unknown_token] = word_vocab_size -1\n",
    "id2word[word_vocab_size-1] = unknown_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f = open('data/word_embedding', 'rb')\n",
    "# word_embedding = pickle.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_tags_vocab = []\n",
    "for line in open('data/pos_tags.txt'):\n",
    "        pos_tags_vocab.append(line.strip())\n",
    "\n",
    "dep_vocab = []\n",
    "for line in open('data/dependency_types.txt'):\n",
    "    dep_vocab.append(line.strip())\n",
    "\n",
    "relation_vocab = []\n",
    "for line in open('data/relation_types.txt'):\n",
    "    relation_vocab.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "rel2id = dict((w, i) for i,w in enumerate(relation_vocab))\n",
    "id2rel = dict((i, w) for i,w in enumerate(relation_vocab))\n",
    "\n",
    "pos_tag2id = dict((w, i) for i,w in enumerate(pos_tags_vocab))\n",
    "id2pos_tag = dict((i, w) for i,w in enumerate(pos_tags_vocab))\n",
    "\n",
    "dep2id = dict((w, i) for i,w in enumerate(dep_vocab))\n",
    "id2dep = dict((i, w) for i,w in enumerate(dep_vocab))\n",
    "\n",
    "pos_tag2id['OTH'] = 9\n",
    "id2pos_tag[9] = 'OTH'\n",
    "\n",
    "dep2id['OTH'] = 20\n",
    "id2dep[20] = 'OTH'\n",
    "\n",
    "JJ_pos_tags = ['JJ', 'JJR', 'JJS']\n",
    "NN_pos_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "RB_pos_tags = ['RB', 'RBR', 'RBS']\n",
    "PRP_pos_tags = ['PRP', 'PRP$']\n",
    "VB_pos_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "_pos_tags = ['CC', 'CD', 'DT', 'IN']\n",
    "\n",
    "def pos_tag(x):\n",
    "    if x in JJ_pos_tags:\n",
    "        return pos_tag2id['JJ']\n",
    "    if x in NN_pos_tags:\n",
    "        return pos_tag2id['NN']\n",
    "    if x in RB_pos_tags:\n",
    "        return pos_tag2id['RB']\n",
    "    if x in PRP_pos_tags:\n",
    "        return pos_tag2id['PRP']\n",
    "    if x in VB_pos_tags:\n",
    "        return pos_tag2id['VB']\n",
    "    if x in _pos_tags:\n",
    "        return pos_tag2id[x]\n",
    "    else:\n",
    "        return 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('data/train_paths', 'rb')\n",
    "word_p1, word_p2, dep_p1, dep_p2, pos_p1, pos_p2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "relations = []\n",
    "for line in open('data/train_relations.txt'):\n",
    "    relations.append(line.strip().split()[1])\n",
    "\n",
    "length = len(word_p1)\n",
    "num_batches = int(length/batch_size)\n",
    "\n",
    "for i in range(length):\n",
    "    for j, word in enumerate(word_p1[i]):\n",
    "        word = word.lower()\n",
    "        word_p1[i][j] = word if word in word2id else unknown_token \n",
    "    for k, word in enumerate(word_p2[i]):\n",
    "        word = word.lower()\n",
    "        word_p2[i][k] = word if word in word2id else unknown_token \n",
    "    for l, d in enumerate(dep_p1[i]):\n",
    "        dep_p1[i][l] = d if d in dep2id else 'OTH'\n",
    "    for m, d in enumerate(dep_p2[i]):\n",
    "        dep_p2[i][m] = d if d in dep2id else 'OTH'\n",
    "\n",
    "word_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "word_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "rel_ids = np.array([rel2id[rel] for rel in relations])\n",
    "path1_len = np.array([len(w) for w in word_p1], dtype=int)\n",
    "path2_len = np.array([len(w) for w in word_p2])\n",
    "\n",
    "for i in range(length):\n",
    "    for j, w in enumerate(word_p1[i]):\n",
    "        word_p1_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(word_p2[i]):\n",
    "        word_p2_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(pos_p1[i]):\n",
    "        pos_p1_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(pos_p2[i]):\n",
    "        pos_p2_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(dep_p1[i]):\n",
    "        dep_p1_ids[i][j] = dep2id[w]\n",
    "    for j, w in enumerate(dep_p2[i]):\n",
    "        dep_p2_ids[i][j] = dep2id[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess.run(embedding_init, feed_dict={embedding_placeholder:word_embedding})\n",
    "# word_embedding_saver.save(sess, word_embd_dir + '/word_embd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.train.latest_checkpoint(model_dir)\n",
    "# saver.restore(sess, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint/word_embd/word_embd\n"
     ]
    }
   ],
   "source": [
    "latest_embd = tf.train.latest_checkpoint(word_embd_dir)\n",
    "word_embedding_saver.restore(sess, latest_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10 loss: 2.87203\n",
      "Step: 20 loss: 2.94049\n",
      "Step: 30 loss: 2.79787\n",
      "Step: 40 loss: 2.79359\n",
      "Step: 50 loss: 2.92462\n",
      "Step: 60 loss: 2.69989\n",
      "Step: 70 loss: 2.50553\n",
      "Step: 80 loss: 2.65037\n",
      "Step: 90 loss: 2.57664\n",
      "Step: 100 loss: 2.36885\n",
      "Step: 110 loss: 2.40983\n",
      "Step: 120 loss: 2.48324\n",
      "Step: 130 loss: 1.99289\n",
      "Step: 140 loss: 1.73668\n",
      "Step: 150 loss: 2.39096\n",
      "Step: 160 loss: 2.46612\n",
      "Step: 170 loss: 2.05314\n",
      "Step: 180 loss: 2.33783\n",
      "Step: 190 loss: 2.19994\n",
      "Step: 200 loss: 2.59979\n",
      "Step: 210 loss: 2.01877\n",
      "Step: 220 loss: 1.636\n",
      "Step: 230 loss: 2.16837\n",
      "Step: 240 loss: 1.73731\n",
      "Step: 250 loss: 1.66309\n",
      "Step: 260 loss: 1.93474\n",
      "Step: 270 loss: 1.65532\n",
      "Step: 280 loss: 1.3331\n",
      "Step: 290 loss: 1.85862\n",
      "Step: 300 loss: 1.37562\n",
      "Step: 310 loss: 1.83715\n",
      "Step: 320 loss: 1.48518\n",
      "Step: 330 loss: 1.67063\n",
      "Step: 340 loss: 2.25372\n",
      "Step: 350 loss: 2.7181\n",
      "Step: 360 loss: 1.91852\n",
      "Step: 370 loss: 1.25768\n",
      "Step: 380 loss: 1.55177\n",
      "Step: 390 loss: 1.68344\n",
      "Step: 400 loss: 1.32262\n",
      "Step: 410 loss: 1.94006\n",
      "Step: 420 loss: 1.34089\n",
      "Step: 430 loss: 0.98974\n",
      "Step: 440 loss: 0.566123\n",
      "Step: 450 loss: 1.32475\n",
      "Step: 460 loss: 1.62111\n",
      "Step: 470 loss: 1.42629\n",
      "Step: 480 loss: 1.59551\n",
      "Step: 490 loss: 1.52813\n",
      "Step: 500 loss: 1.61167\n",
      "Step: 510 loss: 2.18474\n",
      "Step: 520 loss: 1.43646\n",
      "Step: 530 loss: 1.24863\n",
      "Step: 540 loss: 1.08447\n",
      "Step: 550 loss: 1.90063\n",
      "Step: 560 loss: 1.26123\n",
      "Step: 570 loss: 1.30974\n",
      "Step: 580 loss: 1.58124\n",
      "Step: 590 loss: 1.55461\n",
      "Step: 600 loss: 1.31516\n",
      "Step: 610 loss: 1.04211\n",
      "Step: 620 loss: 0.963694\n",
      "Step: 630 loss: 0.927446\n",
      "Step: 640 loss: 0.994973\n",
      "Step: 650 loss: 0.76155\n",
      "Step: 660 loss: 0.853391\n",
      "Step: 670 loss: 0.970658\n",
      "Step: 680 loss: 0.895926\n",
      "Step: 690 loss: 0.662355\n",
      "Step: 700 loss: 1.47365\n",
      "Step: 710 loss: 1.63221\n",
      "Step: 720 loss: 1.46285\n",
      "Step: 730 loss: 3.02557\n",
      "Step: 740 loss: 1.54116\n",
      "Step: 750 loss: 0.753491\n",
      "Step: 760 loss: 1.53659\n",
      "Step: 770 loss: 1.79588\n",
      "Step: 780 loss: 1.21295\n",
      "Step: 790 loss: 0.710501\n",
      "Step: 800 loss: 1.54125\n",
      "Step: 810 loss: 0.919358\n",
      "Step: 820 loss: 1.60375\n",
      "Step: 830 loss: 1.9008\n",
      "Step: 840 loss: 0.8786\n",
      "Step: 850 loss: 1.60376\n",
      "Step: 860 loss: 0.933206\n",
      "Step: 870 loss: 1.10193\n",
      "Step: 880 loss: 0.724347\n",
      "Step: 890 loss: 0.816379\n",
      "Step: 900 loss: 0.751436\n",
      "Step: 910 loss: 1.16386\n",
      "Step: 920 loss: 1.24455\n",
      "Step: 930 loss: 0.349116\n",
      "Step: 940 loss: 0.305777\n",
      "Step: 950 loss: 0.89368\n",
      "Step: 960 loss: 1.19398\n",
      "Step: 970 loss: 0.77312\n",
      "Step: 980 loss: 1.24086\n",
      "Step: 990 loss: 1.07498\n",
      "Step: 1000 loss: 1.25745\n",
      "Saved Model\n",
      "Step: 1010 loss: 0.641244\n",
      "Step: 1020 loss: 0.63924\n",
      "Step: 1030 loss: 1.01667\n",
      "Step: 1040 loss: 0.815327\n",
      "Step: 1050 loss: 0.947396\n",
      "Step: 1060 loss: 1.05141\n",
      "Step: 1070 loss: 0.506212\n",
      "Step: 1080 loss: 0.604035\n",
      "Step: 1090 loss: 0.489681\n",
      "Step: 1100 loss: 0.810204\n",
      "Step: 1110 loss: 0.967969\n",
      "Step: 1120 loss: 0.738522\n",
      "Step: 1130 loss: 0.819173\n",
      "Step: 1140 loss: 1.42964\n",
      "Step: 1150 loss: 1.69382\n",
      "Step: 1160 loss: 0.987622\n",
      "Step: 1170 loss: 0.567567\n",
      "Step: 1180 loss: 0.929212\n",
      "Step: 1190 loss: 1.12148\n",
      "Step: 1200 loss: 0.64097\n",
      "Step: 1210 loss: 1.0385\n",
      "Step: 1220 loss: 0.550507\n",
      "Step: 1230 loss: 0.32371\n",
      "Step: 1240 loss: 0.336373\n",
      "Step: 1250 loss: 0.676136\n",
      "Step: 1260 loss: 0.853659\n",
      "Step: 1270 loss: 0.701284\n",
      "Step: 1280 loss: 1.28934\n",
      "Step: 1290 loss: 0.988826\n",
      "Step: 1300 loss: 0.733803\n",
      "Step: 1310 loss: 1.53673\n",
      "Step: 1320 loss: 0.633459\n",
      "Step: 1330 loss: 0.589422\n",
      "Step: 1340 loss: 0.614072\n",
      "Step: 1350 loss: 1.20001\n",
      "Step: 1360 loss: 0.612215\n",
      "Step: 1370 loss: 0.932379\n",
      "Step: 1380 loss: 0.775964\n",
      "Step: 1390 loss: 0.991666\n",
      "Step: 1400 loss: 0.653191\n",
      "Step: 1410 loss: 0.664293\n",
      "Step: 1420 loss: 0.652167\n",
      "Step: 1430 loss: 0.636288\n",
      "Step: 1440 loss: 0.52523\n",
      "Step: 1450 loss: 0.336438\n",
      "Step: 1460 loss: 0.285449\n",
      "Step: 1470 loss: 0.624039\n",
      "Step: 1480 loss: 0.531904\n",
      "Step: 1490 loss: 0.32919\n",
      "Step: 1500 loss: 0.757704\n",
      "Step: 1510 loss: 0.930007\n",
      "Step: 1520 loss: 0.511705\n",
      "Step: 1530 loss: 2.25691\n",
      "Step: 1540 loss: 1.19948\n",
      "Step: 1550 loss: 0.435151\n",
      "Step: 1560 loss: 0.810579\n",
      "Step: 1570 loss: 1.1123\n",
      "Step: 1580 loss: 0.863758\n",
      "Step: 1590 loss: 0.374377\n",
      "Step: 1600 loss: 1.19179\n",
      "Step: 1610 loss: 0.605866\n",
      "Step: 1620 loss: 0.827607\n",
      "Step: 1630 loss: 1.57022\n",
      "Step: 1640 loss: 0.965426\n",
      "Step: 1650 loss: 1.07399\n",
      "Step: 1660 loss: 0.407676\n",
      "Step: 1670 loss: 0.821843\n",
      "Step: 1680 loss: 0.486775\n",
      "Step: 1690 loss: 0.486364\n",
      "Step: 1700 loss: 0.510597\n",
      "Step: 1710 loss: 0.797067\n",
      "Step: 1720 loss: 0.87167\n",
      "Step: 1730 loss: 0.23844\n",
      "Step: 1740 loss: 0.170892\n",
      "Step: 1750 loss: 0.39493\n",
      "Step: 1760 loss: 1.02742\n",
      "Step: 1770 loss: 0.389828\n",
      "Step: 1780 loss: 0.80231\n",
      "Step: 1790 loss: 0.71747\n",
      "Step: 1800 loss: 0.644011\n",
      "Step: 1810 loss: 0.249269\n",
      "Step: 1820 loss: 0.302156\n",
      "Step: 1830 loss: 0.566663\n",
      "Step: 1840 loss: 0.71043\n",
      "Step: 1850 loss: 0.491273\n",
      "Step: 1860 loss: 0.479086\n",
      "Step: 1870 loss: 0.361218\n",
      "Step: 1880 loss: 0.414629\n",
      "Step: 1890 loss: 0.308123\n",
      "Step: 1900 loss: 0.589562\n",
      "Step: 1910 loss: 0.399516\n",
      "Step: 1920 loss: 0.528618\n",
      "Step: 1930 loss: 0.478195\n",
      "Step: 1940 loss: 0.974707\n",
      "Step: 1950 loss: 0.927638\n",
      "Step: 1960 loss: 0.356723\n",
      "Step: 1970 loss: 0.246757\n",
      "Step: 1980 loss: 0.378954\n",
      "Step: 1990 loss: 0.857787\n",
      "Step: 2000 loss: 0.489517\n",
      "Saved Model\n",
      "Step: 2010 loss: 0.448438\n",
      "Step: 2020 loss: 0.401963\n",
      "Step: 2030 loss: 0.161064\n",
      "Step: 2040 loss: 0.207795\n",
      "Step: 2050 loss: 0.22647\n",
      "Step: 2060 loss: 0.475197\n",
      "Step: 2070 loss: 0.374663\n",
      "Step: 2080 loss: 1.07972\n",
      "Step: 2090 loss: 0.616691\n",
      "Step: 2100 loss: 0.366981\n",
      "Step: 2110 loss: 0.897884\n",
      "Step: 2120 loss: 0.33168\n",
      "Step: 2130 loss: 0.236528\n",
      "Step: 2140 loss: 0.458332\n",
      "Step: 2150 loss: 0.664502\n",
      "Step: 2160 loss: 0.332869\n",
      "Step: 2170 loss: 0.486452\n",
      "Step: 2180 loss: 0.542678\n",
      "Step: 2190 loss: 0.637517\n",
      "Step: 2200 loss: 0.434905\n",
      "Step: 2210 loss: 0.461254\n",
      "Step: 2220 loss: 0.3009\n",
      "Step: 2230 loss: 0.505201\n",
      "Step: 2240 loss: 0.305806\n",
      "Step: 2250 loss: 0.239939\n",
      "Step: 2260 loss: 0.125464\n",
      "Step: 2270 loss: 0.435025\n",
      "Step: 2280 loss: 0.30977\n",
      "Step: 2290 loss: 0.203842\n",
      "Step: 2300 loss: 0.24661\n",
      "Step: 2310 loss: 0.709484\n",
      "Step: 2320 loss: 0.188809\n",
      "Step: 2330 loss: 1.07645\n",
      "Step: 2340 loss: 0.931812\n",
      "Step: 2350 loss: 0.257583\n",
      "Step: 2360 loss: 0.263721\n",
      "Step: 2370 loss: 0.419114\n",
      "Step: 2380 loss: 0.592002\n",
      "Step: 2390 loss: 0.242564\n",
      "Step: 2400 loss: 0.809286\n",
      "Step: 2410 loss: 0.36763\n",
      "Step: 2420 loss: 0.417431\n",
      "Step: 2430 loss: 1.06237\n",
      "Step: 2440 loss: 0.933218\n",
      "Step: 2450 loss: 0.475214\n",
      "Step: 2460 loss: 0.18968\n",
      "Step: 2470 loss: 0.509034\n",
      "Step: 2480 loss: 0.347391\n",
      "Step: 2490 loss: 0.240108\n",
      "Step: 2500 loss: 0.331955\n",
      "Step: 2510 loss: 0.430762\n",
      "Step: 2520 loss: 0.563595\n",
      "Step: 2530 loss: 0.170152\n",
      "Step: 2540 loss: 0.106489\n",
      "Step: 2550 loss: 0.178609\n",
      "Step: 2560 loss: 0.968784\n",
      "Step: 2570 loss: 0.20739\n",
      "Step: 2580 loss: 0.434517\n",
      "Step: 2590 loss: 0.446132\n",
      "Step: 2600 loss: 0.199483\n",
      "Step: 2610 loss: 0.146071\n",
      "Step: 2620 loss: 0.146536\n",
      "Step: 2630 loss: 0.341315\n",
      "Step: 2640 loss: 0.544701\n",
      "Step: 2650 loss: 0.269298\n",
      "Step: 2660 loss: 0.233406\n",
      "Step: 2670 loss: 0.161926\n",
      "Step: 2680 loss: 0.295515\n",
      "Step: 2690 loss: 0.244775\n",
      "Step: 2700 loss: 0.309024\n",
      "Step: 2710 loss: 0.204682\n",
      "Step: 2720 loss: 0.221038\n",
      "Step: 2730 loss: 0.246843\n",
      "Step: 2740 loss: 0.711819\n",
      "Step: 2750 loss: 0.634192\n",
      "Step: 2760 loss: 0.152945\n",
      "Step: 2770 loss: 0.127821\n",
      "Step: 2780 loss: 0.175354\n",
      "Step: 2790 loss: 0.580793\n",
      "Step: 2800 loss: 0.337324\n",
      "Step: 2810 loss: 0.230052\n",
      "Step: 2820 loss: 0.226085\n",
      "Step: 2830 loss: 0.121365\n",
      "Step: 2840 loss: 0.103407\n",
      "Step: 2850 loss: 0.130872\n",
      "Step: 2860 loss: 0.227449\n",
      "Step: 2870 loss: 0.287611\n",
      "Step: 2880 loss: 0.825654\n",
      "Step: 2890 loss: 0.41193\n",
      "Step: 2900 loss: 0.162182\n",
      "Step: 2910 loss: 0.376492\n",
      "Step: 2920 loss: 0.153835\n",
      "Step: 2930 loss: 0.121965\n",
      "Step: 2940 loss: 0.173833\n",
      "Step: 2950 loss: 0.280445\n",
      "Step: 2960 loss: 0.176056\n",
      "Step: 2970 loss: 0.154004\n",
      "Step: 2980 loss: 0.371529\n",
      "Step: 2990 loss: 0.250649\n",
      "Step: 3000 loss: 0.211271\n",
      "Saved Model\n",
      "Step: 3010 loss: 0.219983\n",
      "Step: 3020 loss: 0.144003\n",
      "Step: 3030 loss: 0.263737\n",
      "Step: 3040 loss: 0.235231\n",
      "Step: 3050 loss: 0.206154\n",
      "Step: 3060 loss: 0.0955326\n",
      "Step: 3070 loss: 0.187669\n",
      "Step: 3080 loss: 0.226561\n",
      "Step: 3090 loss: 0.136173\n",
      "Step: 3100 loss: 0.112635\n",
      "Step: 3110 loss: 0.573445\n",
      "Step: 3120 loss: 0.117188\n",
      "Step: 3130 loss: 0.517922\n",
      "Step: 3140 loss: 0.503338\n",
      "Step: 3150 loss: 0.149814\n",
      "Step: 3160 loss: 0.114813\n",
      "Step: 3170 loss: 0.163766\n",
      "Step: 3180 loss: 0.322654\n",
      "Step: 3190 loss: 0.154758\n",
      "Step: 3200 loss: 0.593552\n",
      "Step: 3210 loss: 0.246746\n",
      "Step: 3220 loss: 0.193113\n",
      "Step: 3230 loss: 0.629748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3240 loss: 0.578042\n",
      "Step: 3250 loss: 0.217345\n",
      "Step: 3260 loss: 0.114212\n",
      "Step: 3270 loss: 0.255328\n",
      "Step: 3280 loss: 0.205612\n",
      "Step: 3290 loss: 0.126631\n",
      "Step: 3300 loss: 0.150584\n",
      "Step: 3310 loss: 0.187266\n",
      "Step: 3320 loss: 0.297325\n",
      "Step: 3330 loss: 0.149014\n",
      "Step: 3340 loss: 0.101138\n",
      "Step: 3350 loss: 0.12946\n",
      "Step: 3360 loss: 0.788479\n",
      "Step: 3370 loss: 0.135797\n",
      "Step: 3380 loss: 0.252258\n",
      "Step: 3390 loss: 0.270987\n",
      "Step: 3400 loss: 0.110068\n",
      "Step: 3410 loss: 0.100875\n",
      "Step: 3420 loss: 0.100394\n",
      "Step: 3430 loss: 0.197775\n",
      "Step: 3440 loss: 0.37373\n",
      "Step: 3450 loss: 0.135395\n",
      "Step: 3460 loss: 0.11437\n",
      "Step: 3470 loss: 0.13198\n",
      "Step: 3480 loss: 0.206246\n",
      "Step: 3490 loss: 0.157283\n",
      "Step: 3500 loss: 0.128176\n",
      "Step: 3510 loss: 0.130112\n",
      "Step: 3520 loss: 0.108631\n",
      "Step: 3530 loss: 0.124168\n",
      "Step: 3540 loss: 0.395311\n",
      "Step: 3550 loss: 0.259584\n",
      "Step: 3560 loss: 0.10725\n",
      "Step: 3570 loss: 0.130895\n",
      "Step: 3580 loss: 0.100903\n",
      "Step: 3590 loss: 0.302576\n",
      "Step: 3600 loss: 0.176222\n",
      "Step: 3610 loss: 0.125035\n",
      "Step: 3620 loss: 0.121983\n",
      "Step: 3630 loss: 0.129925\n",
      "Step: 3640 loss: 0.0972927\n",
      "Step: 3650 loss: 0.101577\n",
      "Step: 3660 loss: 0.166524\n",
      "Step: 3670 loss: 0.445433\n",
      "Step: 3680 loss: 0.517837\n",
      "Step: 3690 loss: 0.178787\n",
      "Step: 3700 loss: 0.115456\n",
      "Step: 3710 loss: 0.153966\n",
      "Step: 3720 loss: 0.103079\n",
      "Step: 3730 loss: 0.10644\n",
      "Step: 3740 loss: 0.150168\n",
      "Step: 3750 loss: 0.176898\n",
      "Step: 3760 loss: 0.140416\n",
      "Step: 3770 loss: 0.11504\n",
      "Step: 3780 loss: 0.221142\n",
      "Step: 3790 loss: 0.173377\n",
      "Step: 3800 loss: 0.112948\n",
      "Step: 3810 loss: 0.115687\n",
      "Step: 3820 loss: 0.116759\n",
      "Step: 3830 loss: 0.153482\n",
      "Step: 3840 loss: 0.149331\n",
      "Step: 3850 loss: 0.154834\n",
      "Step: 3860 loss: 0.088969\n",
      "Step: 3870 loss: 0.108449\n",
      "Step: 3880 loss: 0.122139\n",
      "Step: 3890 loss: 0.0987176\n",
      "Step: 3900 loss: 0.0984192\n",
      "Step: 3910 loss: 0.266266\n",
      "Step: 3920 loss: 0.0920193\n",
      "Step: 3930 loss: 0.308998\n",
      "Step: 3940 loss: 0.185333\n",
      "Step: 3950 loss: 0.32886\n",
      "Step: 3960 loss: 0.191394\n",
      "Step: 3970 loss: 0.118054\n",
      "Step: 3980 loss: 0.119695\n",
      "Step: 3990 loss: 0.10148\n",
      "Step: 4000 loss: 0.314172\n",
      "Saved Model\n",
      "Step: 4010 loss: 0.148705\n",
      "Step: 4020 loss: 0.149474\n",
      "Step: 4030 loss: 0.176554\n",
      "Step: 4040 loss: 0.176773\n",
      "Step: 4050 loss: 0.143614\n",
      "Step: 4060 loss: 0.0941934\n",
      "Step: 4070 loss: 0.112567\n",
      "Step: 4080 loss: 0.106175\n",
      "Step: 4090 loss: 0.100135\n",
      "Step: 4100 loss: 0.1127\n",
      "Step: 4110 loss: 0.12385\n",
      "Step: 4120 loss: 0.168224\n",
      "Step: 4130 loss: 0.146942\n",
      "Step: 4140 loss: 0.0853272\n",
      "Step: 4150 loss: 0.114257\n",
      "Step: 4160 loss: 0.595908\n",
      "Step: 4170 loss: 0.0987496\n",
      "Step: 4180 loss: 0.226231\n",
      "Step: 4190 loss: 0.163044\n",
      "Step: 4200 loss: 0.0875967\n",
      "Step: 4210 loss: 0.0879946\n",
      "Step: 4220 loss: 0.0883066\n",
      "Step: 4230 loss: 0.122353\n",
      "Step: 4240 loss: 0.164618\n",
      "Step: 4250 loss: 0.101984\n",
      "Step: 4260 loss: 0.0909683\n",
      "Step: 4270 loss: 0.0972135\n",
      "Step: 4280 loss: 0.125302\n",
      "Step: 4290 loss: 0.155562\n",
      "Step: 4300 loss: 0.100078\n",
      "Step: 4310 loss: 0.0928701\n",
      "Step: 4320 loss: 0.0932873\n",
      "Step: 4330 loss: 0.124393\n",
      "Step: 4340 loss: 0.218195\n",
      "Step: 4350 loss: 0.168344\n",
      "Step: 4360 loss: 0.0890062\n",
      "Step: 4370 loss: 0.124486\n",
      "Step: 4380 loss: 0.0923542\n",
      "Step: 4390 loss: 0.214462\n",
      "Step: 4400 loss: 0.117949\n",
      "Step: 4410 loss: 0.0903112\n",
      "Step: 4420 loss: 0.0993563\n",
      "Step: 4430 loss: 0.120233\n",
      "Step: 4440 loss: 0.09606\n",
      "Step: 4450 loss: 0.113845\n",
      "Step: 4460 loss: 0.0964917\n",
      "Step: 4470 loss: 0.559442\n",
      "Step: 4480 loss: 0.108567\n",
      "Step: 4490 loss: 0.105445\n",
      "Step: 4500 loss: 0.0914407\n",
      "Step: 4510 loss: 0.101934\n",
      "Step: 4520 loss: 0.101247\n",
      "Step: 4530 loss: 0.101486\n",
      "Step: 4540 loss: 0.0972272\n",
      "Step: 4550 loss: 0.119318\n",
      "Step: 4560 loss: 0.115366\n",
      "Step: 4570 loss: 0.0882683\n",
      "Step: 4580 loss: 0.105505\n",
      "Step: 4590 loss: 0.112559\n",
      "Step: 4600 loss: 0.0973571\n",
      "Step: 4610 loss: 0.093667\n",
      "Step: 4620 loss: 0.0866074\n",
      "Step: 4630 loss: 0.141253\n",
      "Step: 4640 loss: 0.103628\n",
      "Step: 4650 loss: 0.103899\n",
      "Step: 4660 loss: 0.082117\n",
      "Step: 4670 loss: 0.126455\n",
      "Step: 4680 loss: 0.0874377\n",
      "Step: 4690 loss: 0.0861268\n",
      "Step: 4700 loss: 0.0886574\n",
      "Step: 4710 loss: 0.12257\n",
      "Step: 4720 loss: 0.0817955\n",
      "Step: 4730 loss: 0.172821\n",
      "Step: 4740 loss: 0.147551\n",
      "Step: 4750 loss: 0.0926716\n",
      "Step: 4760 loss: 0.224273\n",
      "Step: 4770 loss: 0.126187\n",
      "Step: 4780 loss: 0.102232\n",
      "Step: 4790 loss: 0.082895\n",
      "Step: 4800 loss: 0.143827\n",
      "Step: 4810 loss: 0.10474\n",
      "Step: 4820 loss: 0.140876\n",
      "Step: 4830 loss: 0.133338\n",
      "Step: 4840 loss: 0.205403\n",
      "Step: 4850 loss: 0.0973336\n",
      "Step: 4860 loss: 0.0927074\n",
      "Step: 4870 loss: 0.0847506\n",
      "Step: 4880 loss: 0.0991968\n",
      "Step: 4890 loss: 0.0940577\n",
      "Step: 4900 loss: 0.103734\n",
      "Step: 4910 loss: 0.0992002\n",
      "Step: 4920 loss: 0.185841\n",
      "Step: 4930 loss: 0.108793\n",
      "Step: 4940 loss: 0.079923\n",
      "Step: 4950 loss: 0.0990565\n",
      "Step: 4960 loss: 0.377282\n",
      "Step: 4970 loss: 0.0832104\n",
      "Step: 4980 loss: 0.271163\n",
      "Step: 4990 loss: 0.101686\n",
      "Step: 5000 loss: 0.0816746\n",
      "Saved Model\n",
      "Step: 5010 loss: 0.0792448\n",
      "Step: 5020 loss: 0.0825134\n",
      "Step: 5030 loss: 0.111144\n",
      "Step: 5040 loss: 0.0986816\n",
      "Step: 5050 loss: 0.0888806\n",
      "Step: 5060 loss: 0.0907216\n",
      "Step: 5070 loss: 0.0869557\n",
      "Step: 5080 loss: 0.125091\n",
      "Step: 5090 loss: 0.0859091\n",
      "Step: 5100 loss: 0.0820199\n",
      "Step: 5110 loss: 0.0841055\n",
      "Step: 5120 loss: 0.0822861\n",
      "Step: 5130 loss: 0.103859\n",
      "Step: 5140 loss: 0.126979\n",
      "Step: 5150 loss: 0.0941118\n",
      "Step: 5160 loss: 0.0807499\n",
      "Step: 5170 loss: 0.0803901\n",
      "Step: 5180 loss: 0.0859344\n",
      "Step: 5190 loss: 0.0847567\n",
      "Step: 5200 loss: 0.11292\n",
      "Step: 5210 loss: 0.0831337\n",
      "Step: 5220 loss: 0.087329\n",
      "Step: 5230 loss: 0.0866947\n",
      "Step: 5240 loss: 0.0810102\n",
      "Step: 5250 loss: 0.0879982\n",
      "Step: 5260 loss: 0.0821109\n",
      "Step: 5270 loss: 0.301605\n",
      "Step: 5280 loss: 0.0886526\n",
      "Step: 5290 loss: 0.0955361\n",
      "Step: 5300 loss: 0.083986\n",
      "Step: 5310 loss: 0.0920674\n",
      "Step: 5320 loss: 0.0807148\n",
      "Step: 5330 loss: 0.0828778\n",
      "Step: 5340 loss: 0.0900521\n",
      "Step: 5350 loss: 0.0966008\n",
      "Step: 5360 loss: 0.0968066\n",
      "Step: 5370 loss: 0.0802257\n",
      "Step: 5380 loss: 0.0839866\n",
      "Step: 5390 loss: 0.0921271\n",
      "Step: 5400 loss: 0.0792763\n",
      "Step: 5410 loss: 0.0804496\n",
      "Step: 5420 loss: 0.0817065\n",
      "Step: 5430 loss: 0.0901231\n",
      "Step: 5440 loss: 0.106668\n",
      "Step: 5450 loss: 0.100074\n",
      "Step: 5460 loss: 0.0750045\n",
      "Step: 5470 loss: 0.093704\n",
      "Step: 5480 loss: 0.0869417\n",
      "Step: 5490 loss: 0.079127\n",
      "Step: 5500 loss: 0.0796572\n",
      "Step: 5510 loss: 0.0900924\n",
      "Step: 5520 loss: 0.0745649\n",
      "Step: 5530 loss: 0.158047\n",
      "Step: 5540 loss: 0.109903\n",
      "Step: 5550 loss: 0.107961\n",
      "Step: 5560 loss: 0.0779273\n",
      "Step: 5570 loss: 0.0844277\n",
      "Step: 5580 loss: 0.0889191\n",
      "Step: 5590 loss: 0.0749442\n",
      "Step: 5600 loss: 0.0905568\n",
      "Step: 5610 loss: 0.100572\n",
      "Step: 5620 loss: 0.0798617\n",
      "Step: 5630 loss: 0.0953658\n",
      "Step: 5640 loss: 0.0778156\n",
      "Step: 5650 loss: 0.0774293\n",
      "Step: 5660 loss: 0.0829749\n",
      "Step: 5670 loss: 0.0850397\n",
      "Step: 5680 loss: 0.0860413\n",
      "Step: 5690 loss: 0.074093\n",
      "Step: 5700 loss: 0.143565\n",
      "Step: 5710 loss: 0.0821891\n",
      "Step: 5720 loss: 0.0927287\n",
      "Step: 5730 loss: 0.0800599\n",
      "Step: 5740 loss: 0.0731412\n",
      "Step: 5750 loss: 0.0934236\n",
      "Step: 5760 loss: 0.21061\n",
      "Step: 5770 loss: 0.0780153\n",
      "Step: 5780 loss: 0.318139\n",
      "Step: 5790 loss: 0.0812326\n",
      "Step: 5800 loss: 0.0739366\n",
      "Step: 5810 loss: 0.0720062\n",
      "Step: 5820 loss: 0.0733796\n",
      "Step: 5830 loss: 0.0857191\n",
      "Step: 5840 loss: 0.0874589\n",
      "Step: 5850 loss: 0.0782433\n",
      "Step: 5860 loss: 0.0767359\n",
      "Step: 5870 loss: 0.0775934\n",
      "Step: 5880 loss: 0.0976674\n",
      "Step: 5890 loss: 0.0745992\n",
      "Step: 5900 loss: 0.0740285\n",
      "Step: 5910 loss: 0.0762128\n",
      "Step: 5920 loss: 0.0718235\n",
      "Step: 5930 loss: 0.0761162\n",
      "Step: 5940 loss: 0.0865674\n",
      "Step: 5950 loss: 0.0845895\n",
      "Step: 5960 loss: 0.0724562\n",
      "Step: 5970 loss: 0.0714257\n",
      "Step: 5980 loss: 0.0732252\n",
      "Step: 5990 loss: 0.0794966\n",
      "Step: 6000 loss: 0.0814884\n",
      "Saved Model\n",
      "Step: 6010 loss: 0.0718051\n",
      "Step: 6020 loss: 0.0796459\n",
      "Step: 6030 loss: 0.0737906\n",
      "Step: 6040 loss: 0.0713595\n",
      "Step: 6050 loss: 0.0705797\n",
      "Step: 6060 loss: 0.0794976\n",
      "Step: 6070 loss: 0.256866\n",
      "Step: 6080 loss: 0.0752097\n",
      "Step: 6090 loss: 0.0778642\n",
      "Step: 6100 loss: 0.076902\n",
      "Step: 6110 loss: 0.0838072\n",
      "Step: 6120 loss: 0.070459\n",
      "Step: 6130 loss: 0.0771014\n",
      "Step: 6140 loss: 0.074442\n",
      "Step: 6150 loss: 0.0875714\n",
      "Step: 6160 loss: 0.122316\n",
      "Step: 6170 loss: 0.0700642\n",
      "Step: 6180 loss: 0.0802775\n",
      "Step: 6190 loss: 0.0777773\n",
      "Step: 6200 loss: 0.0726022\n",
      "Step: 6210 loss: 0.0698563\n",
      "Step: 6220 loss: 0.0692216\n",
      "Step: 6230 loss: 0.0717388\n",
      "Step: 6240 loss: 0.0736076\n",
      "Step: 6250 loss: 0.0677\n",
      "Step: 6260 loss: 0.0673462\n",
      "Step: 6270 loss: 0.0703136\n",
      "Step: 6280 loss: 0.076186\n",
      "Step: 6290 loss: 0.0686499\n",
      "Step: 6300 loss: 0.0760124\n",
      "Step: 6310 loss: 0.0796052\n",
      "Step: 6320 loss: 0.0672224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6330 loss: 0.228381\n",
      "Step: 6340 loss: 0.0802837\n",
      "Step: 6350 loss: 0.0718766\n",
      "Step: 6360 loss: 0.0698685\n",
      "Step: 6370 loss: 0.0836705\n",
      "Step: 6380 loss: 0.0787375\n",
      "Step: 6390 loss: 0.0777286\n",
      "Step: 6400 loss: 0.0727035\n",
      "Step: 6410 loss: 0.0709271\n",
      "Step: 6420 loss: 0.0766888\n",
      "Step: 6430 loss: 0.0755974\n",
      "Step: 6440 loss: 0.0718128\n",
      "Step: 6450 loss: 0.0704479\n",
      "Step: 6460 loss: 0.0686913\n",
      "Step: 6470 loss: 0.0669666\n",
      "Step: 6480 loss: 0.0683517\n",
      "Step: 6490 loss: 0.0656099\n",
      "Step: 6500 loss: 0.0742228\n",
      "Step: 6510 loss: 0.0670818\n",
      "Step: 6520 loss: 0.0985332\n",
      "Step: 6530 loss: 0.0702038\n",
      "Step: 6540 loss: 0.0644812\n",
      "Step: 6550 loss: 0.069682\n",
      "Step: 6560 loss: 0.10884\n",
      "Step: 6570 loss: 0.0655684\n",
      "Step: 6580 loss: 0.194564\n",
      "Step: 6590 loss: 0.0705782\n",
      "Step: 6600 loss: 0.0670806\n",
      "Step: 6610 loss: 0.063809\n",
      "Step: 6620 loss: 0.0648018\n",
      "Step: 6630 loss: 0.0716937\n",
      "Step: 6640 loss: 0.0778472\n",
      "Step: 6650 loss: 0.0688316\n",
      "Step: 6660 loss: 0.0706609\n",
      "Step: 6670 loss: 0.0758656\n",
      "Step: 6680 loss: 0.083786\n",
      "Step: 6690 loss: 0.0687488\n",
      "Step: 6700 loss: 0.0649788\n",
      "Step: 6710 loss: 0.0712369\n",
      "Step: 6720 loss: 0.0635067\n",
      "Step: 6730 loss: 0.076312\n",
      "Step: 6740 loss: 0.0761529\n",
      "Step: 6750 loss: 0.069833\n",
      "Step: 6760 loss: 0.0641813\n",
      "Step: 6770 loss: 0.0680908\n",
      "Step: 6780 loss: 0.063898\n",
      "Step: 6790 loss: 0.0803626\n",
      "Step: 6800 loss: 0.0701127\n",
      "Step: 6810 loss: 0.0690147\n",
      "Step: 6820 loss: 0.0748334\n",
      "Step: 6830 loss: 0.0637525\n",
      "Step: 6840 loss: 0.0688063\n",
      "Step: 6850 loss: 0.0640538\n",
      "Step: 6860 loss: 0.0643194\n",
      "Step: 6870 loss: 0.298982\n",
      "Step: 6880 loss: 0.0748938\n",
      "Step: 6890 loss: 0.067106\n",
      "Step: 6900 loss: 0.0634936\n",
      "Step: 6910 loss: 0.0660444\n",
      "Step: 6920 loss: 0.0626497\n",
      "Step: 6930 loss: 0.0692667\n",
      "Step: 6940 loss: 0.0632066\n",
      "Step: 6950 loss: 0.0667373\n",
      "Step: 6960 loss: 0.0694051\n",
      "Step: 6970 loss: 0.0623294\n",
      "Step: 6980 loss: 0.0668796\n",
      "Step: 6990 loss: 0.0820533\n",
      "Step: 7000 loss: 0.0623566\n",
      "Saved Model\n",
      "Step: 7010 loss: 0.0617265\n",
      "Step: 7020 loss: 0.0658506\n",
      "Step: 7030 loss: 0.0672536\n",
      "Step: 7040 loss: 0.068849\n",
      "Step: 7050 loss: 0.0597107\n",
      "Step: 7060 loss: 0.0595227\n",
      "Step: 7070 loss: 0.0628105\n",
      "Step: 7080 loss: 0.0639908\n",
      "Step: 7090 loss: 0.0614845\n",
      "Step: 7100 loss: 0.0629051\n",
      "Step: 7110 loss: 0.0815584\n",
      "Step: 7120 loss: 0.0594966\n",
      "Step: 7130 loss: 0.19118\n",
      "Step: 7140 loss: 0.0711715\n",
      "Step: 7150 loss: 0.0773294\n",
      "Step: 7160 loss: 0.0751334\n",
      "Step: 7170 loss: 0.0734196\n",
      "Step: 7180 loss: 0.0666435\n",
      "Step: 7190 loss: 0.0672051\n",
      "Step: 7200 loss: 0.0747555\n",
      "Step: 7210 loss: 0.0661963\n",
      "Step: 7220 loss: 0.0751641\n",
      "Step: 7230 loss: 0.102308\n",
      "Step: 7240 loss: 0.0836889\n",
      "Step: 7250 loss: 0.0690978\n",
      "Step: 7260 loss: 0.0710241\n",
      "Step: 7270 loss: 0.0688741\n",
      "Step: 7280 loss: 0.0682191\n",
      "Step: 7290 loss: 0.305054\n",
      "Step: 7300 loss: 0.22053\n",
      "Step: 7310 loss: 0.0685723\n",
      "Step: 7320 loss: 0.330327\n",
      "Step: 7330 loss: 0.086949\n",
      "Step: 7340 loss: 0.0652568\n",
      "Step: 7350 loss: 0.0684686\n",
      "Step: 7360 loss: 0.213263\n",
      "Step: 7370 loss: 0.0711094\n",
      "Step: 7380 loss: 0.137342\n",
      "Step: 7390 loss: 0.102956\n",
      "Step: 7400 loss: 0.0844967\n",
      "Step: 7410 loss: 0.0722863\n",
      "Step: 7420 loss: 0.29774\n",
      "Step: 7430 loss: 0.0833448\n",
      "Step: 7440 loss: 0.161731\n",
      "Step: 7450 loss: 0.132626\n",
      "Step: 7460 loss: 0.0809863\n",
      "Step: 7470 loss: 0.0747605\n",
      "Step: 7480 loss: 0.164563\n",
      "Step: 7490 loss: 0.128878\n",
      "Step: 7500 loss: 0.0663326\n",
      "Step: 7510 loss: 0.073471\n",
      "Step: 7520 loss: 0.0665314\n",
      "Step: 7530 loss: 0.076694\n",
      "Step: 7540 loss: 0.279574\n",
      "Step: 7550 loss: 0.196723\n",
      "Step: 7560 loss: 0.0685138\n",
      "Step: 7570 loss: 0.0732842\n",
      "Step: 7580 loss: 0.151221\n",
      "Step: 7590 loss: 0.134344\n",
      "Step: 7600 loss: 0.217609\n",
      "Step: 7610 loss: 0.0881133\n",
      "Step: 7620 loss: 0.13825\n",
      "Step: 7630 loss: 0.0674538\n",
      "Step: 7640 loss: 0.151065\n",
      "Step: 7650 loss: 0.0803744\n",
      "Step: 7660 loss: 0.0787835\n",
      "Step: 7670 loss: 0.489053\n",
      "Step: 7680 loss: 0.0722974\n",
      "Step: 7690 loss: 0.257141\n",
      "Step: 7700 loss: 0.0698668\n",
      "Step: 7710 loss: 0.137529\n",
      "Step: 7720 loss: 0.0752428\n",
      "Step: 7730 loss: 0.0811111\n",
      "Step: 7740 loss: 0.0768655\n",
      "Step: 7750 loss: 0.314267\n",
      "Step: 7760 loss: 0.0888957\n",
      "Step: 7770 loss: 0.159253\n",
      "Step: 7780 loss: 0.0964825\n",
      "Step: 7790 loss: 0.0859047\n",
      "Step: 7800 loss: 0.0712892\n",
      "Step: 7810 loss: 0.106262\n",
      "Step: 7820 loss: 0.313514\n",
      "Step: 7830 loss: 0.0782723\n",
      "Step: 7840 loss: 0.0877248\n",
      "Step: 7850 loss: 0.0789152\n",
      "Step: 7860 loss: 0.070598\n",
      "Step: 7870 loss: 0.0807042\n",
      "Step: 7880 loss: 0.0821425\n",
      "Step: 7890 loss: 0.074283\n",
      "Step: 7900 loss: 0.0738444\n",
      "Step: 7910 loss: 0.278255\n",
      "Step: 7920 loss: 0.0795533\n",
      "Step: 7930 loss: 0.157351\n",
      "Step: 7940 loss: 0.0960921\n",
      "Step: 7950 loss: 0.0882629\n",
      "Step: 7960 loss: 0.077235\n",
      "Step: 7970 loss: 0.120016\n",
      "Step: 7980 loss: 0.268543\n",
      "Step: 7990 loss: 0.072849\n",
      "Step: 8000 loss: 0.081709\n",
      "Saved Model\n",
      "Step: 8010 loss: 0.0880373\n",
      "Step: 8020 loss: 0.0740013\n",
      "Step: 8030 loss: 0.0861596\n",
      "Step: 8040 loss: 0.135043\n",
      "Step: 8050 loss: 0.0862104\n",
      "Step: 8060 loss: 0.0750156\n",
      "Step: 8070 loss: 0.0741704\n",
      "Step: 8080 loss: 0.107454\n",
      "Step: 8090 loss: 0.0762665\n",
      "Step: 8100 loss: 0.0745243\n",
      "Step: 8110 loss: 0.0768621\n",
      "Step: 8120 loss: 0.0773114\n",
      "Step: 8130 loss: 0.0860899\n",
      "Step: 8140 loss: 0.0784069\n",
      "Step: 8150 loss: 0.0836646\n",
      "Step: 8160 loss: 0.0951736\n",
      "Step: 8170 loss: 0.0901249\n",
      "Step: 8180 loss: 0.520979\n",
      "Step: 8190 loss: 0.112593\n",
      "Step: 8200 loss: 0.0763038\n",
      "Step: 8210 loss: 0.0701954\n",
      "Step: 8220 loss: 0.0716828\n",
      "Step: 8230 loss: 0.0738519\n",
      "Step: 8240 loss: 0.0745227\n",
      "Step: 8250 loss: 0.0742999\n",
      "Step: 8260 loss: 0.0749422\n",
      "Step: 8270 loss: 0.074348\n",
      "Step: 8280 loss: 0.0919886\n",
      "Step: 8290 loss: 0.0740872\n",
      "Step: 8300 loss: 0.0755562\n",
      "Step: 8310 loss: 0.0738414\n",
      "Step: 8320 loss: 0.101686\n",
      "Step: 8330 loss: 0.0776753\n",
      "Step: 8340 loss: 0.0867815\n",
      "Step: 8350 loss: 0.0767195\n",
      "Step: 8360 loss: 0.0718452\n",
      "Step: 8370 loss: 0.0726823\n",
      "Step: 8380 loss: 0.0761162\n",
      "Step: 8390 loss: 0.0895509\n",
      "Step: 8400 loss: 0.17387\n",
      "Step: 8410 loss: 0.0787949\n",
      "Step: 8420 loss: 0.0831356\n",
      "Step: 8430 loss: 0.0801279\n",
      "Step: 8440 loss: 0.0703809\n",
      "Step: 8450 loss: 0.0697976\n",
      "Step: 8460 loss: 0.0747049\n",
      "Step: 8470 loss: 0.158177\n",
      "Step: 8480 loss: 0.101339\n",
      "Step: 8490 loss: 0.078026\n",
      "Step: 8500 loss: 0.0961141\n",
      "Step: 8510 loss: 0.0739993\n",
      "Step: 8520 loss: 0.0695335\n",
      "Step: 8530 loss: 0.0732287\n",
      "Step: 8540 loss: 0.0823845\n",
      "Step: 8550 loss: 0.0735274\n",
      "Step: 8560 loss: 0.0827787\n",
      "Step: 8570 loss: 0.0694326\n",
      "Step: 8580 loss: 0.0854937\n",
      "Step: 8590 loss: 0.092786\n",
      "Step: 8600 loss: 0.0750727\n",
      "Step: 8610 loss: 0.0699511\n",
      "Step: 8620 loss: 0.0681355\n",
      "Step: 8630 loss: 0.0681922\n",
      "Step: 8640 loss: 0.0753623\n",
      "Step: 8650 loss: 0.068201\n",
      "Step: 8660 loss: 0.0675292\n",
      "Step: 8670 loss: 0.098104\n",
      "Step: 8680 loss: 0.0689726\n",
      "Step: 8690 loss: 0.0684698\n",
      "Step: 8700 loss: 0.0714909\n",
      "Step: 8710 loss: 0.069209\n",
      "Step: 8720 loss: 0.067817\n",
      "Step: 8730 loss: 0.0926241\n",
      "Step: 8740 loss: 0.216177\n",
      "Step: 8750 loss: 0.0796341\n",
      "Step: 8760 loss: 0.239293\n",
      "Step: 8770 loss: 0.0699226\n",
      "Step: 8780 loss: 0.0678734\n",
      "Step: 8790 loss: 0.0856568\n",
      "Step: 8800 loss: 0.0895387\n",
      "Step: 8810 loss: 0.068241\n",
      "Step: 8820 loss: 0.0703798\n",
      "Step: 8830 loss: 0.0784917\n",
      "Step: 8840 loss: 0.0692725\n",
      "Step: 8850 loss: 0.11237\n",
      "Step: 8860 loss: 0.0750167\n",
      "Step: 8870 loss: 0.0684035\n",
      "Step: 8880 loss: 0.0672858\n",
      "Step: 8890 loss: 0.0667199\n",
      "Step: 8900 loss: 0.0926714\n",
      "Step: 8910 loss: 0.0685106\n",
      "Step: 8920 loss: 0.124122\n",
      "Step: 8930 loss: 0.0672721\n",
      "Step: 8940 loss: 0.0663574\n",
      "Step: 8950 loss: 0.0684577\n",
      "Step: 8960 loss: 0.0733424\n",
      "Step: 8970 loss: 0.0688871\n",
      "Step: 8980 loss: 0.408342\n",
      "Step: 8990 loss: 0.0688539\n",
      "Step: 9000 loss: 0.066068\n",
      "Saved Model\n",
      "Step: 9010 loss: 0.0656594\n",
      "Step: 9020 loss: 0.0653962\n",
      "Step: 9030 loss: 0.067568\n",
      "Step: 9040 loss: 0.0736758\n",
      "Step: 9050 loss: 0.0659652\n",
      "Step: 9060 loss: 0.0881252\n",
      "Step: 9070 loss: 0.0660158\n",
      "Step: 9080 loss: 0.0670303\n",
      "Step: 9090 loss: 0.0650612\n",
      "Step: 9100 loss: 0.0666285\n",
      "Step: 9110 loss: 0.066203\n",
      "Step: 9120 loss: 0.0653421\n",
      "Step: 9130 loss: 0.0660872\n",
      "Step: 9140 loss: 0.072151\n",
      "Step: 9150 loss: 0.071718\n",
      "Step: 9160 loss: 0.0660128\n",
      "Step: 9170 loss: 0.0655493\n",
      "Step: 9180 loss: 0.0649239\n",
      "Step: 9190 loss: 0.066647\n",
      "Step: 9200 loss: 0.064815\n",
      "Step: 9210 loss: 0.06473\n",
      "Step: 9220 loss: 0.335605\n",
      "Step: 9230 loss: 0.0663805\n",
      "Step: 9240 loss: 0.0646908\n",
      "Step: 9250 loss: 0.0640392\n",
      "Step: 9260 loss: 0.0655716\n",
      "Step: 9270 loss: 0.372064\n",
      "Step: 9280 loss: 0.191108\n",
      "Step: 9290 loss: 0.0663732\n",
      "Step: 9300 loss: 0.066183\n",
      "Step: 9310 loss: 0.071637\n",
      "Step: 9320 loss: 0.0645818\n",
      "Step: 9330 loss: 0.0678578\n",
      "Step: 9340 loss: 0.0862463\n",
      "Step: 9350 loss: 0.0693215\n",
      "Step: 9360 loss: 0.0739586\n",
      "Step: 9370 loss: 0.0632685\n",
      "Step: 9380 loss: 0.0671983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 9390 loss: 0.0726333\n",
      "Step: 9400 loss: 0.0635252\n",
      "Step: 9410 loss: 0.0634555\n",
      "Step: 9420 loss: 0.0634227\n",
      "Step: 9430 loss: 0.063042\n",
      "Step: 9440 loss: 0.0877039\n",
      "Step: 9450 loss: 0.0628113\n",
      "Step: 9460 loss: 0.0624544\n",
      "Step: 9470 loss: 0.0627328\n",
      "Step: 9480 loss: 0.0629401\n",
      "Step: 9490 loss: 0.0637829\n",
      "Step: 9500 loss: 0.0644716\n",
      "Step: 9510 loss: 0.0658092\n",
      "Step: 9520 loss: 0.062915\n",
      "Step: 9530 loss: 0.0761087\n",
      "Step: 9540 loss: 0.0629772\n",
      "Step: 9550 loss: 0.0628617\n",
      "Step: 9560 loss: 0.0624458\n",
      "Step: 9570 loss: 0.0635827\n",
      "Step: 9580 loss: 0.0664681\n",
      "Step: 9590 loss: 0.0670332\n",
      "Step: 9600 loss: 0.0680933\n",
      "Step: 9610 loss: 0.0626622\n",
      "Step: 9620 loss: 0.0798335\n",
      "Step: 9630 loss: 0.0634341\n",
      "Step: 9640 loss: 0.0708747\n",
      "Step: 9650 loss: 0.0612786\n",
      "Step: 9660 loss: 0.0625679\n",
      "Step: 9670 loss: 0.0610204\n",
      "Step: 9680 loss: 0.0609803\n",
      "Step: 9690 loss: 0.0602213\n",
      "Step: 9700 loss: 0.0648566\n",
      "Step: 9710 loss: 0.0608875\n",
      "Step: 9720 loss: 0.0620151\n",
      "Step: 9730 loss: 0.0692284\n",
      "Step: 9740 loss: 0.0597157\n",
      "Step: 9750 loss: 0.0641531\n",
      "Step: 9760 loss: 0.0657571\n",
      "Step: 9770 loss: 0.0646316\n",
      "Step: 9780 loss: 0.285107\n",
      "Step: 9790 loss: 0.0615594\n",
      "Step: 9800 loss: 0.059523\n",
      "Step: 9810 loss: 0.0590883\n",
      "Step: 9820 loss: 0.059057\n",
      "Step: 9830 loss: 0.0692157\n",
      "Step: 9840 loss: 0.063783\n",
      "Step: 9850 loss: 0.0603827\n",
      "Step: 9860 loss: 0.0609151\n",
      "Step: 9870 loss: 0.0601736\n",
      "Step: 9880 loss: 0.0635683\n",
      "Step: 9890 loss: 0.0594609\n",
      "Step: 9900 loss: 0.058686\n",
      "Step: 9910 loss: 0.0608483\n",
      "Step: 9920 loss: 0.0580834\n",
      "Step: 9930 loss: 0.0592601\n",
      "Step: 9940 loss: 0.06031\n",
      "Step: 9950 loss: 0.0611594\n",
      "Step: 9960 loss: 0.0599747\n",
      "Step: 9970 loss: 0.0580555\n",
      "Step: 9980 loss: 0.0578897\n",
      "Step: 9990 loss: 0.0634057\n",
      "Step: 10000 loss: 0.0578476\n",
      "Saved Model\n",
      "Step: 10010 loss: 0.0574252\n",
      "Step: 10020 loss: 0.0587027\n",
      "Step: 10030 loss: 0.0572362\n",
      "Step: 10040 loss: 0.0571019\n",
      "Step: 10050 loss: 0.0571382\n",
      "Step: 10060 loss: 0.0676725\n",
      "Step: 10070 loss: 0.164547\n",
      "Step: 10080 loss: 0.0579581\n",
      "Step: 10090 loss: 0.0586993\n",
      "Step: 10100 loss: 0.0573329\n",
      "Step: 10110 loss: 0.0583423\n",
      "Step: 10120 loss: 0.0571989\n",
      "Step: 10130 loss: 0.0582531\n",
      "Step: 10140 loss: 0.0575449\n",
      "Step: 10150 loss: 0.059971\n",
      "Step: 10160 loss: 0.0608475\n",
      "Step: 10170 loss: 0.057162\n",
      "Step: 10180 loss: 0.0621291\n",
      "Step: 10190 loss: 0.0618802\n",
      "Step: 10200 loss: 0.0574247\n",
      "Step: 10210 loss: 0.0560602\n",
      "Step: 10220 loss: 0.0553762\n",
      "Step: 10230 loss: 0.0554379\n",
      "Step: 10240 loss: 0.0589073\n",
      "Step: 10250 loss: 0.0554741\n",
      "Step: 10260 loss: 0.0545661\n",
      "Step: 10270 loss: 0.0549449\n",
      "Step: 10280 loss: 0.0553034\n",
      "Step: 10290 loss: 0.0556279\n",
      "Step: 10300 loss: 0.0563967\n",
      "Step: 10310 loss: 0.0559603\n",
      "Step: 10320 loss: 0.0544599\n",
      "Step: 10330 loss: 0.0694565\n",
      "Step: 10340 loss: 0.0626223\n",
      "Step: 10350 loss: 0.0553274\n",
      "Step: 10360 loss: 0.0548988\n",
      "Step: 10370 loss: 0.0553921\n",
      "Step: 10380 loss: 0.0584416\n",
      "Step: 10390 loss: 0.0630826\n",
      "Step: 10400 loss: 0.0554625\n",
      "Step: 10410 loss: 0.0546392\n",
      "Step: 10420 loss: 0.054507\n",
      "Step: 10430 loss: 0.055923\n",
      "Step: 10440 loss: 0.0532951\n",
      "Step: 10450 loss: 0.0534173\n",
      "Step: 10460 loss: 0.0544876\n",
      "Step: 10470 loss: 0.0530152\n",
      "Step: 10480 loss: 0.0542851\n",
      "Step: 10490 loss: 0.052406\n",
      "Step: 10500 loss: 0.0575997\n",
      "Step: 10510 loss: 0.0525369\n",
      "Step: 10520 loss: 0.0544775\n",
      "Step: 10530 loss: 0.0546526\n",
      "Step: 10540 loss: 0.0519402\n",
      "Step: 10550 loss: 0.0531693\n",
      "Step: 10560 loss: 0.058378\n",
      "Step: 10570 loss: 0.0531929\n",
      "Step: 10580 loss: 0.228693\n",
      "Step: 10590 loss: 0.0538254\n",
      "Step: 10600 loss: 0.0523298\n",
      "Step: 10610 loss: 0.0511568\n",
      "Step: 10620 loss: 0.0511038\n",
      "Step: 10630 loss: 0.0529772\n",
      "Step: 10640 loss: 0.0548657\n",
      "Step: 10650 loss: 0.0523622\n",
      "Step: 10660 loss: 0.0527589\n",
      "Step: 10670 loss: 0.0518005\n",
      "Step: 10680 loss: 0.0580158\n",
      "Step: 10690 loss: 0.0511751\n",
      "Step: 10700 loss: 0.0511262\n",
      "Step: 10710 loss: 0.0517914\n",
      "Step: 10720 loss: 0.05005\n",
      "Step: 10730 loss: 0.0518882\n",
      "Step: 10740 loss: 0.054202\n",
      "Step: 10750 loss: 0.0530933\n",
      "Step: 10760 loss: 0.0518089\n",
      "Step: 10770 loss: 0.0498686\n",
      "Step: 10780 loss: 0.0496688\n",
      "Step: 10790 loss: 0.0565777\n",
      "Step: 10800 loss: 0.0504688\n",
      "Step: 10810 loss: 0.0497522\n",
      "Step: 10820 loss: 0.0514049\n",
      "Step: 10830 loss: 0.0497112\n",
      "Step: 10840 loss: 0.0491434\n",
      "Step: 10850 loss: 0.0493178\n",
      "Step: 10860 loss: 0.0495477\n",
      "Step: 10870 loss: 0.289482\n",
      "Step: 10880 loss: 0.0496452\n",
      "Step: 10890 loss: 0.0518491\n",
      "Step: 10900 loss: 0.0501107\n",
      "Step: 10910 loss: 0.0520897\n",
      "Step: 10920 loss: 0.0489517\n",
      "Step: 10930 loss: 0.0494316\n",
      "Step: 10940 loss: 0.0501199\n",
      "Step: 10950 loss: 0.0518951\n",
      "Step: 10960 loss: 0.0524512\n",
      "Step: 10970 loss: 0.0488913\n",
      "Step: 10980 loss: 0.0517471\n",
      "Step: 10990 loss: 0.0602523\n",
      "Step: 11000 loss: 0.0488773\n",
      "Saved Model\n",
      "Step: 11010 loss: 0.0487984\n",
      "Step: 11020 loss: 0.048436\n",
      "Step: 11030 loss: 0.0481377\n",
      "Step: 11040 loss: 0.0515288\n",
      "Step: 11050 loss: 0.0491625\n",
      "Step: 11060 loss: 0.0469853\n",
      "Step: 11070 loss: 0.0475811\n",
      "Step: 11080 loss: 0.047588\n",
      "Step: 11090 loss: 0.0489892\n",
      "Step: 11100 loss: 0.0481409\n",
      "Step: 11110 loss: 0.0487338\n",
      "Step: 11120 loss: 0.0467668\n",
      "Step: 11130 loss: 0.0599619\n",
      "Step: 11140 loss: 0.0500701\n",
      "Step: 11150 loss: 0.0479092\n",
      "Step: 11160 loss: 0.047213\n",
      "Step: 11170 loss: 0.0488484\n",
      "Step: 11180 loss: 0.0520882\n",
      "Step: 11190 loss: 0.0496315\n",
      "Step: 11200 loss: 0.0494789\n",
      "Step: 11210 loss: 0.0469473\n",
      "Step: 11220 loss: 0.0472598\n",
      "Step: 11230 loss: 0.0478246\n",
      "Step: 11240 loss: 0.0461492\n",
      "Step: 11250 loss: 0.0461964\n",
      "Step: 11260 loss: 0.0474671\n",
      "Step: 11270 loss: 0.0459056\n",
      "Step: 11280 loss: 0.0481567\n",
      "Step: 11290 loss: 0.0451526\n",
      "Step: 11300 loss: 0.051183\n",
      "Step: 11310 loss: 0.0458239\n",
      "Step: 11320 loss: 0.0470232\n",
      "Step: 11330 loss: 0.0482869\n",
      "Step: 11340 loss: 0.0445909\n",
      "Step: 11350 loss: 0.046137\n",
      "Step: 11360 loss: 0.0558048\n",
      "Step: 11370 loss: 0.0462859\n",
      "Step: 11380 loss: 0.259932\n",
      "Step: 11390 loss: 0.0462528\n",
      "Step: 11400 loss: 0.0449231\n",
      "Step: 11410 loss: 0.0439602\n",
      "Step: 11420 loss: 0.0438636\n",
      "Step: 11430 loss: 0.0464871\n",
      "Step: 11440 loss: 0.0487611\n",
      "Step: 11450 loss: 0.0453391\n",
      "Step: 11460 loss: 0.0467597\n",
      "Step: 11470 loss: 0.0445792\n",
      "Step: 11480 loss: 0.0498556\n",
      "Step: 11490 loss: 0.0450656\n",
      "Step: 11500 loss: 0.0439889\n",
      "Step: 11510 loss: 0.0461957\n",
      "Step: 11520 loss: 0.0430306\n",
      "Step: 11530 loss: 0.0457874\n",
      "Step: 11540 loss: 0.0483032\n",
      "Step: 11550 loss: 0.0460621\n",
      "Step: 11560 loss: 0.0449258\n",
      "Step: 11570 loss: 0.0430431\n",
      "Step: 11580 loss: 0.042806\n",
      "Step: 11590 loss: 0.0523316\n",
      "Step: 11600 loss: 0.045322\n",
      "Step: 11610 loss: 0.0432174\n",
      "Step: 11620 loss: 0.0442259\n",
      "Step: 11630 loss: 0.0437958\n",
      "Step: 11640 loss: 0.0423896\n",
      "Step: 11650 loss: 0.0425634\n",
      "Step: 11660 loss: 0.0432744\n",
      "Step: 11670 loss: 0.208529\n",
      "Step: 11680 loss: 0.0432924\n",
      "Step: 11690 loss: 0.0464527\n",
      "Step: 11700 loss: 0.0434922\n",
      "Step: 11710 loss: 0.0448595\n",
      "Step: 11720 loss: 0.0422456\n",
      "Step: 11730 loss: 0.0424149\n",
      "Step: 11740 loss: 0.0441285\n",
      "Step: 11750 loss: 0.0448751\n",
      "Step: 11760 loss: 0.0484893\n",
      "Step: 11770 loss: 0.0430788\n",
      "Step: 11780 loss: 0.0479011\n",
      "Step: 11790 loss: 0.0446966\n",
      "Step: 11800 loss: 0.043601\n",
      "Step: 11810 loss: 0.0427329\n",
      "Step: 11820 loss: 0.0431884\n",
      "Step: 11830 loss: 0.0439701\n",
      "Step: 11840 loss: 0.0471809\n",
      "Step: 11850 loss: 0.0445578\n",
      "Step: 11860 loss: 0.0409869\n",
      "Step: 11870 loss: 0.0416449\n",
      "Step: 11880 loss: 0.0417854\n",
      "Step: 11890 loss: 0.0436177\n",
      "Step: 11900 loss: 0.0418918\n",
      "Step: 11910 loss: 0.0441165\n",
      "Step: 11920 loss: 0.0405762\n",
      "Step: 11930 loss: 0.0594751\n",
      "Step: 11940 loss: 0.045447\n",
      "Step: 11950 loss: 0.0429462\n",
      "Step: 11960 loss: 0.0412762\n",
      "Step: 11970 loss: 0.0441506\n",
      "Step: 11980 loss: 0.0475183\n",
      "Step: 11990 loss: 0.0422461\n",
      "Step: 12000 loss: 0.0447159\n",
      "Saved Model\n",
      "Step: 12010 loss: 0.0410505\n",
      "Step: 12020 loss: 0.0429683\n",
      "Step: 12030 loss: 0.0423181\n",
      "Step: 12040 loss: 0.0409337\n",
      "Step: 12050 loss: 0.0399658\n",
      "Step: 12060 loss: 0.0419958\n",
      "Step: 12070 loss: 0.0403486\n",
      "Step: 12080 loss: 0.042775\n",
      "Step: 12090 loss: 0.039976\n",
      "Step: 12100 loss: 0.0434526\n",
      "Step: 12110 loss: 0.0404226\n",
      "Step: 12120 loss: 0.043961\n",
      "Step: 12130 loss: 0.0438161\n",
      "Step: 12140 loss: 0.0389077\n",
      "Step: 12150 loss: 0.0405451\n",
      "Step: 12160 loss: 0.0586603\n",
      "Step: 12170 loss: 0.0405683\n",
      "Step: 12180 loss: 0.189942\n",
      "Step: 12190 loss: 0.0410814\n",
      "Step: 12200 loss: 0.0388813\n",
      "Step: 12210 loss: 0.0383996\n",
      "Step: 12220 loss: 0.0385041\n",
      "Step: 12230 loss: 0.0411224\n",
      "Step: 12240 loss: 0.0431297\n",
      "Step: 12250 loss: 0.0406316\n",
      "Step: 12260 loss: 0.0405653\n",
      "Step: 12270 loss: 0.0391875\n",
      "Step: 12280 loss: 0.0452704\n",
      "Step: 12290 loss: 0.0407114\n",
      "Step: 12300 loss: 0.0383443\n",
      "Step: 12310 loss: 0.0420688\n",
      "Step: 12320 loss: 0.0376921\n",
      "Step: 12330 loss: 0.0408258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 12340 loss: 0.0449373\n",
      "Step: 12350 loss: 0.0406301\n",
      "Step: 12360 loss: 0.0399375\n",
      "Step: 12370 loss: 0.0381377\n",
      "Step: 12380 loss: 0.037408\n",
      "Step: 12390 loss: 0.0556019\n",
      "Step: 12400 loss: 0.0440799\n",
      "Step: 12410 loss: 0.0379918\n",
      "Step: 12420 loss: 0.0387271\n",
      "Step: 12430 loss: 0.0382483\n",
      "Step: 12440 loss: 0.0374323\n",
      "Step: 12450 loss: 0.0374527\n",
      "Step: 12460 loss: 0.0379388\n",
      "Step: 12470 loss: 0.243013\n",
      "Step: 12480 loss: 0.0385961\n",
      "Step: 12490 loss: 0.0408791\n",
      "Step: 12500 loss: 0.0386611\n",
      "Step: 12510 loss: 0.0396702\n",
      "Step: 12520 loss: 0.0374399\n",
      "Step: 12530 loss: 0.037948\n",
      "Step: 12540 loss: 0.039682\n",
      "Step: 12550 loss: 0.0416581\n",
      "Step: 12560 loss: 0.0399892\n",
      "Step: 12570 loss: 0.0377993\n",
      "Step: 12580 loss: 0.0472667\n",
      "Step: 12590 loss: 0.0468467\n",
      "Step: 12600 loss: 0.0385915\n",
      "Step: 12610 loss: 0.0381843\n",
      "Step: 12620 loss: 0.0436951\n",
      "Step: 12630 loss: 0.0388464\n",
      "Step: 12640 loss: 0.0520183\n",
      "Step: 12650 loss: 0.037705\n",
      "Step: 12660 loss: 0.036456\n",
      "Step: 12670 loss: 0.0374101\n",
      "Step: 12680 loss: 0.0443635\n",
      "Step: 12690 loss: 0.0374634\n",
      "Step: 12700 loss: 0.0375091\n",
      "Step: 12710 loss: 0.0407473\n",
      "Step: 12720 loss: 0.0360165\n",
      "Step: 12730 loss: 0.0466309\n",
      "Step: 12740 loss: 0.0404173\n",
      "Step: 12750 loss: 0.0386034\n",
      "Step: 12760 loss: 0.0368898\n",
      "Step: 12770 loss: 0.0421758\n",
      "Step: 12780 loss: 0.0454316\n",
      "Step: 12790 loss: 0.036642\n",
      "Step: 12800 loss: 0.0403659\n",
      "Step: 12810 loss: 0.0407815\n",
      "Step: 12820 loss: 0.0394982\n",
      "Step: 12830 loss: 0.037191\n",
      "Step: 12840 loss: 0.0368471\n",
      "Step: 12850 loss: 0.0358832\n",
      "Step: 12860 loss: 0.0529896\n",
      "Step: 12870 loss: 0.0365928\n",
      "Step: 12880 loss: 0.0429633\n",
      "Step: 12890 loss: 0.0364539\n",
      "Step: 12900 loss: 0.0407857\n",
      "Step: 12910 loss: 0.0375976\n",
      "Step: 12920 loss: 0.0601266\n",
      "Step: 12930 loss: 0.0419523\n",
      "Step: 12940 loss: 0.0360021\n",
      "Step: 12950 loss: 0.0392938\n",
      "Step: 12960 loss: 0.0471366\n",
      "Step: 12970 loss: 0.0424616\n",
      "Step: 12980 loss: 0.206767\n",
      "Step: 12990 loss: 0.0468852\n",
      "Step: 13000 loss: 0.0426039\n",
      "Saved Model\n",
      "Step: 13010 loss: 0.0372223\n",
      "Step: 13020 loss: 0.0380895\n",
      "Step: 13030 loss: 0.0484315\n",
      "Step: 13040 loss: 0.136276\n",
      "Step: 13050 loss: 0.11722\n",
      "Step: 13060 loss: 0.0388325\n",
      "Step: 13070 loss: 0.0482116\n",
      "Step: 13080 loss: 0.115682\n",
      "Step: 13090 loss: 0.0704351\n",
      "Step: 13100 loss: 0.0412185\n",
      "Step: 13110 loss: 0.121406\n",
      "Step: 13120 loss: 0.0416941\n",
      "Step: 13130 loss: 0.0900319\n",
      "Step: 13140 loss: 0.103293\n",
      "Step: 13150 loss: 0.0564414\n",
      "Step: 13160 loss: 0.139062\n",
      "Step: 13170 loss: 0.236738\n",
      "Step: 13180 loss: 0.0491706\n",
      "Step: 13190 loss: 0.0568773\n",
      "Step: 13200 loss: 0.199049\n",
      "Step: 13210 loss: 0.0624894\n",
      "Step: 13220 loss: 0.239129\n",
      "Step: 13230 loss: 0.36622\n",
      "Step: 13240 loss: 0.0824893\n",
      "Step: 13250 loss: 0.0537728\n",
      "Step: 13260 loss: 0.0530893\n",
      "Step: 13270 loss: 0.186856\n",
      "Step: 13280 loss: 0.0715436\n",
      "Step: 13290 loss: 0.0538246\n",
      "Step: 13300 loss: 0.0563378\n",
      "Step: 13310 loss: 0.0868418\n",
      "Step: 13320 loss: 0.187762\n",
      "Step: 13330 loss: 0.0656594\n",
      "Step: 13340 loss: 0.0576364\n",
      "Step: 13350 loss: 0.152451\n",
      "Step: 13360 loss: 0.0669592\n",
      "Step: 13370 loss: 0.0557829\n",
      "Step: 13380 loss: 0.0587173\n",
      "Step: 13390 loss: 0.45117\n",
      "Step: 13400 loss: 0.0825559\n",
      "Step: 13410 loss: 0.0532467\n",
      "Step: 13420 loss: 0.0662403\n",
      "Step: 13430 loss: 0.0703985\n",
      "Step: 13440 loss: 0.0619405\n",
      "Step: 13450 loss: 0.0593263\n",
      "Step: 13460 loss: 0.0527738\n",
      "Step: 13470 loss: 0.072024\n",
      "Step: 13480 loss: 0.123143\n",
      "Step: 13490 loss: 0.160374\n",
      "Step: 13500 loss: 0.0518007\n",
      "Step: 13510 loss: 0.0894951\n",
      "Step: 13520 loss: 0.0960569\n",
      "Step: 13530 loss: 0.199482\n",
      "Step: 13540 loss: 0.522234\n",
      "Step: 13550 loss: 0.0784738\n",
      "Step: 13560 loss: 0.184825\n",
      "Step: 13570 loss: 0.0555991\n",
      "Step: 13580 loss: 0.0550438\n",
      "Step: 13590 loss: 0.105194\n",
      "Step: 13600 loss: 0.0715501\n",
      "Step: 13610 loss: 0.0533139\n",
      "Step: 13620 loss: 0.185869\n",
      "Step: 13630 loss: 0.0686783\n",
      "Step: 13640 loss: 0.160405\n",
      "Step: 13650 loss: 0.0533897\n",
      "Step: 13660 loss: 0.0515861\n",
      "Step: 13670 loss: 0.0541553\n",
      "Step: 13680 loss: 0.0526832\n",
      "Step: 13690 loss: 0.0892212\n",
      "Step: 13700 loss: 0.0523296\n",
      "Step: 13710 loss: 0.0550553\n",
      "Step: 13720 loss: 0.0535673\n",
      "Step: 13730 loss: 0.082996\n",
      "Step: 13740 loss: 0.0833844\n",
      "Step: 13750 loss: 0.0587258\n",
      "Step: 13760 loss: 0.0675914\n",
      "Step: 13770 loss: 0.36145\n",
      "Step: 13780 loss: 0.234085\n",
      "Step: 13790 loss: 0.0511457\n",
      "Step: 13800 loss: 0.0516818\n",
      "Step: 13810 loss: 0.0504998\n",
      "Step: 13820 loss: 0.050555\n",
      "Step: 13830 loss: 0.0536634\n",
      "Step: 13840 loss: 0.0529426\n",
      "Step: 13850 loss: 0.0620283\n",
      "Step: 13860 loss: 0.0596443\n",
      "Step: 13870 loss: 0.0628827\n",
      "Step: 13880 loss: 0.0557782\n",
      "Step: 13890 loss: 0.0506738\n",
      "Step: 13900 loss: 0.0565957\n",
      "Step: 13910 loss: 0.0503406\n",
      "Step: 13920 loss: 0.0500478\n",
      "Step: 13930 loss: 0.0796531\n",
      "Step: 13940 loss: 0.143522\n",
      "Step: 13950 loss: 0.0515416\n",
      "Step: 13960 loss: 0.299162\n",
      "Step: 13970 loss: 0.0533715\n",
      "Step: 13980 loss: 0.0584654\n",
      "Step: 13990 loss: 0.14513\n",
      "Step: 14000 loss: 0.0530709\n",
      "Saved Model\n",
      "Step: 14010 loss: 0.0557802\n",
      "Step: 14020 loss: 0.0560965\n",
      "Step: 14030 loss: 0.0502407\n",
      "Step: 14040 loss: 0.0533992\n",
      "Step: 14050 loss: 0.0506762\n",
      "Step: 14060 loss: 0.0596662\n",
      "Step: 14070 loss: 0.176918\n",
      "Step: 14080 loss: 0.0582277\n",
      "Step: 14090 loss: 0.0514352\n",
      "Step: 14100 loss: 0.0520504\n",
      "Step: 14110 loss: 0.0501603\n",
      "Step: 14120 loss: 0.049306\n",
      "Step: 14130 loss: 0.0498978\n",
      "Step: 14140 loss: 0.052521\n",
      "Step: 14150 loss: 0.0532526\n",
      "Step: 14160 loss: 0.0781764\n",
      "Step: 14170 loss: 0.0501524\n",
      "Step: 14180 loss: 0.0537955\n",
      "Step: 14190 loss: 0.053041\n",
      "Step: 14200 loss: 0.0505073\n",
      "Step: 14210 loss: 0.0497923\n",
      "Step: 14220 loss: 0.048535\n",
      "Step: 14230 loss: 0.0503524\n",
      "Step: 14240 loss: 0.0606767\n",
      "Step: 14250 loss: 0.0977544\n",
      "Step: 14260 loss: 0.0483871\n",
      "Step: 14270 loss: 0.0488\n",
      "Step: 14280 loss: 0.0588655\n",
      "Step: 14290 loss: 0.0484894\n",
      "Step: 14300 loss: 0.0514676\n",
      "Step: 14310 loss: 0.0612451\n",
      "Step: 14320 loss: 0.0477744\n",
      "Step: 14330 loss: 0.0591537\n",
      "Step: 14340 loss: 0.0551793\n",
      "Step: 14350 loss: 0.0575077\n",
      "Step: 14360 loss: 0.0487536\n",
      "Step: 14370 loss: 0.0562329\n",
      "Step: 14380 loss: 0.0576676\n",
      "Step: 14390 loss: 0.0509934\n",
      "Step: 14400 loss: 0.0527191\n",
      "Step: 14410 loss: 0.0499565\n",
      "Step: 14420 loss: 0.0480392\n",
      "Step: 14430 loss: 0.0483441\n",
      "Step: 14440 loss: 0.0482216\n",
      "Step: 14450 loss: 0.0483552\n",
      "Step: 14460 loss: 0.0475683\n",
      "Step: 14470 loss: 0.0502848\n",
      "Step: 14480 loss: 0.0474323\n",
      "Step: 14490 loss: 0.0473886\n",
      "Step: 14500 loss: 0.0485492\n",
      "Step: 14510 loss: 0.0478469\n",
      "Step: 14520 loss: 0.0497883\n",
      "Step: 14530 loss: 0.0482172\n",
      "Step: 14540 loss: 0.0467042\n",
      "Step: 14550 loss: 0.0510273\n",
      "Step: 14560 loss: 0.0610697\n",
      "Step: 14570 loss: 0.0482054\n",
      "Step: 14580 loss: 0.188209\n",
      "Step: 14590 loss: 0.0498063\n",
      "Step: 14600 loss: 0.0465712\n",
      "Step: 14610 loss: 0.0462808\n",
      "Step: 14620 loss: 0.0463777\n",
      "Step: 14630 loss: 0.0514362\n",
      "Step: 14640 loss: 0.054752\n",
      "Step: 14650 loss: 0.0483682\n",
      "Step: 14660 loss: 0.0501699\n",
      "Step: 14670 loss: 0.0464938\n",
      "Step: 14680 loss: 0.0466113\n",
      "Step: 14690 loss: 0.0459501\n",
      "Step: 14700 loss: 0.0494529\n",
      "Step: 14710 loss: 0.0462441\n",
      "Step: 14720 loss: 0.0489639\n",
      "Step: 14730 loss: 0.045603\n",
      "Step: 14740 loss: 0.0463159\n",
      "Step: 14750 loss: 0.0480802\n",
      "Step: 14760 loss: 0.0455822\n",
      "Step: 14770 loss: 0.0457213\n",
      "Step: 14780 loss: 0.045366\n",
      "Step: 14790 loss: 0.0583908\n",
      "Step: 14800 loss: 0.0457478\n",
      "Step: 14810 loss: 0.044977\n",
      "Step: 14820 loss: 0.0485228\n",
      "Step: 14830 loss: 0.046831\n",
      "Step: 14840 loss: 0.0449034\n",
      "Step: 14850 loss: 0.0451502\n",
      "Step: 14860 loss: 0.0451423\n",
      "Step: 14870 loss: 0.262975\n",
      "Step: 14880 loss: 0.046302\n",
      "Step: 14890 loss: 0.0456402\n",
      "Step: 14900 loss: 0.0466128\n",
      "Step: 14910 loss: 0.0476052\n",
      "Step: 14920 loss: 0.0443923\n",
      "Step: 14930 loss: 0.0450876\n",
      "Step: 14940 loss: 0.0461384\n",
      "Step: 14950 loss: 0.0471495\n",
      "Step: 14960 loss: 0.0479012\n",
      "Step: 14970 loss: 0.044663\n",
      "Step: 14980 loss: 0.0504162\n",
      "Step: 14990 loss: 0.0463683\n",
      "Step: 15000 loss: 0.0540088\n",
      "Saved Model\n",
      "Step: 15010 loss: 0.0441978\n",
      "Step: 15020 loss: 0.0437238\n",
      "Step: 15030 loss: 0.0437153\n",
      "Step: 15040 loss: 0.0621896\n",
      "Step: 15050 loss: 0.0437091\n",
      "Step: 15060 loss: 0.0434288\n",
      "Step: 15070 loss: 0.0441814\n",
      "Step: 15080 loss: 0.0432834\n",
      "Step: 15090 loss: 0.0436726\n",
      "Step: 15100 loss: 0.0437103\n",
      "Step: 15110 loss: 0.0446999\n",
      "Step: 15120 loss: 0.0431127\n",
      "Step: 15130 loss: 0.0521152\n",
      "Step: 15140 loss: 0.049749\n",
      "Step: 15150 loss: 0.0433279\n",
      "Step: 15160 loss: 0.043498\n",
      "Step: 15170 loss: 0.0446154\n",
      "Step: 15180 loss: 0.0454719\n",
      "Step: 15190 loss: 0.0448073\n",
      "Step: 15200 loss: 0.0445712\n",
      "Step: 15210 loss: 0.0439005\n",
      "Step: 15220 loss: 0.0426778\n",
      "Step: 15230 loss: 0.0426578\n",
      "Step: 15240 loss: 0.0425173\n",
      "Step: 15250 loss: 0.042943\n",
      "Step: 15260 loss: 0.0426997\n",
      "Step: 15270 loss: 0.0423767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 15280 loss: 0.042355\n",
      "Step: 15290 loss: 0.0422148\n",
      "Step: 15300 loss: 0.0499156\n",
      "Step: 15310 loss: 0.0423368\n",
      "Step: 15320 loss: 0.104294\n",
      "Step: 15330 loss: 0.0432414\n",
      "Step: 15340 loss: 0.0418278\n",
      "Step: 15350 loss: 0.0422188\n",
      "Step: 15360 loss: 0.0451274\n",
      "Step: 15370 loss: 0.0419246\n",
      "Step: 15380 loss: 0.143821\n",
      "Step: 15390 loss: 0.0424784\n",
      "Step: 15400 loss: 0.0428406\n",
      "Step: 15410 loss: 0.0412053\n",
      "Step: 15420 loss: 0.0411216\n",
      "Step: 15430 loss: 0.0421234\n",
      "Step: 15440 loss: 0.0443036\n",
      "Step: 15450 loss: 0.0433544\n",
      "Step: 15460 loss: 0.0419378\n",
      "Step: 15470 loss: 0.0416222\n",
      "Step: 15480 loss: 0.0420453\n",
      "Step: 15490 loss: 0.0408432\n",
      "Step: 15500 loss: 0.0408033\n",
      "Step: 15510 loss: 0.0410112\n",
      "Step: 15520 loss: 0.0411865\n",
      "Step: 15530 loss: 0.0407078\n",
      "Step: 15540 loss: 0.0411532\n",
      "Step: 15550 loss: 0.0418497\n",
      "Step: 15560 loss: 0.0405876\n",
      "Step: 15570 loss: 0.0414735\n",
      "Step: 15580 loss: 0.0401146\n",
      "Step: 15590 loss: 0.0453463\n",
      "Step: 15600 loss: 0.0416337\n",
      "Step: 15610 loss: 0.0401773\n",
      "Step: 15620 loss: 0.0407705\n",
      "Step: 15630 loss: 0.0411939\n",
      "Step: 15640 loss: 0.0402436\n",
      "Step: 15650 loss: 0.039742\n",
      "Step: 15660 loss: 0.0420422\n",
      "Step: 15670 loss: 0.223298\n",
      "Step: 15680 loss: 0.0416543\n",
      "Step: 15690 loss: 0.0465035\n",
      "Step: 15700 loss: 0.0399897\n",
      "Step: 15710 loss: 0.0408854\n",
      "Step: 15720 loss: 0.039087\n",
      "Step: 15730 loss: 0.0400909\n",
      "Step: 15740 loss: 0.0416998\n",
      "Step: 15750 loss: 0.0406101\n",
      "Step: 15760 loss: 0.0432966\n",
      "Step: 15770 loss: 0.0395258\n",
      "Step: 15780 loss: 0.0412713\n",
      "Step: 15790 loss: 0.0412488\n",
      "Step: 15800 loss: 0.0392338\n",
      "Step: 15810 loss: 0.0389656\n",
      "Step: 15820 loss: 0.0386243\n",
      "Step: 15830 loss: 0.0386709\n",
      "Step: 15840 loss: 0.0390282\n",
      "Step: 15850 loss: 0.038579\n",
      "Step: 15860 loss: 0.0383517\n",
      "Step: 15870 loss: 0.0390859\n",
      "Step: 15880 loss: 0.0381821\n",
      "Step: 15890 loss: 0.0386432\n",
      "Step: 15900 loss: 0.0384448\n",
      "Step: 15910 loss: 0.0406052\n",
      "Step: 15920 loss: 0.037983\n",
      "Step: 15930 loss: 0.0432175\n",
      "Step: 15940 loss: 0.0426728\n",
      "Step: 15950 loss: 0.0381609\n",
      "Step: 15960 loss: 0.0381186\n",
      "Step: 15970 loss: 0.0407095\n",
      "Step: 15980 loss: 0.044561\n",
      "Step: 15990 loss: 0.0395032\n",
      "Step: 16000 loss: 0.0408961\n",
      "Saved Model\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    for j in range(num_batches):\n",
    "        path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "        word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "        \n",
    "        feed_dict = {\n",
    "            path_length:path_dict,\n",
    "            word_ids:word_dict,\n",
    "            pos_ids:pos_dict,\n",
    "            dep_ids:dep_dict,\n",
    "            y:y_dict}\n",
    "        _, loss, step = sess.run([optimizer, total_loss, global_step], feed_dict)\n",
    "        if step%10==0:\n",
    "            print(\"Step:\", step, \"loss:\",loss)\n",
    "        if step % 1000 == 0:\n",
    "            saver.save(sess, model_dir + '/model')\n",
    "            print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy 99.95\n"
     ]
    }
   ],
   "source": [
    "# training accuracy\n",
    "all_predictions = []\n",
    "for j in range(num_batches):\n",
    "    path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "    word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "\n",
    "    feed_dict = {\n",
    "        path_length:path_dict,\n",
    "        word_ids:word_dict,\n",
    "        pos_ids:pos_dict,\n",
    "        dep_ids:dep_dict,\n",
    "        y:y_dict}\n",
    "    batch_predictions = sess.run(predictions, feed_dict)\n",
    "    all_predictions.append(batch_predictions)\n",
    "\n",
    "y_pred = []\n",
    "for i in range(num_batches):\n",
    "    for pred in all_predictions[i]:\n",
    "        y_pred.append(pred)\n",
    "\n",
    "count = 0\n",
    "for i in range(batch_size*num_batches):\n",
    "    count += y_pred[i]==rel_ids[i]\n",
    "accuracy = count/(batch_size*num_batches) * 100\n",
    "\n",
    "print(\"training accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('data/test_paths', 'rb')\n",
    "word_p1, word_p2, dep_p1, dep_p2, pos_p1, pos_p2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "relations = []\n",
    "for line in open('data/test_relations.txt'):\n",
    "    relations.append(line.strip().split()[0])\n",
    "\n",
    "length = len(word_p1)\n",
    "num_batches = int(length/batch_size)\n",
    "\n",
    "for i in range(length):\n",
    "    for j, word in enumerate(word_p1[i]):\n",
    "        word = word.lower()\n",
    "        word_p1[i][j] = word if word in word2id else unknown_token \n",
    "    for k, word in enumerate(word_p2[i]):\n",
    "        word = word.lower()\n",
    "        word_p2[i][k] = word if word in word2id else unknown_token \n",
    "    for l, d in enumerate(dep_p1[i]):\n",
    "        dep_p1[i][l] = d if d in dep2id else 'OTH'\n",
    "    for m, d in enumerate(dep_p2[i]):\n",
    "        dep_p2[i][m] = d if d in dep2id else 'OTH'\n",
    "\n",
    "word_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "word_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "rel_ids = np.array([rel2id[rel] for rel in relations])\n",
    "path1_len = np.array([len(w) for w in word_p1], dtype=int)\n",
    "path2_len = np.array([len(w) for w in word_p2])\n",
    "\n",
    "for i in range(length):\n",
    "    for j, w in enumerate(word_p1[i]):\n",
    "        word_p1_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(word_p2[i]):\n",
    "        word_p2_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(pos_p1[i]):\n",
    "        pos_p1_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(pos_p2[i]):\n",
    "        pos_p2_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(dep_p1[i]):\n",
    "        dep_p1_ids[i][j] = dep2id[w]\n",
    "    for j, w in enumerate(dep_p2[i]):\n",
    "        dep_p2_ids[i][j] = dep2id[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 65.5350553506\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "all_predictions = []\n",
    "for j in range(num_batches):\n",
    "    path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "    word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "\n",
    "    feed_dict = {\n",
    "        path_length:path_dict,\n",
    "        word_ids:word_dict,\n",
    "        pos_ids:pos_dict,\n",
    "        dep_ids:dep_dict,\n",
    "        y:y_dict}\n",
    "    batch_predictions = sess.run(predictions, feed_dict)\n",
    "    all_predictions.append(batch_predictions)\n",
    "\n",
    "y_pred = []\n",
    "for i in range(num_batches):\n",
    "    for pred in all_predictions[i]:\n",
    "        y_pred.append(pred)\n",
    "\n",
    "count = 0\n",
    "for i in range(batch_size*num_batches):\n",
    "    count += y_pred[i]==rel_ids[i]\n",
    "accuracy = count/(batch_size*num_batches) * 100\n",
    "\n",
    "print(\"test accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, _pickle as pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "ckpt_dir = 'checkpoint'\n",
    "word_embd_dir = 'checkpoint/word_embd'\n",
    "model_dir = 'checkpoint/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embd_dim = 100\n",
    "pos_embd_dim = 25\n",
    "dep_embd_dim = 25\n",
    "word_vocab_size = 400001\n",
    "pos_vocab_size = 10\n",
    "dep_vocab_size = 21\n",
    "relation_classes = 19\n",
    "word_state_size = 100\n",
    "other_state_size = 100\n",
    "batch_size = 10\n",
    "channels = 3\n",
    "lambda_l2 = 0.0001\n",
    "max_len_path = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"input\"):\n",
    "    path_length = tf.placeholder(tf.int32, shape=[2, batch_size], name=\"path1_length\")\n",
    "    word_ids = tf.placeholder(tf.int32, shape=[2, batch_size, max_len_path], name=\"word_ids\")\n",
    "    pos_ids = tf.placeholder(tf.int32, [2, batch_size, max_len_path], name=\"pos_ids\")\n",
    "    dep_ids = tf.placeholder(tf.int32, [2, batch_size, max_len_path], name=\"dep_ids\")\n",
    "    y = tf.placeholder(tf.int32, [batch_size], name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"word_embedding\"):\n",
    "    W = tf.Variable(tf.constant(0.0, shape=[word_vocab_size, word_embd_dim]), name=\"W\")\n",
    "    embedding_placeholder = tf.placeholder(tf.float32,[word_vocab_size, word_embd_dim])\n",
    "    embedding_init = W.assign(embedding_placeholder)\n",
    "    embedded_word = tf.nn.embedding_lookup(W, word_ids)\n",
    "    word_embedding_saver = tf.train.Saver({\"word_embedding/W\": W})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"pos_embedding\"):\n",
    "    W = tf.Variable(tf.random_uniform([pos_vocab_size, pos_embd_dim]), name=\"W\")\n",
    "    embedded_pos = tf.nn.embedding_lookup(W, pos_ids)\n",
    "    pos_embedding_saver = tf.train.Saver({\"pos_embedding/W\": W})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dep_embedding\"):\n",
    "    W = tf.Variable(tf.random_uniform([dep_vocab_size, dep_embd_dim]), name=\"W\")\n",
    "    embedded_dep = tf.nn.embedding_lookup(W, dep_ids)\n",
    "    dep_embedding_saver = tf.train.Saver({\"dep_embedding/W\": W})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_hidden_state = tf.zeros([batch_size, word_state_size], name='word_hidden_state')\n",
    "word_cell_state = tf.zeros([batch_size, word_state_size], name='word_cell_state')\n",
    "word_init_state = tf.contrib.rnn.LSTMStateTuple(word_hidden_state, word_cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_hidden_states = tf.zeros([channels-1, batch_size, other_state_size], name=\"hidden_state\")\n",
    "other_cell_states = tf.zeros([channels-1, batch_size, other_state_size], name=\"cell_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_init_states = [tf.contrib.rnn.LSTMStateTuple(other_hidden_states[i], other_cell_states[i]) for i in range(channels-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"word_lstm1\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(word_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_word[0], sequence_length=path_length[0], initial_state=word_init_state)\n",
    "    state_series_word1 = tf.reduce_max(state_series, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"word_lstm2\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(word_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_word[1], sequence_length=path_length[1], initial_state=word_init_state)\n",
    "    state_series_word2 = tf.reduce_max(state_series, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"pos_lstm1\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_pos[0], sequence_length=path_length[0],initial_state=other_init_states[0])\n",
    "    state_series_pos1 = tf.reduce_max(state_series, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"pos_lstm2\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_pos[1], sequence_length=path_length[1],initial_state=other_init_states[0])\n",
    "    state_series_pos2 = tf.reduce_max(state_series, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"dep_lstm1\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_dep[0], sequence_length=path_length[0], initial_state=other_init_states[1])\n",
    "    state_series_dep1 = tf.reduce_max(state_series, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"dep_lstm2\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_dep[1], sequence_length=path_length[1], initial_state=other_init_states[1])\n",
    "    state_series_dep2 = tf.reduce_max(state_series, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_series1 = tf.concat([state_series_word1, state_series_pos1, state_series_dep1], 1)\n",
    "state_series2 = tf.concat([state_series_word2, state_series_pos2, state_series_dep2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_series = tf.concat([state_series1, state_series2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat_2:0' shape=(10, 600) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"hidden_layer\"):\n",
    "    W = tf.Variable(tf.truncated_normal([600, 100], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([100]), name=\"b\")\n",
    "    y_hidden_layer = tf.matmul(state_series, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"softmax_layer\"):\n",
    "    W = tf.Variable(tf.truncated_normal([100, relation_classes], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([relation_classes]), name=\"b\")\n",
    "    logits = tf.matmul(y_hidden_layer, W) + b\n",
    "    predictions = tf.argmax(logits, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name=\"global_step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tv_all = tf.trainable_variables()\n",
    "tv_regu = []\n",
    "non_reg = [\"word_embedding/W:0\",\"pos_embedding/W:0\",'dep_embedding/W:0',\"global_step:0\",'hidden_layer/b:0','softmax_layer/b:0']\n",
    "for t in tv_all:\n",
    "    if t.name not in non_reg:\n",
    "        if(t.name.find('biases')==-1):\n",
    "            tv_regu.append(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    l2_loss = lambda_l2 * tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv_regu ])\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    total_loss = loss + l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions = tf.equal(predictions, tf.arg_max(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(total_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('data/vocab.pkl', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f = open('data/word_embedding', 'rb')\n",
    "# word_embedding = pickle.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "word2id[unknown_token] = word_vocab_size -1\n",
    "id2word[word_vocab_size-1] = unknown_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('data/train_paths', 'rb')\n",
    "word_p1, word_p2, dep_p1, dep_p2, pos_p1, pos_p2 = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relations = []\n",
    "for line in open('data/train_relation.txt'):\n",
    "    relations.append(line.strip().split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches = int(8000/batch_size)\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tags_vocab = []\n",
    "for line in open('data/pos_tags.txt'):\n",
    "        pos_tags_vocab.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep_vocab = []\n",
    "for line in open('data/dependency_types.txt'):\n",
    "    dep_vocab.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relation_vocab = []\n",
    "for line in open('data/relation_types.txt'):\n",
    "    relation_vocab.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rel2id = dict((w, i) for i,w in enumerate(relation_vocab))\n",
    "id2rel = dict((i, w) for i,w in enumerate(relation_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tag2id = dict((w, i) for i,w in enumerate(pos_tags_vocab))\n",
    "id2pos_tag = dict((i, w) for i,w in enumerate(pos_tags_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep2id = dict((w, i) for i,w in enumerate(dep_vocab))\n",
    "id2dep = dict((i, w) for i,w in enumerate(dep_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tag2id['OTH'] = 9\n",
    "id2pos_tag[9] = 'OTH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep2id['OTH'] = 20\n",
    "id2dep[20] = 'OTH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JJ_pos_tags = ['JJ', 'JJR', 'JJS']\n",
    "NN_pos_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "RB_pos_tags = ['RB', 'RBR', 'RBS']\n",
    "PRP_pos_tags = ['PRP', 'PRP$']\n",
    "VB_pos_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "_pos_tags = ['CC', 'CD', 'DT', 'IN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def pos_tag(x):\n",
    "    if x in JJ_pos_tags:\n",
    "        return pos_tag2id['JJ']\n",
    "    if x in NN_pos_tags:\n",
    "        return pos_tag2id['NN']\n",
    "    if x in RB_pos_tags:\n",
    "        return pos_tag2id['RB']\n",
    "    if x in PRP_pos_tags:\n",
    "        return pos_tag2id['PRP']\n",
    "    if x in VB_pos_tags:\n",
    "        return pos_tag2id['VB']\n",
    "    if x in _pos_tags:\n",
    "        return pos_tag2id[x]\n",
    "    else:\n",
    "        return 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep = []\n",
    "for i in range(8000):\n",
    "    dep.append(dep_p1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(8000):\n",
    "    dep.append(dep_p2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep_freq = nltk.FreqDist(chain(*dep))\n",
    "dep_types = dep_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(8000):\n",
    "    for j, word in enumerate(word_p1[i]):\n",
    "        word = word.lower()\n",
    "        word_p1[i][j] = word if word in word2id else unknown_token \n",
    "    for k, word in enumerate(word_p2[i]):\n",
    "        word = word.lower()\n",
    "        word_p2[i][k] = word if word in word2id else unknown_token \n",
    "    for l, d in enumerate(dep_p1[i]):\n",
    "        dep_p1[i][l] = d if d in dep2id else 'OTH'\n",
    "    for m, d in enumerate(dep_p2[i]):\n",
    "        dep_p2[i][m] = d if d in dep2id else 'OTH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_p1_ids = np.ones([8000, max_len_path],dtype=int)\n",
    "word_p2_ids = np.ones([8000, max_len_path],dtype=int)\n",
    "pos_p1_ids = np.ones([8000, max_len_path],dtype=int)\n",
    "pos_p2_ids = np.ones([8000, max_len_path],dtype=int)\n",
    "dep_p1_ids = np.ones([8000, max_len_path],dtype=int)\n",
    "dep_p2_ids = np.ones([8000, max_len_path],dtype=int)\n",
    "rel_ids = np.array([rel2id[rel] for rel in relations])\n",
    "path1_len = np.array([len(w) for w in word_p1], dtype=int)\n",
    "path2_len = np.array([len(w) for w in word_p2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(8000):\n",
    "    for j, w in enumerate(word_p1[i]):\n",
    "        word_p1_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(word_p2[i]):\n",
    "        word_p2_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(pos_p1[i]):\n",
    "        pos_p1_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(pos_p2[i]):\n",
    "        pos_p2_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(dep_p1[i]):\n",
    "        dep_p1_ids[i][j] = dep2id[w]\n",
    "    for j, w in enumerate(dep_p2[i]):\n",
    "        dep_p2_ids[i][j] = dep2id[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess.run(embedding_init, feed_dict={embedding_placeholder:word_embedding})\n",
    "# word_embedding_saver.save(sess, word_embd_dir + '/word_embd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint/model/model-8000\n"
     ]
    }
   ],
   "source": [
    "model = tf.train.latest_checkpoint(model_dir)\n",
    "saver.restore(sess, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# latest_embd = tf.train.latest_checkpoint(word_embd_dir)\n",
    "# word_embedding_saver.restore(sess, latest_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10 loss: 3.07525\n",
      "Step: 20 loss: 2.91253\n",
      "Step: 30 loss: 2.77473\n",
      "Step: 40 loss: 2.72092\n",
      "Step: 50 loss: 2.94963\n",
      "Step: 60 loss: 2.64186\n",
      "Step: 70 loss: 2.49993\n",
      "Step: 80 loss: 2.45959\n",
      "Step: 90 loss: 2.52036\n",
      "Step: 100 loss: 2.30564\n",
      "Saved Model\n",
      "Step: 110 loss: 2.24492\n",
      "Step: 120 loss: 2.48354\n",
      "Step: 130 loss: 1.99022\n",
      "Step: 140 loss: 1.59583\n",
      "Step: 150 loss: 2.18596\n",
      "Step: 160 loss: 2.39128\n",
      "Step: 170 loss: 1.92281\n",
      "Step: 180 loss: 2.26467\n",
      "Step: 190 loss: 2.24388\n",
      "Step: 200 loss: 2.50149\n",
      "Saved Model\n",
      "Step: 210 loss: 1.8673\n",
      "Step: 220 loss: 1.57685\n",
      "Step: 230 loss: 2.0235\n",
      "Step: 240 loss: 1.47464\n",
      "Step: 250 loss: 1.51098\n",
      "Step: 260 loss: 1.91565\n",
      "Step: 270 loss: 1.46834\n",
      "Step: 280 loss: 1.24425\n",
      "Step: 290 loss: 1.74657\n",
      "Step: 300 loss: 1.32431\n",
      "Saved Model\n",
      "Step: 310 loss: 1.72367\n",
      "Step: 320 loss: 1.5951\n",
      "Step: 330 loss: 1.41446\n",
      "Step: 340 loss: 2.0474\n",
      "Step: 350 loss: 2.63629\n",
      "Step: 360 loss: 1.9582\n",
      "Step: 370 loss: 1.17965\n",
      "Step: 380 loss: 1.48109\n",
      "Step: 390 loss: 1.8047\n",
      "Step: 400 loss: 1.37329\n",
      "Saved Model\n",
      "Step: 410 loss: 1.82049\n",
      "Step: 420 loss: 1.34931\n",
      "Step: 430 loss: 0.905337\n",
      "Step: 440 loss: 0.486882\n",
      "Step: 450 loss: 1.1473\n",
      "Step: 460 loss: 1.42686\n",
      "Step: 470 loss: 1.53201\n",
      "Step: 480 loss: 1.66664\n",
      "Step: 490 loss: 1.32563\n",
      "Step: 500 loss: 1.48341\n",
      "Saved Model\n",
      "Step: 510 loss: 2.16655\n",
      "Step: 520 loss: 1.32586\n",
      "Step: 530 loss: 1.14583\n",
      "Step: 540 loss: 1.09127\n",
      "Step: 550 loss: 2.08901\n",
      "Step: 560 loss: 1.17626\n",
      "Step: 570 loss: 1.10782\n",
      "Step: 580 loss: 1.70686\n",
      "Step: 590 loss: 1.694\n",
      "Step: 600 loss: 1.32643\n",
      "Saved Model\n",
      "Step: 610 loss: 1.0354\n",
      "Step: 620 loss: 0.994441\n",
      "Step: 630 loss: 1.04597\n",
      "Step: 640 loss: 0.952827\n",
      "Step: 650 loss: 0.592779\n",
      "Step: 660 loss: 0.910191\n",
      "Step: 670 loss: 0.864144\n",
      "Step: 680 loss: 0.930762\n",
      "Step: 690 loss: 0.611621\n",
      "Step: 700 loss: 1.29839\n",
      "Saved Model\n",
      "Step: 710 loss: 1.75311\n",
      "Step: 720 loss: 1.59369\n",
      "Step: 730 loss: 3.20955\n",
      "Step: 740 loss: 1.54563\n",
      "Step: 750 loss: 0.689956\n",
      "Step: 760 loss: 1.49046\n",
      "Step: 770 loss: 1.89159\n",
      "Step: 780 loss: 1.27739\n",
      "Step: 790 loss: 0.72977\n",
      "Step: 800 loss: 1.42931\n",
      "Saved Model\n",
      "Step: 810 loss: 0.766475\n",
      "Step: 820 loss: 1.54402\n",
      "Step: 830 loss: 1.64733\n",
      "Step: 840 loss: 1.01568\n",
      "Step: 850 loss: 1.45985\n",
      "Step: 860 loss: 0.990958\n",
      "Step: 870 loss: 1.01442\n",
      "Step: 880 loss: 0.782171\n",
      "Step: 890 loss: 0.766621\n",
      "Step: 900 loss: 0.853557\n",
      "Saved Model\n",
      "Step: 910 loss: 1.07314\n",
      "Step: 920 loss: 1.26031\n",
      "Step: 930 loss: 0.309086\n",
      "Step: 940 loss: 0.341793\n",
      "Step: 950 loss: 0.958591\n",
      "Step: 960 loss: 1.21482\n",
      "Step: 970 loss: 0.757277\n",
      "Step: 980 loss: 1.1368\n",
      "Step: 990 loss: 1.00275\n",
      "Step: 1000 loss: 1.30543\n",
      "Saved Model\n",
      "Step: 1010 loss: 0.715721\n",
      "Step: 1020 loss: 0.61269\n",
      "Step: 1030 loss: 0.974784\n",
      "Step: 1040 loss: 0.673508\n",
      "Step: 1050 loss: 0.961388\n",
      "Step: 1060 loss: 0.8791\n",
      "Step: 1070 loss: 0.515027\n",
      "Step: 1080 loss: 0.589934\n",
      "Step: 1090 loss: 0.462053\n",
      "Step: 1100 loss: 0.79358\n",
      "Saved Model\n",
      "Step: 1110 loss: 1.11594\n",
      "Step: 1120 loss: 0.696955\n",
      "Step: 1130 loss: 0.72165\n",
      "Step: 1140 loss: 1.08053\n",
      "Step: 1150 loss: 1.82407\n",
      "Step: 1160 loss: 0.884877\n",
      "Step: 1170 loss: 0.626989\n",
      "Step: 1180 loss: 0.729721\n",
      "Step: 1190 loss: 1.13808\n",
      "Step: 1200 loss: 0.829362\n",
      "Saved Model\n",
      "Step: 1210 loss: 1.17495\n",
      "Step: 1220 loss: 0.585132\n",
      "Step: 1230 loss: 0.38131\n",
      "Step: 1240 loss: 0.32579\n",
      "Step: 1250 loss: 0.536989\n",
      "Step: 1260 loss: 0.686344\n",
      "Step: 1270 loss: 0.742269\n",
      "Step: 1280 loss: 1.16245\n",
      "Step: 1290 loss: 0.84998\n",
      "Step: 1300 loss: 0.657265\n",
      "Saved Model\n",
      "Step: 1310 loss: 1.65294\n",
      "Step: 1320 loss: 0.547858\n",
      "Step: 1330 loss: 0.513946\n",
      "Step: 1340 loss: 0.507074\n",
      "Step: 1350 loss: 1.3187\n",
      "Step: 1360 loss: 0.618727\n",
      "Step: 1370 loss: 0.766661\n",
      "Step: 1380 loss: 0.902539\n",
      "Step: 1390 loss: 1.05801\n",
      "Step: 1400 loss: 0.776497\n",
      "Saved Model\n",
      "Step: 1410 loss: 0.528699\n",
      "Step: 1420 loss: 0.620719\n",
      "Step: 1430 loss: 0.728061\n",
      "Step: 1440 loss: 0.648551\n",
      "Step: 1450 loss: 0.333325\n",
      "Step: 1460 loss: 0.286899\n",
      "Step: 1470 loss: 0.605039\n",
      "Step: 1480 loss: 0.588017\n",
      "Step: 1490 loss: 0.384864\n",
      "Step: 1500 loss: 0.667893\n",
      "Saved Model\n",
      "Step: 1510 loss: 0.984711\n",
      "Step: 1520 loss: 0.721608\n",
      "Step: 1530 loss: 2.56707\n",
      "Step: 1540 loss: 1.24763\n",
      "Step: 1550 loss: 0.389697\n",
      "Step: 1560 loss: 0.958236\n",
      "Step: 1570 loss: 1.21219\n",
      "Step: 1580 loss: 0.775051\n",
      "Step: 1590 loss: 0.334032\n",
      "Step: 1600 loss: 1.05161\n",
      "Saved Model\n",
      "Step: 1610 loss: 0.450637\n",
      "Step: 1620 loss: 0.992637\n",
      "Step: 1630 loss: 1.20627\n",
      "Step: 1640 loss: 0.93114\n",
      "Step: 1650 loss: 0.886337\n",
      "Step: 1660 loss: 0.526195\n",
      "Step: 1670 loss: 0.742635\n",
      "Step: 1680 loss: 0.575336\n",
      "Step: 1690 loss: 0.419496\n",
      "Step: 1700 loss: 0.611597\n",
      "Saved Model\n",
      "Step: 1710 loss: 0.828476\n",
      "Step: 1720 loss: 1.01192\n",
      "Step: 1730 loss: 0.215793\n",
      "Step: 1740 loss: 0.194654\n",
      "Step: 1750 loss: 0.483049\n",
      "Step: 1760 loss: 1.10852\n",
      "Step: 1770 loss: 0.453156\n",
      "Step: 1780 loss: 0.745703\n",
      "Step: 1790 loss: 0.55084\n",
      "Step: 1800 loss: 0.659754\n",
      "Saved Model\n",
      "Step: 1810 loss: 0.286923\n",
      "Step: 1820 loss: 0.290807\n",
      "Step: 1830 loss: 0.631747\n",
      "Step: 1840 loss: 0.64294\n",
      "Step: 1850 loss: 0.604223\n",
      "Step: 1860 loss: 0.366766\n",
      "Step: 1870 loss: 0.314054\n",
      "Step: 1880 loss: 0.398538\n",
      "Step: 1890 loss: 0.339665\n",
      "Step: 1900 loss: 0.627004\n",
      "Saved Model\n",
      "Step: 1910 loss: 0.382852\n",
      "Step: 1920 loss: 0.491668\n",
      "Step: 1930 loss: 0.374526\n",
      "Step: 1940 loss: 0.544264\n",
      "Step: 1950 loss: 1.08841\n",
      "Step: 1960 loss: 0.390202\n",
      "Step: 1970 loss: 0.284221\n",
      "Step: 1980 loss: 0.286228\n",
      "Step: 1990 loss: 0.982587\n",
      "Step: 2000 loss: 0.6591\n",
      "Saved Model\n",
      "Step: 2010 loss: 0.513919\n",
      "Step: 2020 loss: 0.294193\n",
      "Step: 2030 loss: 0.211178\n",
      "Step: 2040 loss: 0.214718\n",
      "Step: 2050 loss: 0.260789\n",
      "Step: 2060 loss: 0.405221\n",
      "Step: 2070 loss: 0.437248\n",
      "Step: 2080 loss: 0.863954\n",
      "Step: 2090 loss: 0.578984\n",
      "Step: 2100 loss: 0.327176\n",
      "Saved Model\n",
      "Step: 2110 loss: 1.03481\n",
      "Step: 2120 loss: 0.29652\n",
      "Step: 2130 loss: 0.266237\n",
      "Step: 2140 loss: 0.342018\n",
      "Step: 2150 loss: 0.789311\n",
      "Step: 2160 loss: 0.407151\n",
      "Step: 2170 loss: 0.469072\n",
      "Step: 2180 loss: 0.571944\n",
      "Step: 2190 loss: 0.621966\n",
      "Step: 2200 loss: 0.509288\n",
      "Saved Model\n",
      "Step: 2210 loss: 0.370482\n",
      "Step: 2220 loss: 0.344702\n",
      "Step: 2230 loss: 0.40795\n",
      "Step: 2240 loss: 0.440775\n",
      "Step: 2250 loss: 0.201882\n",
      "Step: 2260 loss: 0.159303\n",
      "Step: 2270 loss: 0.327429\n",
      "Step: 2280 loss: 0.378335\n",
      "Step: 2290 loss: 0.204222\n",
      "Step: 2300 loss: 0.221457\n",
      "Saved Model\n",
      "Step: 2310 loss: 0.695112\n",
      "Step: 2320 loss: 0.261645\n",
      "Step: 2330 loss: 1.48118\n",
      "Step: 2340 loss: 0.894055\n",
      "Step: 2350 loss: 0.243542\n",
      "Step: 2360 loss: 0.363756\n",
      "Step: 2370 loss: 0.67163\n",
      "Step: 2380 loss: 0.512359\n",
      "Step: 2390 loss: 0.172816\n",
      "Step: 2400 loss: 0.772859\n",
      "Saved Model\n",
      "Step: 2410 loss: 0.293533\n",
      "Step: 2420 loss: 0.471467\n",
      "Step: 2430 loss: 0.755119\n",
      "Step: 2440 loss: 0.887704\n",
      "Step: 2450 loss: 0.434094\n",
      "Step: 2460 loss: 0.176593\n",
      "Step: 2470 loss: 0.548155\n",
      "Step: 2480 loss: 0.49512\n",
      "Step: 2490 loss: 0.236266\n",
      "Step: 2500 loss: 0.383757\n",
      "Saved Model\n",
      "Step: 2510 loss: 0.447226\n",
      "Step: 2520 loss: 0.77145\n",
      "Step: 2530 loss: 0.199518\n",
      "Step: 2540 loss: 0.134243\n",
      "Step: 2550 loss: 0.247173\n",
      "Step: 2560 loss: 0.944799\n",
      "Step: 2570 loss: 0.237775\n",
      "Step: 2580 loss: 0.385763\n",
      "Step: 2590 loss: 0.27899\n",
      "Step: 2600 loss: 0.252164\n",
      "Saved Model\n",
      "Step: 2610 loss: 0.142223\n",
      "Step: 2620 loss: 0.144689\n",
      "Step: 2630 loss: 0.370868\n",
      "Step: 2640 loss: 0.430073\n",
      "Step: 2650 loss: 0.392105\n",
      "Step: 2660 loss: 0.181566\n",
      "Step: 2670 loss: 0.143683\n",
      "Step: 2680 loss: 0.271882\n",
      "Step: 2690 loss: 0.233368\n",
      "Step: 2700 loss: 0.37115\n",
      "Saved Model\n",
      "Step: 2710 loss: 0.325873\n",
      "Step: 2720 loss: 0.230817\n",
      "Step: 2730 loss: 0.220824\n",
      "Step: 2740 loss: 0.321605\n",
      "Step: 2750 loss: 0.595004\n",
      "Step: 2760 loss: 0.186304\n",
      "Step: 2770 loss: 0.132476\n",
      "Step: 2780 loss: 0.200006\n",
      "Step: 2790 loss: 0.929271\n",
      "Step: 2800 loss: 0.420693\n",
      "Saved Model\n",
      "Step: 2810 loss: 0.238178\n",
      "Step: 2820 loss: 0.225746\n",
      "Step: 2830 loss: 0.160605\n",
      "Step: 2840 loss: 0.109943\n",
      "Step: 2850 loss: 0.157247\n",
      "Step: 2860 loss: 0.184158\n",
      "Step: 2870 loss: 0.274193\n",
      "Step: 2880 loss: 0.483922\n",
      "Step: 2890 loss: 0.330795\n",
      "Step: 2900 loss: 0.170109\n",
      "Saved Model\n",
      "Step: 2910 loss: 0.450064\n",
      "Step: 2920 loss: 0.170654\n",
      "Step: 2930 loss: 0.239422\n",
      "Step: 2940 loss: 0.150746\n",
      "Step: 2950 loss: 0.40505\n",
      "Step: 2960 loss: 0.206158\n",
      "Step: 2970 loss: 0.173002\n",
      "Step: 2980 loss: 0.382467\n",
      "Step: 2990 loss: 0.27754\n",
      "Step: 3000 loss: 0.24464\n",
      "Saved Model\n",
      "Step: 3010 loss: 0.250923\n",
      "Step: 3020 loss: 0.126031\n",
      "Step: 3030 loss: 0.300529\n",
      "Step: 3040 loss: 0.232822\n",
      "Step: 3050 loss: 0.210954\n",
      "Step: 3060 loss: 0.114377\n",
      "Step: 3070 loss: 0.164272\n",
      "Step: 3080 loss: 0.284575\n",
      "Step: 3090 loss: 0.128512\n",
      "Step: 3100 loss: 0.117331\n",
      "Saved Model\n",
      "Step: 3110 loss: 0.513798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3120 loss: 0.12847\n",
      "Step: 3130 loss: 0.596706\n",
      "Step: 3140 loss: 0.634581\n",
      "Step: 3150 loss: 0.164035\n",
      "Step: 3160 loss: 0.103016\n",
      "Step: 3170 loss: 0.226977\n",
      "Step: 3180 loss: 0.31817\n",
      "Step: 3190 loss: 0.117716\n",
      "Step: 3200 loss: 0.445029\n",
      "Saved Model\n",
      "Step: 3210 loss: 0.178751\n",
      "Step: 3220 loss: 0.290206\n",
      "Step: 3230 loss: 0.42809\n",
      "Step: 3240 loss: 0.626906\n",
      "Step: 3250 loss: 0.180218\n",
      "Step: 3260 loss: 0.107408\n",
      "Step: 3270 loss: 0.314676\n",
      "Step: 3280 loss: 0.319207\n",
      "Step: 3290 loss: 0.203178\n",
      "Step: 3300 loss: 0.171296\n",
      "Saved Model\n",
      "Step: 3310 loss: 0.195655\n",
      "Step: 3320 loss: 0.42697\n",
      "Step: 3330 loss: 0.161593\n",
      "Step: 3340 loss: 0.0945418\n",
      "Step: 3350 loss: 0.14084\n",
      "Step: 3360 loss: 0.757847\n",
      "Step: 3370 loss: 0.137735\n",
      "Step: 3380 loss: 0.278729\n",
      "Step: 3390 loss: 0.152857\n",
      "Step: 3400 loss: 0.127166\n",
      "Saved Model\n",
      "Step: 3410 loss: 0.0983252\n",
      "Step: 3420 loss: 0.102324\n",
      "Step: 3430 loss: 0.190508\n",
      "Step: 3440 loss: 0.20741\n",
      "Step: 3450 loss: 0.196798\n",
      "Step: 3460 loss: 0.113721\n",
      "Step: 3470 loss: 0.12239\n",
      "Step: 3480 loss: 0.208887\n",
      "Step: 3490 loss: 0.15604\n",
      "Step: 3500 loss: 0.146444\n",
      "Saved Model\n",
      "Step: 3510 loss: 0.152348\n",
      "Step: 3520 loss: 0.109812\n",
      "Step: 3530 loss: 0.142623\n",
      "Step: 3540 loss: 0.270057\n",
      "Step: 3550 loss: 0.18792\n",
      "Step: 3560 loss: 0.110655\n",
      "Step: 3570 loss: 0.116918\n",
      "Step: 3580 loss: 0.130807\n",
      "Step: 3590 loss: 0.540215\n",
      "Step: 3600 loss: 0.24227\n",
      "Saved Model\n",
      "Step: 3610 loss: 0.12121\n",
      "Step: 3620 loss: 0.131239\n",
      "Step: 3630 loss: 0.175719\n",
      "Step: 3640 loss: 0.092931\n",
      "Step: 3650 loss: 0.116544\n",
      "Step: 3660 loss: 0.107074\n",
      "Step: 3670 loss: 0.341621\n",
      "Step: 3680 loss: 0.136717\n",
      "Step: 3690 loss: 0.135723\n",
      "Step: 3700 loss: 0.137495\n",
      "Saved Model\n",
      "Step: 3710 loss: 0.181055\n",
      "Step: 3720 loss: 0.114276\n",
      "Step: 3730 loss: 0.137013\n",
      "Step: 3740 loss: 0.100489\n",
      "Step: 3750 loss: 0.179575\n",
      "Step: 3760 loss: 0.144016\n",
      "Step: 3770 loss: 0.113525\n",
      "Step: 3780 loss: 0.213537\n",
      "Step: 3790 loss: 0.155479\n",
      "Step: 3800 loss: 0.121365\n",
      "Saved Model\n",
      "Step: 3810 loss: 0.110899\n",
      "Step: 3820 loss: 0.15684\n",
      "Step: 3830 loss: 0.124719\n",
      "Step: 3840 loss: 0.170428\n",
      "Step: 3850 loss: 0.363722\n",
      "Step: 3860 loss: 0.0939359\n",
      "Step: 3870 loss: 0.11143\n",
      "Step: 3880 loss: 0.117087\n",
      "Step: 3890 loss: 0.102522\n",
      "Step: 3900 loss: 0.0966517\n",
      "Saved Model\n",
      "Step: 3910 loss: 0.235681\n",
      "Step: 3920 loss: 0.0962533\n",
      "Step: 3930 loss: 0.395485\n",
      "Step: 3940 loss: 0.21123\n",
      "Step: 3950 loss: 0.190456\n",
      "Step: 3960 loss: 0.103916\n",
      "Step: 3970 loss: 0.145652\n",
      "Step: 3980 loss: 0.217209\n",
      "Step: 3990 loss: 0.116789\n",
      "Step: 4000 loss: 0.245702\n",
      "Saved Model\n",
      "Step: 4010 loss: 0.179513\n",
      "Step: 4020 loss: 0.221728\n",
      "Step: 4030 loss: 0.211475\n",
      "Step: 4040 loss: 0.400789\n",
      "Step: 4050 loss: 0.12424\n",
      "Step: 4060 loss: 0.0923346\n",
      "Step: 4070 loss: 0.125124\n",
      "Step: 4080 loss: 0.13262\n",
      "Step: 4090 loss: 0.107405\n",
      "Step: 4100 loss: 0.111169\n",
      "Saved Model\n",
      "Step: 4110 loss: 0.115167\n",
      "Step: 4120 loss: 0.213463\n",
      "Step: 4130 loss: 0.122309\n",
      "Step: 4140 loss: 0.0875439\n",
      "Step: 4150 loss: 0.0997348\n",
      "Step: 4160 loss: 0.499879\n",
      "Step: 4170 loss: 0.0903824\n",
      "Step: 4180 loss: 0.282771\n",
      "Step: 4190 loss: 0.113217\n",
      "Step: 4200 loss: 0.0933959\n",
      "Saved Model\n",
      "Step: 4210 loss: 0.0864649\n",
      "Step: 4220 loss: 0.0972626\n",
      "Step: 4230 loss: 0.101178\n",
      "Step: 4240 loss: 0.119482\n",
      "Step: 4250 loss: 0.116869\n",
      "Step: 4260 loss: 0.0952576\n",
      "Step: 4270 loss: 0.142802\n",
      "Step: 4280 loss: 0.107525\n",
      "Step: 4290 loss: 0.153579\n",
      "Step: 4300 loss: 0.103071\n",
      "Saved Model\n",
      "Step: 4310 loss: 0.0941334\n",
      "Step: 4320 loss: 0.0982998\n",
      "Step: 4330 loss: 0.112442\n",
      "Step: 4340 loss: 0.175185\n",
      "Step: 4350 loss: 0.110875\n",
      "Step: 4360 loss: 0.0875001\n",
      "Step: 4370 loss: 0.0970892\n",
      "Step: 4380 loss: 0.0990833\n",
      "Step: 4390 loss: 0.281691\n",
      "Step: 4400 loss: 0.154454\n",
      "Saved Model\n",
      "Step: 4410 loss: 0.0900967\n",
      "Step: 4420 loss: 0.102378\n",
      "Step: 4430 loss: 0.104772\n",
      "Step: 4440 loss: 0.0893965\n",
      "Step: 4450 loss: 0.107889\n",
      "Step: 4460 loss: 0.0905827\n",
      "Step: 4470 loss: 0.342065\n",
      "Step: 4480 loss: 0.117637\n",
      "Step: 4490 loss: 0.0963046\n",
      "Step: 4500 loss: 0.108462\n",
      "Saved Model\n",
      "Step: 4510 loss: 0.124985\n",
      "Step: 4520 loss: 0.0949431\n",
      "Step: 4530 loss: 0.110121\n",
      "Step: 4540 loss: 0.092474\n",
      "Step: 4550 loss: 0.13412\n",
      "Step: 4560 loss: 0.116099\n",
      "Step: 4570 loss: 0.0890526\n",
      "Step: 4580 loss: 0.109383\n",
      "Step: 4590 loss: 0.111188\n",
      "Step: 4600 loss: 0.105192\n",
      "Saved Model\n",
      "Step: 4610 loss: 0.0936599\n",
      "Step: 4620 loss: 0.0839247\n",
      "Step: 4630 loss: 0.0904518\n",
      "Step: 4640 loss: 0.0980637\n",
      "Step: 4650 loss: 0.093832\n",
      "Step: 4660 loss: 0.0851475\n",
      "Step: 4670 loss: 0.197078\n",
      "Step: 4680 loss: 0.0984405\n",
      "Step: 4690 loss: 0.0843055\n",
      "Step: 4700 loss: 0.090023\n",
      "Saved Model\n",
      "Step: 4710 loss: 0.137474\n",
      "Step: 4720 loss: 0.0861596\n",
      "Step: 4730 loss: 0.262351\n",
      "Step: 4740 loss: 0.202621\n",
      "Step: 4750 loss: 0.138327\n",
      "Step: 4760 loss: 0.349831\n",
      "Step: 4770 loss: 0.201863\n",
      "Step: 4780 loss: 0.184715\n",
      "Step: 4790 loss: 0.236593\n",
      "Step: 4800 loss: 0.128718\n",
      "Saved Model\n",
      "Step: 4810 loss: 0.0948605\n",
      "Step: 4820 loss: 0.1398\n",
      "Step: 4830 loss: 0.112583\n",
      "Step: 4840 loss: 0.12017\n",
      "Step: 4850 loss: 0.10744\n",
      "Step: 4860 loss: 0.0894863\n",
      "Step: 4870 loss: 0.0894861\n",
      "Step: 4880 loss: 0.0988091\n",
      "Step: 4890 loss: 0.105973\n",
      "Step: 4900 loss: 0.124554\n",
      "Saved Model\n",
      "Step: 4910 loss: 0.098594\n",
      "Step: 4920 loss: 0.098655\n",
      "Step: 4930 loss: 0.0875882\n",
      "Step: 4940 loss: 0.0810822\n",
      "Step: 4950 loss: 0.0975217\n",
      "Step: 4960 loss: 0.319662\n",
      "Step: 4970 loss: 0.0843093\n",
      "Step: 4980 loss: 0.283701\n",
      "Step: 4990 loss: 0.0900051\n",
      "Step: 5000 loss: 0.0836683\n",
      "Saved Model\n",
      "Step: 5010 loss: 0.0804843\n",
      "Step: 5020 loss: 0.0825795\n",
      "Step: 5030 loss: 0.0963037\n",
      "Step: 5040 loss: 0.0959602\n",
      "Step: 5050 loss: 0.0904571\n",
      "Step: 5060 loss: 0.0851428\n",
      "Step: 5070 loss: 0.0882915\n",
      "Step: 5080 loss: 0.10108\n",
      "Step: 5090 loss: 0.179413\n",
      "Step: 5100 loss: 0.0853599\n",
      "Saved Model\n",
      "Step: 5110 loss: 0.0915579\n",
      "Step: 5120 loss: 0.0834669\n",
      "Step: 5130 loss: 0.10136\n",
      "Step: 5140 loss: 0.114576\n",
      "Step: 5150 loss: 0.132318\n",
      "Step: 5160 loss: 0.0810759\n",
      "Step: 5170 loss: 0.0917877\n",
      "Step: 5180 loss: 0.0896475\n",
      "Step: 5190 loss: 0.273067\n",
      "Step: 5200 loss: 0.0961814\n",
      "Saved Model\n",
      "Step: 5210 loss: 0.0832356\n",
      "Step: 5220 loss: 0.106998\n",
      "Step: 5230 loss: 0.0886369\n",
      "Step: 5240 loss: 0.0803952\n",
      "Step: 5250 loss: 0.0823213\n",
      "Step: 5260 loss: 0.0836824\n",
      "Step: 5270 loss: 0.341398\n",
      "Step: 5280 loss: 0.0941927\n",
      "Step: 5290 loss: 0.0913519\n",
      "Step: 5300 loss: 0.086164\n",
      "Saved Model\n",
      "Step: 5310 loss: 0.117857\n",
      "Step: 5320 loss: 0.0895527\n",
      "Step: 5330 loss: 0.10639\n",
      "Step: 5340 loss: 0.0823967\n",
      "Step: 5350 loss: 0.0943455\n",
      "Step: 5360 loss: 0.120161\n",
      "Step: 5370 loss: 0.0815164\n",
      "Step: 5380 loss: 0.105743\n",
      "Step: 5390 loss: 0.0938588\n",
      "Step: 5400 loss: 0.0902899\n",
      "Saved Model\n",
      "Step: 5410 loss: 0.103904\n",
      "Step: 5420 loss: 0.0798359\n",
      "Step: 5430 loss: 0.0805534\n",
      "Step: 5440 loss: 0.0856455\n",
      "Step: 5450 loss: 0.103802\n",
      "Step: 5460 loss: 0.0783082\n",
      "Step: 5470 loss: 0.0963433\n",
      "Step: 5480 loss: 0.0832718\n",
      "Step: 5490 loss: 0.0859672\n",
      "Step: 5500 loss: 0.0769382\n",
      "Saved Model\n",
      "Step: 5510 loss: 0.0945086\n",
      "Step: 5520 loss: 0.0766378\n",
      "Step: 5530 loss: 0.203189\n",
      "Step: 5540 loss: 0.103909\n",
      "Step: 5550 loss: 0.0838356\n",
      "Step: 5560 loss: 0.0831691\n",
      "Step: 5570 loss: 0.087422\n",
      "Step: 5580 loss: 0.0959905\n",
      "Step: 5590 loss: 0.0765698\n",
      "Step: 5600 loss: 0.0937221\n",
      "Saved Model\n",
      "Step: 5610 loss: 0.0810435\n",
      "Step: 5620 loss: 0.0866515\n",
      "Step: 5630 loss: 0.119714\n",
      "Step: 5640 loss: 0.13131\n",
      "Step: 5650 loss: 0.0958261\n",
      "Step: 5660 loss: 0.0839569\n",
      "Step: 5670 loss: 0.0823224\n",
      "Step: 5680 loss: 0.101402\n",
      "Step: 5690 loss: 0.0773835\n",
      "Step: 5700 loss: 0.088423\n",
      "Saved Model\n",
      "Step: 5710 loss: 0.0862453\n",
      "Step: 5720 loss: 0.0970394\n",
      "Step: 5730 loss: 0.0843009\n",
      "Step: 5740 loss: 0.0740804\n",
      "Step: 5750 loss: 0.0934781\n",
      "Step: 5760 loss: 0.172047\n",
      "Step: 5770 loss: 0.0765069\n",
      "Step: 5780 loss: 0.291006\n",
      "Step: 5790 loss: 0.0834035\n",
      "Step: 5800 loss: 0.0766104\n",
      "Saved Model\n",
      "Step: 5810 loss: 0.0737727\n",
      "Step: 5820 loss: 0.0740838\n",
      "Step: 5830 loss: 0.0825833\n",
      "Step: 5840 loss: 0.0938699\n",
      "Step: 5850 loss: 0.0858307\n",
      "Step: 5860 loss: 0.0783962\n",
      "Step: 5870 loss: 0.0798798\n",
      "Step: 5880 loss: 0.0857467\n",
      "Step: 5890 loss: 0.0762052\n",
      "Step: 5900 loss: 0.0764282\n",
      "Saved Model\n",
      "Step: 5910 loss: 0.0746389\n",
      "Step: 5920 loss: 0.0770856\n",
      "Step: 5930 loss: 0.0818408\n",
      "Step: 5940 loss: 0.0820268\n",
      "Step: 5950 loss: 0.0788598\n",
      "Step: 5960 loss: 0.0737337\n",
      "Step: 5970 loss: 0.0775637\n",
      "Step: 5980 loss: 0.0751857\n",
      "Step: 5990 loss: 0.0760914\n",
      "Step: 6000 loss: 0.0811365\n",
      "Saved Model\n",
      "Step: 6010 loss: 0.0761639\n",
      "Step: 6020 loss: 0.0778549\n",
      "Step: 6030 loss: 0.0741043\n",
      "Step: 6040 loss: 0.0739901\n",
      "Step: 6050 loss: 0.0788928\n",
      "Step: 6060 loss: 0.0748256\n",
      "Step: 6070 loss: 0.108459\n",
      "Step: 6080 loss: 0.0806507\n",
      "Step: 6090 loss: 0.0751066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6100 loss: 0.0729333\n",
      "Saved Model\n",
      "Step: 6110 loss: 0.077366\n",
      "Step: 6120 loss: 0.0713063\n",
      "Step: 6130 loss: 0.0759162\n",
      "Step: 6140 loss: 0.0737381\n",
      "Step: 6150 loss: 0.0857024\n",
      "Step: 6160 loss: 0.0750679\n",
      "Step: 6170 loss: 0.0714396\n",
      "Step: 6180 loss: 0.0829965\n",
      "Step: 6190 loss: 0.0785289\n",
      "Step: 6200 loss: 0.0742677\n",
      "Saved Model\n",
      "Step: 6210 loss: 0.0713263\n",
      "Step: 6220 loss: 0.0718791\n",
      "Step: 6230 loss: 0.0727806\n",
      "Step: 6240 loss: 0.0765687\n",
      "Step: 6250 loss: 0.141632\n",
      "Step: 6260 loss: 0.0692255\n",
      "Step: 6270 loss: 0.0718845\n",
      "Step: 6280 loss: 0.0860427\n",
      "Step: 6290 loss: 0.0756211\n",
      "Step: 6300 loss: 0.0696329\n",
      "Saved Model\n",
      "Step: 6310 loss: 0.0779379\n",
      "Step: 6320 loss: 0.069084\n",
      "Step: 6330 loss: 0.215432\n",
      "Step: 6340 loss: 0.0814476\n",
      "Step: 6350 loss: 0.0761775\n",
      "Step: 6360 loss: 0.0702846\n",
      "Step: 6370 loss: 0.0804309\n",
      "Step: 6380 loss: 0.0787294\n",
      "Step: 6390 loss: 0.0685218\n",
      "Step: 6400 loss: 0.0822539\n",
      "Saved Model\n",
      "Step: 6410 loss: 0.0717715\n",
      "Step: 6420 loss: 0.0795451\n",
      "Step: 6430 loss: 0.0729375\n",
      "Step: 6440 loss: 0.0690055\n",
      "Step: 6450 loss: 0.0698646\n",
      "Step: 6460 loss: 0.0679719\n",
      "Step: 6470 loss: 0.0737543\n",
      "Step: 6480 loss: 0.0693623\n",
      "Step: 6490 loss: 0.0680551\n",
      "Step: 6500 loss: 0.0798167\n",
      "Saved Model\n",
      "Step: 6510 loss: 0.0735178\n",
      "Step: 6520 loss: 0.0767395\n",
      "Step: 6530 loss: 0.069577\n",
      "Step: 6540 loss: 0.066381\n",
      "Step: 6550 loss: 0.0723703\n",
      "Step: 6560 loss: 0.0836905\n",
      "Step: 6570 loss: 0.0733301\n",
      "Step: 6580 loss: 0.0958265\n",
      "Step: 6590 loss: 0.0827836\n",
      "Step: 6600 loss: 0.0690454\n",
      "Saved Model\n",
      "Step: 6610 loss: 0.0676324\n",
      "Step: 6620 loss: 0.178483\n",
      "Step: 6630 loss: 0.0700979\n",
      "Step: 6640 loss: 0.103375\n",
      "Step: 6650 loss: 0.0775571\n",
      "Step: 6660 loss: 0.0741181\n",
      "Step: 6670 loss: 0.0782121\n",
      "Step: 6680 loss: 0.0783355\n",
      "Step: 6690 loss: 0.0719948\n",
      "Step: 6700 loss: 0.0730414\n",
      "Saved Model\n",
      "Step: 6710 loss: 0.0757117\n",
      "Step: 6720 loss: 0.0677632\n",
      "Step: 6730 loss: 0.0806504\n",
      "Step: 6740 loss: 0.0958362\n",
      "Step: 6750 loss: 0.111006\n",
      "Step: 6760 loss: 0.0973106\n",
      "Step: 6770 loss: 0.0788739\n",
      "Step: 6780 loss: 0.0716092\n",
      "Step: 6790 loss: 0.0704039\n",
      "Step: 6800 loss: 0.146404\n",
      "Saved Model\n",
      "Step: 6810 loss: 0.074312\n",
      "Step: 6820 loss: 0.11891\n",
      "Step: 6830 loss: 0.134952\n",
      "Step: 6840 loss: 0.0816011\n",
      "Step: 6850 loss: 0.0739143\n",
      "Step: 6860 loss: 0.093678\n",
      "Step: 6870 loss: 0.54869\n",
      "Step: 6880 loss: 0.127992\n",
      "Step: 6890 loss: 0.190283\n",
      "Step: 6900 loss: 0.0797461\n",
      "Saved Model\n",
      "Step: 6910 loss: 0.135104\n",
      "Step: 6920 loss: 0.0785228\n",
      "Step: 6930 loss: 0.0769975\n",
      "Step: 6940 loss: 0.0935032\n",
      "Step: 6950 loss: 0.0953851\n",
      "Step: 6960 loss: 0.282149\n",
      "Step: 6970 loss: 0.0718433\n",
      "Step: 6980 loss: 0.301908\n",
      "Step: 6990 loss: 0.112441\n",
      "Step: 7000 loss: 0.161293\n",
      "Saved Model\n",
      "Step: 7010 loss: 0.0736414\n",
      "Step: 7020 loss: 0.16137\n",
      "Step: 7030 loss: 0.249795\n",
      "Step: 7040 loss: 0.0980327\n",
      "Step: 7050 loss: 0.089792\n",
      "Step: 7060 loss: 0.0735762\n",
      "Step: 7070 loss: 0.0780841\n",
      "Step: 7080 loss: 0.102216\n",
      "Step: 7090 loss: 0.0727305\n",
      "Step: 7100 loss: 0.0851147\n",
      "Saved Model\n",
      "Step: 7110 loss: 0.262264\n",
      "Step: 7120 loss: 0.0857605\n",
      "Step: 7130 loss: 0.141825\n",
      "Step: 7140 loss: 0.376358\n",
      "Step: 7150 loss: 0.145307\n",
      "Step: 7160 loss: 0.0792343\n",
      "Step: 7170 loss: 0.0888405\n",
      "Step: 7180 loss: 0.128017\n",
      "Step: 7190 loss: 0.0824142\n",
      "Step: 7200 loss: 0.0931294\n",
      "Saved Model\n",
      "Step: 7210 loss: 0.107785\n",
      "Step: 7220 loss: 0.0866495\n",
      "Step: 7230 loss: 0.0853345\n",
      "Step: 7240 loss: 0.190943\n",
      "Step: 7250 loss: 0.0741278\n",
      "Step: 7260 loss: 0.0748427\n",
      "Step: 7270 loss: 0.0761528\n",
      "Step: 7280 loss: 0.0898684\n",
      "Step: 7290 loss: 0.088756\n",
      "Step: 7300 loss: 0.0773525\n",
      "Saved Model\n",
      "Step: 7310 loss: 0.0735161\n",
      "Step: 7320 loss: 0.115525\n",
      "Step: 7330 loss: 0.0758074\n",
      "Step: 7340 loss: 0.0723913\n",
      "Step: 7350 loss: 0.0757731\n",
      "Step: 7360 loss: 0.0855369\n",
      "Step: 7370 loss: 0.0861057\n",
      "Step: 7380 loss: 0.213041\n",
      "Step: 7390 loss: 0.0784751\n",
      "Step: 7400 loss: 0.0755408\n",
      "Saved Model\n",
      "Step: 7410 loss: 0.0714102\n",
      "Step: 7420 loss: 0.0719732\n",
      "Step: 7430 loss: 0.0806058\n",
      "Step: 7440 loss: 0.101133\n",
      "Step: 7450 loss: 0.0769786\n",
      "Step: 7460 loss: 0.0785041\n",
      "Step: 7470 loss: 0.0863608\n",
      "Step: 7480 loss: 0.0934745\n",
      "Step: 7490 loss: 0.133093\n",
      "Step: 7500 loss: 0.0735602\n",
      "Saved Model\n",
      "Step: 7510 loss: 0.0724916\n",
      "Step: 7520 loss: 0.0720509\n",
      "Step: 7530 loss: 0.102981\n",
      "Step: 7540 loss: 0.138218\n",
      "Step: 7550 loss: 0.0913622\n",
      "Step: 7560 loss: 0.0722082\n",
      "Step: 7570 loss: 0.0706581\n",
      "Step: 7580 loss: 0.238904\n",
      "Step: 7590 loss: 0.138412\n",
      "Step: 7600 loss: 0.128304\n",
      "Saved Model\n",
      "Step: 7610 loss: 0.124806\n",
      "Step: 7620 loss: 0.161818\n",
      "Step: 7630 loss: 0.0756364\n",
      "Step: 7640 loss: 0.113207\n",
      "Step: 7650 loss: 0.0781457\n",
      "Step: 7660 loss: 0.0795374\n",
      "Step: 7670 loss: 0.186913\n",
      "Step: 7680 loss: 0.156402\n",
      "Step: 7690 loss: 0.080424\n",
      "Step: 7700 loss: 0.075193\n",
      "Saved Model\n",
      "Step: 7710 loss: 0.0856578\n",
      "Step: 7720 loss: 0.0723921\n",
      "Step: 7730 loss: 0.0903731\n",
      "Step: 7740 loss: 0.0745333\n",
      "Step: 7750 loss: 0.117153\n",
      "Step: 7760 loss: 0.0972156\n",
      "Step: 7770 loss: 0.0708652\n",
      "Step: 7780 loss: 0.071044\n",
      "Step: 7790 loss: 0.0735045\n",
      "Step: 7800 loss: 0.0743432\n",
      "Saved Model\n",
      "Step: 7810 loss: 0.0707031\n",
      "Step: 7820 loss: 0.0697737\n",
      "Step: 7830 loss: 0.0738959\n",
      "Step: 7840 loss: 0.0878537\n",
      "Step: 7850 loss: 0.0718074\n",
      "Step: 7860 loss: 0.0691626\n",
      "Step: 7870 loss: 0.0835874\n",
      "Step: 7880 loss: 0.0810671\n",
      "Step: 7890 loss: 0.0722042\n",
      "Step: 7900 loss: 0.0693719\n",
      "Saved Model\n",
      "Step: 7910 loss: 0.0714497\n",
      "Step: 7920 loss: 0.0696705\n",
      "Step: 7930 loss: 0.289232\n",
      "Step: 7940 loss: 0.151312\n",
      "Step: 7950 loss: 0.122715\n",
      "Step: 7960 loss: 0.0736793\n",
      "Step: 7970 loss: 0.0783126\n",
      "Step: 7980 loss: 0.0785416\n",
      "Step: 7990 loss: 0.0760925\n",
      "Step: 8000 loss: 0.0720014\n",
      "Saved Model\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for i in range(num_epochs):\n",
    "    for j in range(num_batches):\n",
    "        path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "        word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "        \n",
    "        feed_dict = {\n",
    "            path_length:path_dict,\n",
    "            word_ids:word_dict,\n",
    "            pos_ids:pos_dict,\n",
    "            dep_ids:dep_dict,\n",
    "            y:y_dict}\n",
    "        _, loss, step = sess.run([optimizer, total_loss, global_step], feed_dict)\n",
    "        if step % 10 ==0:\n",
    "            print(\"Step:\", step, \"loss:\",loss)\n",
    "        if step % 100 == 0:\n",
    "            saver.save(sess, model_dir + '/model', global_step = step)\n",
    "            print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 99.475\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "all_predictions = []\n",
    "for j in range(num_batches):\n",
    "    path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "    word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "\n",
    "    feed_dict = {\n",
    "        path_length:path_dict,\n",
    "        word_ids:word_dict,\n",
    "        pos_ids:pos_dict,\n",
    "        dep_ids:dep_dict,\n",
    "        y:y_dict}\n",
    "    batch_predictions = sess.run(predictions, feed_dict)\n",
    "    all_predictions.append(batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(800):\n",
    "    for pred in all_predictions[i]:\n",
    "        y_pred.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(8000):\n",
    "    count += y_pred[i]==rel_ids[i]\n",
    "accuracy = count/80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.475\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1 = f1_score(rel_ids, y_pred, average='micro')*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.475000000000009"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

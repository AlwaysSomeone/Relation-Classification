{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, _pickle as pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "ckpt_dir = 'checkpoint'\n",
    "word_embd_dir = 'checkpoint/word_embd'\n",
    "model_dir = 'checkpoint/modelv2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embd_dim = 100\n",
    "pos_embd_dim = 25\n",
    "dep_embd_dim = 25\n",
    "word_vocab_size = 400001\n",
    "pos_vocab_size = 10\n",
    "dep_vocab_size = 21\n",
    "relation_classes = 19\n",
    "word_state_size = 100\n",
    "other_state_size = 100\n",
    "batch_size = 10\n",
    "channels = 3\n",
    "lambda_l2 = 0.0001\n",
    "max_len_path = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"input\"):\n",
    "    path_length = tf.placeholder(tf.int32, shape=[2, batch_size], name=\"path1_length\")\n",
    "    word_ids = tf.placeholder(tf.int32, shape=[2, batch_size, max_len_path], name=\"word_ids\")\n",
    "    pos_ids = tf.placeholder(tf.int32, [2, batch_size, max_len_path], name=\"pos_ids\")\n",
    "    dep_ids = tf.placeholder(tf.int32, [2, batch_size, max_len_path], name=\"dep_ids\")\n",
    "    y = tf.placeholder(tf.int32, [batch_size], name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"word_embedding\"):\n",
    "    W = tf.Variable(tf.constant(0.0, shape=[word_vocab_size, word_embd_dim]), name=\"W\")\n",
    "    embedding_placeholder = tf.placeholder(tf.float32,[word_vocab_size, word_embd_dim])\n",
    "    embedding_init = W.assign(embedding_placeholder)\n",
    "    embedded_word = tf.nn.embedding_lookup(W, word_ids)\n",
    "    word_embedding_saver = tf.train.Saver({\"word_embedding/W\": W})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"pos_embedding\"):\n",
    "    W = tf.Variable(tf.random_uniform([pos_vocab_size, pos_embd_dim]), name=\"W\")\n",
    "    embedded_pos = tf.nn.embedding_lookup(W, pos_ids)\n",
    "    pos_embedding_saver = tf.train.Saver({\"pos_embedding/W\": W})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dep_embedding\"):\n",
    "    W = tf.Variable(tf.random_uniform([dep_vocab_size, dep_embd_dim]), name=\"W\")\n",
    "    embedded_dep = tf.nn.embedding_lookup(W, dep_ids)\n",
    "    dep_embedding_saver = tf.train.Saver({\"dep_embedding/W\": W})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_hidden_state = tf.zeros([batch_size, word_state_size], name='word_hidden_state')\n",
    "word_cell_state = tf.zeros([batch_size, word_state_size], name='word_cell_state')\n",
    "word_init_state = tf.contrib.rnn.LSTMStateTuple(word_hidden_state, word_cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_hidden_states = tf.zeros([channels-1, batch_size, other_state_size], name=\"hidden_state\")\n",
    "other_cell_states = tf.zeros([channels-1, batch_size, other_state_size], name=\"cell_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_init_states = [tf.contrib.rnn.LSTMStateTuple(other_hidden_states[i], other_cell_states[i]) for i in range(channels-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"word_lstm1\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(word_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_word[0], sequence_length=path_length[0], initial_state=word_init_state)\n",
    "    state_series_word1 = tf.reduce_max(state_series, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"word_lstm2\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(word_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_word[1], sequence_length=path_length[1], initial_state=word_init_state)\n",
    "    state_series_word2 = tf.reduce_max(state_series, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"pos_lstm1\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_pos[0], sequence_length=path_length[0],initial_state=other_init_states[0])\n",
    "    state_series_pos1 = tf.reduce_max(state_series, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"pos_lstm2\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_pos[1], sequence_length=path_length[1],initial_state=other_init_states[0])\n",
    "    state_series_pos2 = tf.reduce_max(state_series, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"dep_lstm1\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_dep[0], sequence_length=path_length[0], initial_state=other_init_states[1])\n",
    "    state_series_dep1 = tf.reduce_max(state_series, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"dep_lstm2\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "    state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_dep[1], sequence_length=path_length[1], initial_state=other_init_states[1])\n",
    "    state_series_dep2 = tf.reduce_max(state_series, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_series1 = tf.concat([state_series_word1, state_series_pos1, state_series_dep1], 1)\n",
    "state_series2 = tf.concat([state_series_word2, state_series_pos2, state_series_dep2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_series = tf.concat([state_series1, state_series2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat_5:0' shape=(10, 600) dtype=float32>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"hidden_layer\"):\n",
    "    W = tf.Variable(tf.truncated_normal([600, 100], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([100]), name=\"b\")\n",
    "    y_hidden_layer = tf.matmul(state_series, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dropout\"):\n",
    "    h_drop = tf.nn.dropout(y_hidden_layer, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"softmax_layer\"):\n",
    "    W = tf.Variable(tf.truncated_normal([100, relation_classes], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([relation_classes]), name=\"b\")\n",
    "    logits = tf.matmul(y_hidden_layer, W) + b\n",
    "    predictions = tf.argmax(logits, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name=\"global_step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tv_all = tf.trainable_variables()\n",
    "tv_regu = []\n",
    "non_reg = [\"word_embedding/W:0\",\"pos_embedding/W:0\",'dep_embedding/W:0',\"global_step:0\",'hidden_layer/b:0','softmax_layer/b:0']\n",
    "for t in tv_all:\n",
    "    if t.name not in non_reg:\n",
    "        if(t.name.find('biases')==-1):\n",
    "            tv_regu.append(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    l2_loss = lambda_l2 * tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv_regu ])\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    total_loss = loss + l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions = tf.equal(predictions, tf.arg_max(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(total_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('data/vocab.pkl', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f = open('data/word_embedding', 'rb')\n",
    "# word_embedding = pickle.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "word2id[unknown_token] = word_vocab_size -1\n",
    "id2word[word_vocab_size-1] = unknown_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('data/train_paths', 'rb')\n",
    "word_p1, word_p2, dep_p1, dep_p2, pos_p1, pos_p2 = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relations = []\n",
    "for line in open('data/train_relation.txt'):\n",
    "    relations.append(line.strip().split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches = int(8000/batch_size)\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tags_vocab = []\n",
    "for line in open('data/pos_tags.txt'):\n",
    "        pos_tags_vocab.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep_vocab = []\n",
    "for line in open('data/dependency_types.txt'):\n",
    "    dep_vocab.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relation_vocab = []\n",
    "for line in open('data/relation_types.txt'):\n",
    "    relation_vocab.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rel2id = dict((w, i) for i,w in enumerate(relation_vocab))\n",
    "id2rel = dict((i, w) for i,w in enumerate(relation_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tag2id = dict((w, i) for i,w in enumerate(pos_tags_vocab))\n",
    "id2pos_tag = dict((i, w) for i,w in enumerate(pos_tags_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep2id = dict((w, i) for i,w in enumerate(dep_vocab))\n",
    "id2dep = dict((i, w) for i,w in enumerate(dep_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tag2id['OTH'] = 9\n",
    "id2pos_tag[9] = 'OTH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep2id['OTH'] = 20\n",
    "id2dep[20] = 'OTH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JJ_pos_tags = ['JJ', 'JJR', 'JJS']\n",
    "NN_pos_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "RB_pos_tags = ['RB', 'RBR', 'RBS']\n",
    "PRP_pos_tags = ['PRP', 'PRP$']\n",
    "VB_pos_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "_pos_tags = ['CC', 'CD', 'DT', 'IN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def pos_tag(x):\n",
    "    if x in JJ_pos_tags:\n",
    "        return pos_tag2id['JJ']\n",
    "    if x in NN_pos_tags:\n",
    "        return pos_tag2id['NN']\n",
    "    if x in RB_pos_tags:\n",
    "        return pos_tag2id['RB']\n",
    "    if x in PRP_pos_tags:\n",
    "        return pos_tag2id['PRP']\n",
    "    if x in VB_pos_tags:\n",
    "        return pos_tag2id['VB']\n",
    "    if x in _pos_tags:\n",
    "        return pos_tag2id[x]\n",
    "    else:\n",
    "        return 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep = []\n",
    "for i in range(8000):\n",
    "    dep.append(dep_p1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(8000):\n",
    "    dep.append(dep_p2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep_freq = nltk.FreqDist(chain(*dep))\n",
    "dep_types = dep_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(8000):\n",
    "    for j, word in enumerate(word_p1[i]):\n",
    "        word = word.lower()\n",
    "        word_p1[i][j] = word if word in word2id else unknown_token \n",
    "    for k, word in enumerate(word_p2[i]):\n",
    "        word = word.lower()\n",
    "        word_p2[i][k] = word if word in word2id else unknown_token \n",
    "    for l, d in enumerate(dep_p1[i]):\n",
    "        dep_p1[i][l] = d if d in dep2id else 'OTH'\n",
    "    for m, d in enumerate(dep_p2[i]):\n",
    "        dep_p2[i][m] = d if d in dep2id else 'OTH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_p1_ids = np.ones([8000, max_len_path],dtype=int)\n",
    "word_p2_ids = np.ones([8000, max_len_path],dtype=int)\n",
    "pos_p1_ids = np.ones([8000, max_len_path],dtype=int)\n",
    "pos_p2_ids = np.ones([8000, max_len_path],dtype=int)\n",
    "dep_p1_ids = np.ones([8000, max_len_path],dtype=int)\n",
    "dep_p2_ids = np.ones([8000, max_len_path],dtype=int)\n",
    "rel_ids = np.array([rel2id[rel] for rel in relations])\n",
    "path1_len = np.array([len(w) for w in word_p1], dtype=int)\n",
    "path2_len = np.array([len(w) for w in word_p2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(8000):\n",
    "    for j, w in enumerate(word_p1[i]):\n",
    "        word_p1_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(word_p2[i]):\n",
    "        word_p2_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(pos_p1[i]):\n",
    "        pos_p1_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(pos_p2[i]):\n",
    "        pos_p2_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(dep_p1[i]):\n",
    "        dep_p1_ids[i][j] = dep2id[w]\n",
    "    for j, w in enumerate(dep_p2[i]):\n",
    "        dep_p2_ids[i][j] = dep2id[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess.run(embedding_init, feed_dict={embedding_placeholder:word_embedding})\n",
    "# word_embedding_saver.save(sess, word_embd_dir + '/word_embd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint/modelv2/model\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key word_embedding/W/Adam_1 not found in checkpoint\n\t [[Node: save/RestoreV2_47 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_47/tensor_names, save/RestoreV2_47/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_47', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-51-426404ac637a>\", line 3, in <module>\n    saver = tf.train.Saver()\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1056, in __init__\n    self.build()\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 669, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key word_embedding/W/Adam_1 not found in checkpoint\n\t [[Node: save/RestoreV2_47 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_47/tensor_names, save/RestoreV2_47/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key word_embedding/W/Adam_1 not found in checkpoint\n\t [[Node: save/RestoreV2_47 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_47/tensor_names, save/RestoreV2_47/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-3a8d3e4f2338>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1457\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key word_embedding/W/Adam_1 not found in checkpoint\n\t [[Node: save/RestoreV2_47 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_47/tensor_names, save/RestoreV2_47/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_47', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-51-426404ac637a>\", line 3, in <module>\n    saver = tf.train.Saver()\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1056, in __init__\n    self.build()\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 669, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key word_embedding/W/Adam_1 not found in checkpoint\n\t [[Node: save/RestoreV2_47 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_47/tensor_names, save/RestoreV2_47/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "model = tf.train.latest_checkpoint(model_dir)\n",
    "saver.restore(sess, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# latest_embd = tf.train.latest_checkpoint(word_embd_dir)\n",
    "# word_embedding_saver.restore(sess, latest_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10 loss: 2.80779\n",
      "Step: 20 loss: 2.91857\n",
      "Step: 30 loss: 2.5435\n",
      "Step: 40 loss: 2.77815\n",
      "Step: 50 loss: 2.86619\n",
      "Step: 60 loss: 2.70983\n",
      "Step: 70 loss: 2.59677\n",
      "Step: 80 loss: 2.57367\n",
      "Step: 90 loss: 2.63232\n",
      "Step: 100 loss: 2.67632\n",
      "Step: 110 loss: 2.46945\n",
      "Step: 120 loss: 2.41762\n",
      "Step: 130 loss: 2.47456\n",
      "Step: 140 loss: 2.18848\n",
      "Step: 150 loss: 2.51237\n",
      "Step: 160 loss: 2.62843\n",
      "Step: 170 loss: 2.14578\n",
      "Step: 180 loss: 2.19004\n",
      "Step: 190 loss: 2.36966\n",
      "Step: 200 loss: 2.43195\n",
      "Step: 210 loss: 2.35718\n",
      "Step: 220 loss: 2.12848\n",
      "Step: 230 loss: 2.46286\n",
      "Step: 240 loss: 1.78023\n",
      "Step: 250 loss: 1.88158\n",
      "Step: 260 loss: 2.60348\n",
      "Step: 270 loss: 2.12932\n",
      "Step: 280 loss: 1.77932\n",
      "Step: 290 loss: 2.52527\n",
      "Step: 300 loss: 2.03072\n",
      "Step: 310 loss: 1.65725\n",
      "Step: 320 loss: 2.38889\n",
      "Step: 330 loss: 1.7463\n",
      "Step: 340 loss: 2.76068\n",
      "Step: 350 loss: 3.97415\n",
      "Step: 360 loss: 1.99721\n",
      "Step: 370 loss: 1.42537\n",
      "Step: 380 loss: 1.83674\n",
      "Step: 390 loss: 2.24145\n",
      "Step: 400 loss: 2.22669\n",
      "Step: 410 loss: 2.5695\n",
      "Step: 420 loss: 1.93245\n",
      "Step: 430 loss: 1.43215\n",
      "Step: 440 loss: 1.36516\n",
      "Step: 450 loss: 1.72822\n",
      "Step: 460 loss: 1.97294\n",
      "Step: 470 loss: 2.21387\n",
      "Step: 480 loss: 2.29581\n",
      "Step: 490 loss: 1.81742\n",
      "Step: 500 loss: 2.22091\n",
      "Step: 510 loss: 2.17334\n",
      "Step: 520 loss: 1.72496\n",
      "Step: 530 loss: 1.73135\n",
      "Step: 540 loss: 2.15394\n",
      "Step: 550 loss: 2.79019\n",
      "Step: 560 loss: 1.86871\n",
      "Step: 570 loss: 1.82688\n",
      "Step: 580 loss: 2.11267\n",
      "Step: 590 loss: 2.40371\n",
      "Step: 600 loss: 2.0241\n",
      "Step: 610 loss: 1.67159\n",
      "Step: 620 loss: 1.85799\n",
      "Step: 630 loss: 1.49225\n",
      "Step: 640 loss: 1.57209\n",
      "Step: 650 loss: 1.33378\n",
      "Step: 660 loss: 1.97145\n",
      "Step: 670 loss: 2.5472\n",
      "Step: 680 loss: 1.47655\n",
      "Step: 690 loss: 1.2492\n",
      "Step: 700 loss: 2.04557\n",
      "Step: 710 loss: 2.13845\n",
      "Step: 720 loss: 2.04659\n",
      "Step: 730 loss: 2.54441\n",
      "Step: 740 loss: 2.35806\n",
      "Step: 750 loss: 1.75022\n",
      "Step: 760 loss: 1.69592\n",
      "Step: 770 loss: 1.68255\n",
      "Step: 780 loss: 2.18706\n",
      "Step: 790 loss: 1.89099\n",
      "Step: 800 loss: 1.42934\n",
      "Step: 810 loss: 1.53029\n",
      "Step: 820 loss: 2.43371\n",
      "Step: 830 loss: 1.5521\n",
      "Step: 840 loss: 1.41016\n",
      "Step: 850 loss: 2.00514\n",
      "Step: 860 loss: 1.34262\n",
      "Step: 870 loss: 1.51234\n",
      "Step: 880 loss: 1.26734\n",
      "Step: 890 loss: 1.46363\n",
      "Step: 900 loss: 2.21399\n",
      "Step: 910 loss: 1.25637\n",
      "Step: 920 loss: 1.69733\n",
      "Step: 930 loss: 0.710095\n",
      "Step: 940 loss: 0.80871\n",
      "Step: 950 loss: 1.83563\n",
      "Step: 960 loss: 1.42548\n",
      "Step: 970 loss: 1.31903\n",
      "Step: 980 loss: 1.32721\n",
      "Step: 990 loss: 1.35985\n",
      "Step: 1000 loss: 1.87178\n",
      "Saved Model\n",
      "Step: 1010 loss: 1.07465\n",
      "Step: 1020 loss: 1.14743\n",
      "Step: 1030 loss: 2.01722\n",
      "Step: 1040 loss: 1.07011\n",
      "Step: 1050 loss: 1.41236\n",
      "Step: 1060 loss: 1.41468\n",
      "Step: 1070 loss: 1.12984\n",
      "Step: 1080 loss: 1.1004\n",
      "Step: 1090 loss: 1.46325\n",
      "Step: 1100 loss: 0.94754\n",
      "Step: 1110 loss: 1.28209\n",
      "Step: 1120 loss: 1.24672\n",
      "Step: 1130 loss: 1.03963\n",
      "Step: 1140 loss: 2.30612\n",
      "Step: 1150 loss: 3.08776\n",
      "Step: 1160 loss: 1.34762\n",
      "Step: 1170 loss: 0.999867\n",
      "Step: 1180 loss: 1.47572\n",
      "Step: 1190 loss: 1.38837\n",
      "Step: 1200 loss: 1.47402\n",
      "Step: 1210 loss: 1.69465\n",
      "Step: 1220 loss: 1.30756\n",
      "Step: 1230 loss: 0.762352\n",
      "Step: 1240 loss: 0.681113\n",
      "Step: 1250 loss: 1.18623\n",
      "Step: 1260 loss: 0.937927\n",
      "Step: 1270 loss: 1.55704\n",
      "Step: 1280 loss: 1.32217\n",
      "Step: 1290 loss: 0.901533\n",
      "Step: 1300 loss: 1.29536\n",
      "Step: 1310 loss: 1.86017\n",
      "Step: 1320 loss: 0.978925\n",
      "Step: 1330 loss: 1.12164\n",
      "Step: 1340 loss: 1.0424\n",
      "Step: 1350 loss: 1.88257\n",
      "Step: 1360 loss: 1.37704\n",
      "Step: 1370 loss: 1.66459\n",
      "Step: 1380 loss: 1.03419\n",
      "Step: 1390 loss: 1.25468\n",
      "Step: 1400 loss: 1.05265\n",
      "Step: 1410 loss: 0.752249\n",
      "Step: 1420 loss: 1.15698\n",
      "Step: 1430 loss: 0.840905\n",
      "Step: 1440 loss: 0.746235\n",
      "Step: 1450 loss: 0.892695\n",
      "Step: 1460 loss: 0.847824\n",
      "Step: 1470 loss: 1.76607\n",
      "Step: 1480 loss: 0.844165\n",
      "Step: 1490 loss: 0.739607\n",
      "Step: 1500 loss: 1.26908\n",
      "Step: 1510 loss: 1.3015\n",
      "Step: 1520 loss: 1.06725\n",
      "Step: 1530 loss: 2.15476\n",
      "Step: 1540 loss: 2.11609\n",
      "Step: 1550 loss: 0.97689\n",
      "Step: 1560 loss: 0.707218\n",
      "Step: 1570 loss: 0.888292\n",
      "Step: 1580 loss: 1.7632\n",
      "Step: 1590 loss: 0.885557\n",
      "Step: 1600 loss: 1.08809\n",
      "Step: 1610 loss: 1.18293\n",
      "Step: 1620 loss: 1.82516\n",
      "Step: 1630 loss: 1.19474\n",
      "Step: 1640 loss: 1.02892\n",
      "Step: 1650 loss: 1.39906\n",
      "Step: 1660 loss: 0.968386\n",
      "Step: 1670 loss: 1.20328\n",
      "Step: 1680 loss: 0.634556\n",
      "Step: 1690 loss: 0.968465\n",
      "Step: 1700 loss: 1.62168\n",
      "Step: 1710 loss: 0.630523\n",
      "Step: 1720 loss: 0.977327\n",
      "Step: 1730 loss: 0.30693\n",
      "Step: 1740 loss: 0.450326\n",
      "Step: 1750 loss: 1.35989\n",
      "Step: 1760 loss: 1.15494\n",
      "Step: 1770 loss: 0.772031\n",
      "Step: 1780 loss: 0.915567\n",
      "Step: 1790 loss: 0.836507\n",
      "Step: 1800 loss: 1.40167\n",
      "Step: 1810 loss: 0.569495\n",
      "Step: 1820 loss: 0.769843\n",
      "Step: 1830 loss: 1.30344\n",
      "Step: 1840 loss: 1.01782\n",
      "Step: 1850 loss: 0.881146\n",
      "Step: 1860 loss: 0.819891\n",
      "Step: 1870 loss: 0.806371\n",
      "Step: 1880 loss: 0.565446\n",
      "Step: 1890 loss: 0.637321\n",
      "Step: 1900 loss: 0.352928\n",
      "Step: 1910 loss: 0.699177\n",
      "Step: 1920 loss: 0.620582\n",
      "Step: 1930 loss: 0.645748\n",
      "Step: 1940 loss: 1.50655\n",
      "Step: 1950 loss: 1.54128\n",
      "Step: 1960 loss: 0.954772\n",
      "Step: 1970 loss: 0.462654\n",
      "Step: 1980 loss: 1.1376\n",
      "Step: 1990 loss: 1.42897\n",
      "Step: 2000 loss: 1.24211\n",
      "Saved Model\n",
      "Step: 2010 loss: 0.501059\n",
      "Step: 2020 loss: 1.04652\n",
      "Step: 2030 loss: 0.33076\n",
      "Step: 2040 loss: 0.693647\n",
      "Step: 2050 loss: 0.749331\n",
      "Step: 2060 loss: 0.652774\n",
      "Step: 2070 loss: 1.31997\n",
      "Step: 2080 loss: 0.80711\n",
      "Step: 2090 loss: 0.442939\n",
      "Step: 2100 loss: 0.924869\n",
      "Step: 2110 loss: 0.78869\n",
      "Step: 2120 loss: 0.467321\n",
      "Step: 2130 loss: 0.658172\n",
      "Step: 2140 loss: 0.719485\n",
      "Step: 2150 loss: 1.2334\n",
      "Step: 2160 loss: 0.911494\n",
      "Step: 2170 loss: 0.994744\n",
      "Step: 2180 loss: 0.657392\n",
      "Step: 2190 loss: 1.05786\n",
      "Step: 2200 loss: 0.74633\n",
      "Step: 2210 loss: 0.292225\n",
      "Step: 2220 loss: 0.572513\n",
      "Step: 2230 loss: 0.635224\n",
      "Step: 2240 loss: 0.382504\n",
      "Step: 2250 loss: 0.705476\n",
      "Step: 2260 loss: 0.359047\n",
      "Step: 2270 loss: 1.10121\n",
      "Step: 2280 loss: 0.383437\n",
      "Step: 2290 loss: 0.305753\n",
      "Step: 2300 loss: 0.735411\n",
      "Step: 2310 loss: 0.739361\n",
      "Step: 2320 loss: 0.418383\n",
      "Step: 2330 loss: 1.74282\n",
      "Step: 2340 loss: 0.941518\n",
      "Step: 2350 loss: 0.620376\n",
      "Step: 2360 loss: 0.403746\n",
      "Step: 2370 loss: 0.382935\n",
      "Step: 2380 loss: 1.33746\n",
      "Step: 2390 loss: 0.392732\n",
      "Step: 2400 loss: 0.590936\n",
      "Step: 2410 loss: 0.582584\n",
      "Step: 2420 loss: 1.3547\n",
      "Step: 2430 loss: 0.673139\n",
      "Step: 2440 loss: 1.01319\n",
      "Step: 2450 loss: 1.11465\n",
      "Step: 2460 loss: 0.73399\n",
      "Step: 2470 loss: 0.872151\n",
      "Step: 2480 loss: 0.294742\n",
      "Step: 2490 loss: 0.650667\n",
      "Step: 2500 loss: 1.11923\n",
      "Step: 2510 loss: 0.420434\n",
      "Step: 2520 loss: 0.71681\n",
      "Step: 2530 loss: 0.182528\n",
      "Step: 2540 loss: 0.250423\n",
      "Step: 2550 loss: 1.1188\n",
      "Step: 2560 loss: 1.04003\n",
      "Step: 2570 loss: 0.34242\n",
      "Step: 2580 loss: 0.583532\n",
      "Step: 2590 loss: 0.534887\n",
      "Step: 2600 loss: 0.559442\n",
      "Step: 2610 loss: 0.264497\n",
      "Step: 2620 loss: 0.441281\n",
      "Step: 2630 loss: 0.460359\n",
      "Step: 2640 loss: 0.464572\n",
      "Step: 2650 loss: 0.797805\n",
      "Step: 2660 loss: 0.55852\n",
      "Step: 2670 loss: 0.52766\n",
      "Step: 2680 loss: 0.55177\n",
      "Step: 2690 loss: 0.221957\n",
      "Step: 2700 loss: 0.195293\n",
      "Step: 2710 loss: 0.457977\n",
      "Step: 2720 loss: 0.5188\n",
      "Step: 2730 loss: 0.408517\n",
      "Step: 2740 loss: 0.743241\n",
      "Step: 2750 loss: 0.741312\n",
      "Step: 2760 loss: 0.560375\n",
      "Step: 2770 loss: 0.285914\n",
      "Step: 2780 loss: 0.553423\n",
      "Step: 2790 loss: 1.2082\n",
      "Step: 2800 loss: 0.827708\n",
      "Step: 2810 loss: 0.272985\n",
      "Step: 2820 loss: 0.522917\n",
      "Step: 2830 loss: 0.215335\n",
      "Step: 2840 loss: 0.318928\n",
      "Step: 2850 loss: 0.534665\n",
      "Step: 2860 loss: 0.34578\n",
      "Step: 2870 loss: 0.678226\n",
      "Step: 2880 loss: 0.37372\n",
      "Step: 2890 loss: 0.187034\n",
      "Step: 2900 loss: 0.781811\n",
      "Step: 2910 loss: 0.315417\n",
      "Step: 2920 loss: 0.202806\n",
      "Step: 2930 loss: 0.298897\n",
      "Step: 2940 loss: 0.426693\n",
      "Step: 2950 loss: 0.599579\n",
      "Step: 2960 loss: 0.485466\n",
      "Step: 2970 loss: 0.459041\n",
      "Step: 2980 loss: 0.345937\n",
      "Step: 2990 loss: 0.71386\n",
      "Step: 3000 loss: 0.325374\n",
      "Saved Model\n",
      "Step: 3010 loss: 0.189127\n",
      "Step: 3020 loss: 0.245685\n",
      "Step: 3030 loss: 0.216468\n",
      "Step: 3040 loss: 0.380124\n",
      "Step: 3050 loss: 0.421312\n",
      "Step: 3060 loss: 0.278748\n",
      "Step: 3070 loss: 0.438408\n",
      "Step: 3080 loss: 0.233688\n",
      "Step: 3090 loss: 0.190263\n",
      "Step: 3100 loss: 0.354947\n",
      "Step: 3110 loss: 0.32813\n",
      "Step: 3120 loss: 0.351418\n",
      "Step: 3130 loss: 0.890704\n",
      "Step: 3140 loss: 0.604725\n",
      "Step: 3150 loss: 0.338528\n",
      "Step: 3160 loss: 0.380654\n",
      "Step: 3170 loss: 0.457897\n",
      "Step: 3180 loss: 1.16789\n",
      "Step: 3190 loss: 0.289864\n",
      "Step: 3200 loss: 0.452603\n",
      "Step: 3210 loss: 0.424467\n",
      "Step: 3220 loss: 0.813356\n",
      "Step: 3230 loss: 0.419018\n",
      "Step: 3240 loss: 0.684128\n",
      "Step: 3250 loss: 0.547689\n",
      "Step: 3260 loss: 0.22437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3270 loss: 0.873815\n",
      "Step: 3280 loss: 0.192159\n",
      "Step: 3290 loss: 0.425214\n",
      "Step: 3300 loss: 0.542901\n",
      "Step: 3310 loss: 0.241862\n",
      "Step: 3320 loss: 0.698558\n",
      "Step: 3330 loss: 0.148543\n",
      "Step: 3340 loss: 0.203272\n",
      "Step: 3350 loss: 0.789588\n",
      "Step: 3360 loss: 1.136\n",
      "Step: 3370 loss: 0.315635\n",
      "Step: 3380 loss: 0.449273\n",
      "Step: 3390 loss: 0.45932\n",
      "Step: 3400 loss: 0.363575\n",
      "Step: 3410 loss: 0.183468\n",
      "Step: 3420 loss: 0.223123\n",
      "Step: 3430 loss: 0.228775\n",
      "Step: 3440 loss: 0.37514\n",
      "Step: 3450 loss: 0.464133\n",
      "Step: 3460 loss: 0.292304\n",
      "Step: 3470 loss: 0.339957\n",
      "Step: 3480 loss: 0.505914\n",
      "Step: 3490 loss: 0.207361\n",
      "Step: 3500 loss: 0.17801\n",
      "Step: 3510 loss: 0.313443\n",
      "Step: 3520 loss: 0.446755\n",
      "Step: 3530 loss: 0.290929\n",
      "Step: 3540 loss: 0.615026\n",
      "Step: 3550 loss: 0.530993\n",
      "Step: 3560 loss: 0.29253\n",
      "Step: 3570 loss: 0.279436\n",
      "Step: 3580 loss: 0.237378\n",
      "Step: 3590 loss: 0.886074\n",
      "Step: 3600 loss: 0.468314\n",
      "Step: 3610 loss: 0.237626\n",
      "Step: 3620 loss: 0.482566\n",
      "Step: 3630 loss: 0.191543\n",
      "Step: 3640 loss: 0.158047\n",
      "Step: 3650 loss: 0.396286\n",
      "Step: 3660 loss: 0.194934\n",
      "Step: 3670 loss: 0.444721\n",
      "Step: 3680 loss: 0.227833\n",
      "Step: 3690 loss: 0.214852\n",
      "Step: 3700 loss: 0.29076\n",
      "Step: 3710 loss: 0.266543\n",
      "Step: 3720 loss: 0.276693\n",
      "Step: 3730 loss: 0.193421\n",
      "Step: 3740 loss: 0.234184\n",
      "Step: 3750 loss: 0.430565\n",
      "Step: 3760 loss: 0.320412\n",
      "Step: 3770 loss: 0.196605\n",
      "Step: 3780 loss: 0.203329\n",
      "Step: 3790 loss: 0.738182\n",
      "Step: 3800 loss: 0.263198\n",
      "Step: 3810 loss: 0.161345\n",
      "Step: 3820 loss: 0.161211\n",
      "Step: 3830 loss: 0.189592\n",
      "Step: 3840 loss: 0.195735\n",
      "Step: 3850 loss: 0.310571\n",
      "Step: 3860 loss: 0.198553\n",
      "Step: 3870 loss: 0.249575\n",
      "Step: 3880 loss: 0.252854\n",
      "Step: 3890 loss: 0.207249\n",
      "Step: 3900 loss: 0.184173\n",
      "Step: 3910 loss: 0.195191\n",
      "Step: 3920 loss: 0.260273\n",
      "Step: 3930 loss: 0.904425\n",
      "Step: 3940 loss: 0.578173\n",
      "Step: 3950 loss: 0.17869\n",
      "Step: 3960 loss: 0.204394\n",
      "Step: 3970 loss: 0.645423\n",
      "Step: 3980 loss: 0.841893\n",
      "Step: 3990 loss: 0.244148\n",
      "Step: 4000 loss: 0.247138\n",
      "Saved Model\n",
      "Step: 4010 loss: 0.17368\n",
      "Step: 4020 loss: 0.60329\n",
      "Step: 4030 loss: 0.367786\n",
      "Step: 4040 loss: 0.842427\n",
      "Step: 4050 loss: 0.270589\n",
      "Step: 4060 loss: 0.174544\n",
      "Step: 4070 loss: 0.524515\n",
      "Step: 4080 loss: 0.172935\n",
      "Step: 4090 loss: 0.265039\n",
      "Step: 4100 loss: 0.376165\n",
      "Step: 4110 loss: 0.203829\n",
      "Step: 4120 loss: 0.574954\n",
      "Step: 4130 loss: 0.147455\n",
      "Step: 4140 loss: 0.162642\n",
      "Step: 4150 loss: 0.235736\n",
      "Step: 4160 loss: 0.927939\n",
      "Step: 4170 loss: 0.259387\n",
      "Step: 4180 loss: 0.354735\n",
      "Step: 4190 loss: 0.301515\n",
      "Step: 4200 loss: 0.294105\n",
      "Step: 4210 loss: 0.159539\n",
      "Step: 4220 loss: 0.17205\n",
      "Step: 4230 loss: 0.19154\n",
      "Step: 4240 loss: 0.193511\n",
      "Step: 4250 loss: 0.256419\n",
      "Step: 4260 loss: 0.181731\n",
      "Step: 4270 loss: 0.307613\n",
      "Step: 4280 loss: 0.348477\n",
      "Step: 4290 loss: 0.292635\n",
      "Step: 4300 loss: 0.149401\n",
      "Step: 4310 loss: 0.185066\n",
      "Step: 4320 loss: 0.233834\n",
      "Step: 4330 loss: 0.201818\n",
      "Step: 4340 loss: 0.198085\n",
      "Step: 4350 loss: 0.312857\n",
      "Step: 4360 loss: 0.214722\n",
      "Step: 4370 loss: 0.225578\n",
      "Step: 4380 loss: 0.174452\n",
      "Step: 4390 loss: 0.195197\n",
      "Step: 4400 loss: 0.509981\n",
      "Step: 4410 loss: 0.171098\n",
      "Step: 4420 loss: 0.245524\n",
      "Step: 4430 loss: 0.192645\n",
      "Step: 4440 loss: 0.16506\n",
      "Step: 4450 loss: 0.220923\n",
      "Step: 4460 loss: 0.179104\n",
      "Step: 4470 loss: 0.373876\n",
      "Step: 4480 loss: 0.188156\n",
      "Step: 4490 loss: 0.181803\n",
      "Step: 4500 loss: 0.166838\n",
      "Step: 4510 loss: 0.191984\n",
      "Step: 4520 loss: 0.200771\n",
      "Step: 4530 loss: 0.195242\n",
      "Step: 4540 loss: 0.171211\n",
      "Step: 4550 loss: 0.277959\n",
      "Step: 4560 loss: 0.23167\n",
      "Step: 4570 loss: 0.147022\n",
      "Step: 4580 loss: 0.166026\n",
      "Step: 4590 loss: 0.736273\n",
      "Step: 4600 loss: 0.15394\n",
      "Step: 4610 loss: 0.157507\n",
      "Step: 4620 loss: 0.158083\n",
      "Step: 4630 loss: 0.149744\n",
      "Step: 4640 loss: 0.191281\n",
      "Step: 4650 loss: 0.250982\n",
      "Step: 4660 loss: 0.179309\n",
      "Step: 4670 loss: 0.178213\n",
      "Step: 4680 loss: 0.20497\n",
      "Step: 4690 loss: 0.246096\n",
      "Step: 4700 loss: 0.221611\n",
      "Step: 4710 loss: 0.175733\n",
      "Step: 4720 loss: 0.177581\n",
      "Step: 4730 loss: 0.512438\n",
      "Step: 4740 loss: 0.44582\n",
      "Step: 4750 loss: 0.228622\n",
      "Step: 4760 loss: 0.208719\n",
      "Step: 4770 loss: 0.45233\n",
      "Step: 4780 loss: 0.530806\n",
      "Step: 4790 loss: 0.200425\n",
      "Step: 4800 loss: 0.173731\n",
      "Step: 4810 loss: 0.163048\n",
      "Step: 4820 loss: 0.333005\n",
      "Step: 4830 loss: 0.223349\n",
      "Step: 4840 loss: 0.92837\n",
      "Step: 4850 loss: 0.177512\n",
      "Step: 4860 loss: 0.149268\n",
      "Step: 4870 loss: 0.275864\n",
      "Step: 4880 loss: 0.165931\n",
      "Step: 4890 loss: 0.197624\n",
      "Step: 4900 loss: 0.228715\n",
      "Step: 4910 loss: 0.173495\n",
      "Step: 4920 loss: 0.314064\n",
      "Step: 4930 loss: 0.246519\n",
      "Step: 4940 loss: 0.233423\n",
      "Step: 4950 loss: 0.249419\n",
      "Step: 4960 loss: 1.07884\n",
      "Step: 4970 loss: 0.162446\n",
      "Step: 4980 loss: 0.340224\n",
      "Step: 4990 loss: 0.177151\n",
      "Step: 5000 loss: 0.182247\n",
      "Saved Model\n",
      "Step: 5010 loss: 0.14379\n",
      "Step: 5020 loss: 0.161669\n",
      "Step: 5030 loss: 0.176526\n",
      "Step: 5040 loss: 0.151346\n",
      "Step: 5050 loss: 0.228403\n",
      "Step: 5060 loss: 0.153004\n",
      "Step: 5070 loss: 0.20535\n",
      "Step: 5080 loss: 0.173924\n",
      "Step: 5090 loss: 0.154432\n",
      "Step: 5100 loss: 0.219863\n",
      "Step: 5110 loss: 0.162435\n",
      "Step: 5120 loss: 0.165709\n",
      "Step: 5130 loss: 0.188544\n",
      "Step: 5140 loss: 0.174366\n",
      "Step: 5150 loss: 0.249566\n",
      "Step: 5160 loss: 0.349133\n",
      "Step: 5170 loss: 0.406114\n",
      "Step: 5180 loss: 0.166534\n",
      "Step: 5190 loss: 0.147355\n",
      "Step: 5200 loss: 0.295967\n",
      "Step: 5210 loss: 0.161124\n",
      "Step: 5220 loss: 0.171826\n",
      "Step: 5230 loss: 0.14447\n",
      "Step: 5240 loss: 0.182761\n",
      "Step: 5250 loss: 0.172021\n",
      "Step: 5260 loss: 0.240561\n",
      "Step: 5270 loss: 0.291484\n",
      "Step: 5280 loss: 0.198451\n",
      "Step: 5290 loss: 0.190378\n",
      "Step: 5300 loss: 0.229138\n",
      "Step: 5310 loss: 0.149512\n",
      "Step: 5320 loss: 0.14916\n",
      "Step: 5330 loss: 0.202214\n",
      "Step: 5340 loss: 0.218159\n",
      "Step: 5350 loss: 0.181158\n",
      "Step: 5360 loss: 0.289317\n",
      "Step: 5370 loss: 0.146958\n",
      "Step: 5380 loss: 0.240299\n",
      "Step: 5390 loss: 0.622096\n",
      "Step: 5400 loss: 0.219797\n",
      "Step: 5410 loss: 0.147392\n",
      "Step: 5420 loss: 0.145267\n",
      "Step: 5430 loss: 0.15369\n",
      "Step: 5440 loss: 0.252613\n",
      "Step: 5450 loss: 0.165139\n",
      "Step: 5460 loss: 0.162522\n",
      "Step: 5470 loss: 0.145612\n",
      "Step: 5480 loss: 0.179923\n",
      "Step: 5490 loss: 0.207555\n",
      "Step: 5500 loss: 0.144292\n",
      "Step: 5510 loss: 0.177479\n",
      "Step: 5520 loss: 0.142113\n",
      "Step: 5530 loss: 0.354284\n",
      "Step: 5540 loss: 0.17825\n",
      "Step: 5550 loss: 0.170062\n",
      "Step: 5560 loss: 0.145295\n",
      "Step: 5570 loss: 0.294844\n",
      "Step: 5580 loss: 0.421472\n",
      "Step: 5590 loss: 0.206641\n",
      "Step: 5600 loss: 0.146863\n",
      "Step: 5610 loss: 0.165607\n",
      "Step: 5620 loss: 0.30381\n",
      "Step: 5630 loss: 0.157194\n",
      "Step: 5640 loss: 0.610473\n",
      "Step: 5650 loss: 0.152258\n",
      "Step: 5660 loss: 0.155824\n",
      "Step: 5670 loss: 0.166095\n",
      "Step: 5680 loss: 0.139759\n",
      "Step: 5690 loss: 0.157254\n",
      "Step: 5700 loss: 0.1516\n",
      "Step: 5710 loss: 0.407584\n",
      "Step: 5720 loss: 0.159915\n",
      "Step: 5730 loss: 0.137104\n",
      "Step: 5740 loss: 0.136185\n",
      "Step: 5750 loss: 0.143251\n",
      "Step: 5760 loss: 0.898899\n",
      "Step: 5770 loss: 0.259787\n",
      "Step: 5780 loss: 0.293876\n",
      "Step: 5790 loss: 0.239629\n",
      "Step: 5800 loss: 0.149332\n",
      "Step: 5810 loss: 0.14386\n",
      "Step: 5820 loss: 0.275351\n",
      "Step: 5830 loss: 0.150369\n",
      "Step: 5840 loss: 0.188539\n",
      "Step: 5850 loss: 0.140448\n",
      "Step: 5860 loss: 0.151948\n",
      "Step: 5870 loss: 0.180824\n",
      "Step: 5880 loss: 0.154134\n",
      "Step: 5890 loss: 0.148107\n",
      "Step: 5900 loss: 0.136592\n",
      "Step: 5910 loss: 0.267965\n",
      "Step: 5920 loss: 0.154998\n",
      "Step: 5930 loss: 0.20229\n",
      "Step: 5940 loss: 0.544162\n",
      "Step: 5950 loss: 0.290761\n",
      "Step: 5960 loss: 0.158052\n",
      "Step: 5970 loss: 0.330936\n",
      "Step: 5980 loss: 0.134357\n",
      "Step: 5990 loss: 0.153769\n",
      "Step: 6000 loss: 0.261367\n",
      "Saved Model\n",
      "Step: 6010 loss: 0.143776\n",
      "Step: 6020 loss: 0.32329\n",
      "Step: 6030 loss: 0.175063\n",
      "Step: 6040 loss: 0.158035\n",
      "Step: 6050 loss: 0.432002\n",
      "Step: 6060 loss: 0.14176\n",
      "Step: 6070 loss: 0.653227\n",
      "Step: 6080 loss: 0.16196\n",
      "Step: 6090 loss: 0.138223\n",
      "Step: 6100 loss: 0.300651\n",
      "Step: 6110 loss: 0.165797\n",
      "Step: 6120 loss: 0.150779\n",
      "Step: 6130 loss: 0.297885\n",
      "Step: 6140 loss: 0.139815\n",
      "Step: 6150 loss: 0.172257\n",
      "Step: 6160 loss: 0.198737\n",
      "Step: 6170 loss: 0.14645\n",
      "Step: 6180 loss: 0.142521\n",
      "Step: 6190 loss: 0.284453\n",
      "Step: 6200 loss: 0.146891\n",
      "Step: 6210 loss: 0.138808\n",
      "Step: 6220 loss: 0.144\n",
      "Step: 6230 loss: 0.135714\n",
      "Step: 6240 loss: 0.138779\n",
      "Step: 6250 loss: 0.150888\n",
      "Step: 6260 loss: 0.138228\n",
      "Step: 6270 loss: 0.154633\n",
      "Step: 6280 loss: 0.244746\n",
      "Step: 6290 loss: 0.17109\n",
      "Step: 6300 loss: 0.156665\n",
      "Step: 6310 loss: 0.205353\n",
      "Step: 6320 loss: 0.19073\n",
      "Step: 6330 loss: 0.744352\n",
      "Step: 6340 loss: 0.311455\n",
      "Step: 6350 loss: 0.15753\n",
      "Step: 6360 loss: 0.175681\n",
      "Step: 6370 loss: 0.194128\n",
      "Step: 6380 loss: 0.302071\n",
      "Step: 6390 loss: 0.172337\n",
      "Step: 6400 loss: 0.22367\n",
      "Step: 6410 loss: 0.249691\n",
      "Step: 6420 loss: 0.367283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6430 loss: 0.275526\n",
      "Step: 6440 loss: 0.180767\n",
      "Step: 6450 loss: 0.146435\n",
      "Step: 6460 loss: 0.13959\n",
      "Step: 6470 loss: 0.319911\n",
      "Step: 6480 loss: 0.138233\n",
      "Step: 6490 loss: 0.242108\n",
      "Step: 6500 loss: 0.208529\n",
      "Step: 6510 loss: 0.446008\n",
      "Step: 6520 loss: 0.146951\n",
      "Step: 6530 loss: 0.149112\n",
      "Step: 6540 loss: 0.196982\n",
      "Step: 6550 loss: 0.507518\n",
      "Step: 6560 loss: 0.751069\n",
      "Step: 6570 loss: 0.255289\n",
      "Step: 6580 loss: 0.548136\n",
      "Step: 6590 loss: 0.149067\n",
      "Step: 6600 loss: 0.248962\n",
      "Step: 6610 loss: 0.137674\n",
      "Step: 6620 loss: 0.153976\n",
      "Step: 6630 loss: 0.26702\n",
      "Step: 6640 loss: 0.137183\n",
      "Step: 6650 loss: 0.417848\n",
      "Step: 6660 loss: 0.248134\n",
      "Step: 6670 loss: 0.145402\n",
      "Step: 6680 loss: 0.157638\n",
      "Step: 6690 loss: 0.177348\n",
      "Step: 6700 loss: 0.888012\n",
      "Step: 6710 loss: 0.146005\n",
      "Step: 6720 loss: 0.14551\n",
      "Step: 6730 loss: 0.179755\n",
      "Step: 6740 loss: 0.142292\n",
      "Step: 6750 loss: 0.14972\n",
      "Step: 6760 loss: 0.135946\n",
      "Step: 6770 loss: 0.15104\n",
      "Step: 6780 loss: 0.148572\n",
      "Step: 6790 loss: 0.251365\n",
      "Step: 6800 loss: 0.233396\n",
      "Step: 6810 loss: 0.15848\n",
      "Step: 6820 loss: 0.157523\n",
      "Step: 6830 loss: 0.160844\n",
      "Step: 6840 loss: 0.164019\n",
      "Step: 6850 loss: 0.170586\n",
      "Step: 6860 loss: 0.313192\n",
      "Step: 6870 loss: 0.2368\n",
      "Step: 6880 loss: 0.141667\n",
      "Step: 6890 loss: 0.140095\n",
      "Step: 6900 loss: 0.140965\n",
      "Step: 6910 loss: 0.154795\n",
      "Step: 6920 loss: 0.136099\n",
      "Step: 6930 loss: 0.306557\n",
      "Step: 6940 loss: 0.139028\n",
      "Step: 6950 loss: 0.157793\n",
      "Step: 6960 loss: 0.144633\n",
      "Step: 6970 loss: 0.135286\n",
      "Step: 6980 loss: 0.295813\n",
      "Step: 6990 loss: 0.23349\n",
      "Step: 7000 loss: 0.135728\n",
      "Saved Model\n",
      "Step: 7010 loss: 0.197021\n",
      "Step: 7020 loss: 0.141452\n",
      "Step: 7030 loss: 0.139649\n",
      "Step: 7040 loss: 0.146197\n",
      "Step: 7050 loss: 0.15974\n",
      "Step: 7060 loss: 0.220565\n",
      "Step: 7070 loss: 0.175818\n",
      "Step: 7080 loss: 0.146853\n",
      "Step: 7090 loss: 0.162165\n",
      "Step: 7100 loss: 0.13872\n",
      "Step: 7110 loss: 0.63201\n",
      "Step: 7120 loss: 0.134299\n",
      "Step: 7130 loss: 0.465611\n",
      "Step: 7140 loss: 0.15031\n",
      "Step: 7150 loss: 0.14053\n",
      "Step: 7160 loss: 0.267693\n",
      "Step: 7170 loss: 0.208275\n",
      "Step: 7180 loss: 0.169673\n",
      "Step: 7190 loss: 0.145978\n",
      "Step: 7200 loss: 0.164668\n",
      "Step: 7210 loss: 0.144766\n",
      "Step: 7220 loss: 0.215961\n",
      "Step: 7230 loss: 0.136929\n",
      "Step: 7240 loss: 0.31528\n",
      "Step: 7250 loss: 0.146374\n",
      "Step: 7260 loss: 0.274633\n",
      "Step: 7270 loss: 0.193744\n",
      "Step: 7280 loss: 0.162372\n",
      "Step: 7290 loss: 0.341213\n",
      "Step: 7300 loss: 0.147609\n",
      "Step: 7310 loss: 0.140568\n",
      "Step: 7320 loss: 0.259324\n",
      "Step: 7330 loss: 0.134259\n",
      "Step: 7340 loss: 0.146886\n",
      "Step: 7350 loss: 0.148818\n",
      "Step: 7360 loss: 0.227027\n",
      "Step: 7370 loss: 0.142942\n",
      "Step: 7380 loss: 0.273472\n",
      "Step: 7390 loss: 0.175567\n",
      "Step: 7400 loss: 0.142797\n",
      "Step: 7410 loss: 0.255349\n",
      "Step: 7420 loss: 0.137873\n",
      "Step: 7430 loss: 0.155867\n",
      "Step: 7440 loss: 0.182714\n",
      "Step: 7450 loss: 0.151909\n",
      "Step: 7460 loss: 0.29509\n",
      "Step: 7470 loss: 0.200412\n",
      "Step: 7480 loss: 0.177453\n",
      "Step: 7490 loss: 0.139139\n",
      "Step: 7500 loss: 0.133714\n",
      "Step: 7510 loss: 0.135122\n",
      "Step: 7520 loss: 0.139671\n",
      "Step: 7530 loss: 0.290354\n",
      "Step: 7540 loss: 0.304471\n",
      "Step: 7550 loss: 0.169638\n",
      "Step: 7560 loss: 0.198371\n",
      "Step: 7570 loss: 0.15415\n",
      "Step: 7580 loss: 0.245703\n",
      "Step: 7590 loss: 0.198805\n",
      "Step: 7600 loss: 0.15464\n",
      "Step: 7610 loss: 0.176052\n",
      "Step: 7620 loss: 0.151273\n",
      "Step: 7630 loss: 0.150613\n",
      "Step: 7640 loss: 0.137228\n",
      "Step: 7650 loss: 0.95766\n",
      "Step: 7660 loss: 0.136094\n",
      "Step: 7670 loss: 0.4591\n",
      "Step: 7680 loss: 0.272267\n",
      "Step: 7690 loss: 0.14206\n",
      "Step: 7700 loss: 0.348479\n",
      "Step: 7710 loss: 0.342617\n",
      "Step: 7720 loss: 0.131903\n",
      "Step: 7730 loss: 0.145675\n",
      "Step: 7740 loss: 0.168045\n",
      "Step: 7750 loss: 0.18932\n",
      "Step: 7760 loss: 0.213672\n",
      "Step: 7770 loss: 0.301973\n",
      "Step: 7780 loss: 0.136863\n",
      "Step: 7790 loss: 0.323414\n",
      "Step: 7800 loss: 0.135034\n",
      "Step: 7810 loss: 0.142339\n",
      "Step: 7820 loss: 0.157226\n",
      "Step: 7830 loss: 0.424516\n",
      "Step: 7840 loss: 0.141623\n",
      "Step: 7850 loss: 0.162707\n",
      "Step: 7860 loss: 0.139837\n",
      "Step: 7870 loss: 0.136708\n",
      "Step: 7880 loss: 0.22923\n",
      "Step: 7890 loss: 0.157115\n",
      "Step: 7900 loss: 0.166574\n",
      "Step: 7910 loss: 0.136849\n",
      "Step: 7920 loss: 0.271222\n",
      "Step: 7930 loss: 0.272432\n",
      "Step: 7940 loss: 0.350514\n",
      "Step: 7950 loss: 0.213564\n",
      "Step: 7960 loss: 0.162709\n",
      "Step: 7970 loss: 0.142014\n",
      "Step: 7980 loss: 0.283705\n",
      "Step: 7990 loss: 0.187434\n",
      "Step: 8000 loss: 0.166995\n",
      "Saved Model\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for i in range(num_epochs):\n",
    "    for j in range(num_batches):\n",
    "        path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "        word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "        \n",
    "        feed_dict = {\n",
    "            path_length:path_dict,\n",
    "            word_ids:word_dict,\n",
    "            pos_ids:pos_dict,\n",
    "            dep_ids:dep_dict,\n",
    "            y:y_dict}\n",
    "        _, loss, step = sess.run([optimizer, total_loss, global_step], feed_dict)\n",
    "        if step % 10 ==0:\n",
    "            print(\"Step:\", step, \"loss:\",loss)\n",
    "        if step % 1000 == 0:\n",
    "            saver.save(sess, model_dir + '/model', global_step = step)\n",
    "            print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 94.175\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "all_predictions = []\n",
    "for j in range(num_batches):\n",
    "    path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "    word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "\n",
    "    feed_dict = {\n",
    "        path_length:path_dict,\n",
    "        word_ids:word_dict,\n",
    "        pos_ids:pos_dict,\n",
    "        dep_ids:dep_dict,\n",
    "        y:y_dict}\n",
    "    batch_predictions = sess.run(predictions, feed_dict)\n",
    "    all_predictions.append(batch_predictions)\n",
    "count = 0\n",
    "for i in range(800):\n",
    "    count += sum(all_predictions[i] == rel_ids[i*10:(i+1)* 10])\n",
    "print(\"accuracy\", count/8000* 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

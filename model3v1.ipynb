{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, _pickle as pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "data_dir = 'data'\n",
    "ckpt_dir = 'checkpoint'\n",
    "word_embd_dir = 'checkpoint/word_embd'\n",
    "model_dir = 'checkpoint/model2v2'\n",
    "\n",
    "word_embd_dim = 100\n",
    "pos_embd_dim = 25\n",
    "dep_embd_dim = 25\n",
    "word_vocab_size = 400001\n",
    "pos_vocab_size = 10\n",
    "dep_vocab_size = 21\n",
    "relation_classes = 19\n",
    "state_size = 100\n",
    "batch_size = 10\n",
    "channels = 3\n",
    "lambda_l2 = 0.0001\n",
    "max_len_path = 70\n",
    "starter_learning_rate = 0.001\n",
    "decay_steps = 2000\n",
    "decay_rate = 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"input\"):\n",
    "    fp_length = tf.placeholder(tf.int32, shape=[batch_size], name=\"fp_ength\")\n",
    "    fp = tf.placeholder(tf.int32, [2, batch_size, max_len_path], name=\"full_path\")\n",
    "    sp_length = tf.placeholder(tf.int32, shape=[batch_size, 2], name=\"sp_length\")\n",
    "    sp = tf.placeholder(tf.int32, [batch_size, 2, None], name=\"shortest_path\")\n",
    "    sp_pos = tf.placeholder(tf.int32, [batch_size, 2, None], name=\"sp_pos\")\n",
    "    sp_childs = tf.placeholder(tf.int32, [batch_size, 2, None, None], name=\"sp_childs\")\n",
    "    relation = tf.placeholder(tf.int32, [batch_size], name=\"relation\")\n",
    "\n",
    "with tf.name_scope(\"word_embedding\"):\n",
    "    W = tf.Variable(tf.constant(0.0, shape=[word_vocab_size, word_embd_dim]), name=\"W\")\n",
    "    embedding_placeholder = tf.placeholder(tf.float32,[word_vocab_size, word_embd_dim])\n",
    "    embedding_init = W.assign(embedding_placeholder)\n",
    "    embd_fp_word = tf.nn.embedding_lookup(W,fp[0])\n",
    "    word_embedding_saver = tf.train.Saver({\"word_embedding/W\": W})\n",
    "\n",
    "with tf.name_scope(\"pos_embedding\"):\n",
    "    W = tf.Variable(tf.random_uniform([pos_vocab_size, pos_embd_dim]), name=\"W\")\n",
    "    embd_fp_pos = tf.nn.embedding_lookup(W, fp[1])\n",
    "    pos_embedding_saver = tf.train.Saver({\"pos_embedding/W\": W})\n",
    "\n",
    "with tf.name_scope(\"dep_embedding\"):\n",
    "    W = tf.Variable(tf.random_uniform([dep_vocab_size, dep_embd_dim]), name=\"W\")\n",
    "    embd_sp = tf.nn.embedding_lookup(W, sp)\n",
    "    dep_embedding_saver = tf.train.Saver({\"dep_embedding/W\": W})\n",
    "    \n",
    "embd_fp = tf.concat([embd_fp_word, embd_fp_pos], axis=2)\n",
    "embd_fp_rev = tf.reverse(embd_fp, [1])\n",
    "fp_length_rev = tf.reverse(fp_length, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"lstm_fw\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(state_size)\n",
    "    states_fw, _ = tf.nn.dynamic_rnn(cell, embd_fp, sequence_length=fp_length, dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope(\"lstm_bw\"):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(state_size)\n",
    "    states, _ = tf.nn.dynamic_rnn(cell, embd_fp_rev, sequence_length=fp_length_rev, dtype=tf.float32)\n",
    "    states_bw = tf.reverse(states, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hidden_states_seq = tf.concat([states_fw, states_bw], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell_states_seq = tf.concat([states_fw, states_bw], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree_input_size = state_size * 2 + dep_embd_dim\n",
    "init_const = tf.zeros([1, state_size])\n",
    "\n",
    "# Tree LSTM bottom-up\n",
    "with tf.variable_scope(\"lstm_tree_btup\"):\n",
    "    W_i = tf.get_variable(\"W_i\", shape=[tree_input_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    U_i = tf.get_variable(\"U_i\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_i = tf.get_variable(\"b_i\", initializer=init_const)\n",
    "    \n",
    "    W_f = tf.get_variable(\"W_f\", shape=[tree_input_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    U_f = tf.get_variable(\"U_f\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_f = tf.get_variable(\"b_f\", initializer=init_const)\n",
    "    \n",
    "    W_o = tf.get_variable(\"W_o\", shape=[tree_input_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    U_o = tf.get_variable(\"U_o\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_o = tf.get_variable(\"b_o\", initializer=init_const)\n",
    "    \n",
    "    U_ft = tf.get_variable(\"U_ft\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# Tree LSTM top-down\n",
    "with tf.variable_scope(\"lstm_tree_tpdw\"):\n",
    "    W_i = tf.get_variable(\"W_i\", shape=[tree_input_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    U_i = tf.get_variable(\"U_i\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_i = tf.get_variable(\"b_i\", initializer=init_const)\n",
    "    U_it = tf.get_variable(\"U_it\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    W_f = tf.get_variable(\"W_f\", shape=[tree_input_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    U_f = tf.get_variable(\"U_f\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_f = tf.get_variable(\"b_f\", initializer=init_const)\n",
    "    U_ft = tf.get_variable(\"U_ft\", shape=[max_len_path, state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    W_o = tf.get_variable(\"W_o\", shape=[tree_input_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    U_o = tf.get_variable(\"U_o\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_o = tf.get_variable(\"b_o\", initializer=init_const)\n",
    "    U_ot = tf.get_variable(\"U_ot\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    W_u = tf.get_variable(\"W_u\", shape=[tree_input_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    U_u = tf.get_variable(\"U_u\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_u = tf.get_variable(\"b_u\", initializer=init_const)\n",
    "    U_ut = tf.get_variable(\"U_ut\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cond(i, steps, *agrs):\n",
    "    return i< steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_btup = tf.zeros([1, 1, state_size])\n",
    "cell_states_btup = tf.zeros([1, 1, state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_state_tree = tf.expand_dims(tf.zeros([1, state_size]), 0)\n",
    "cell_state_tree = tf.expand_dims(tf.zeros([1, state_size]), 0)\n",
    "num_child_sp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body1(index, steps, hds, cds, hds_btup, cds_btup):\n",
    "    i = tf.constant(0)\n",
    "    inputs = tf.expand_dims(tf.concat([tf.gather(hidden_states_seq[0], sp_pos[0][0][index]), embd_sp[0][0][index]],0), 0)\n",
    "    childs = sp_childs[0][0][index]\n",
    "    num_child = tf.shape(childs)[0]\n",
    "\n",
    "    ht = tf.expand_dims(states_fw[0], 1)\n",
    "    ct = tf.expand_dims(states_fw[0], 1)\n",
    "    \n",
    "    it = tf.matmul(inputs, W_i) + b_i + tf.matmul(hds[0], U_i)\n",
    "    ft = tf.matmul(inputs, W_f) + b_f + tf.matmul(hds[0], U_f)\n",
    "    ot = tf.matmul(inputs, W_o) + b_o + tf.matmul(hds[0], U_o)\n",
    "    ut = tf.matmul(inputs, W_u) + b_u + tf.matmul(hds[0], U_u)\n",
    "    \n",
    "    def body4(k, steps, it, ft, ot, ut):\n",
    "        it += tf.matmul(hds[k], U_i)\n",
    "        ft += tf.matmul(hds[k], U_f) \n",
    "        ot += tf.matmul(hds[k], U_o) \n",
    "        ut += tf.matmul(hds[k], U_u) \n",
    "        return k+1, steps, it, ft, ot, ut\n",
    "    _, _, it, ft, ot, ut = tf.while_loop(cond, body4, [1, num_child_sp, it, ft, ot, ut])\n",
    "\n",
    "    \n",
    "    def body(k, steps, out, U):\n",
    "        out += tf.matmul(tf.gather(ht, childs[k]), U)\n",
    "        return k+1, steps, out, U\n",
    "\n",
    "    _, _, ht_i, _ = tf.while_loop(cond, body, [i, num_child, it, U_it])\n",
    "    _, _, ht_o, _ = tf.while_loop(cond, body, [i, num_child, ot, U_ot])\n",
    "    _, _, ht_u, _ = tf.while_loop(cond, body, [i, num_child, ut, U_ut])\n",
    "\n",
    "    input_gate = tf.sigmoid(ht_i)\n",
    "    output_gate = tf.sigmoid(ht_o)\n",
    "    u_input = tf.sigmoid(ht_u)\n",
    "    \n",
    "    cell_state = input_gate * u_input \n",
    "    \n",
    "    def body5(k, steps, cell_state):\n",
    "        _, _, f, _ = tf.while_loop(cond, body, [i, num_child, ft, U_ft[k]])\n",
    "        cell_state += tf.sigmoid(f) * cds[k]\n",
    "        return k+1, steps, cell_state\n",
    "    _, _, cell_state = tf.while_loop(cond, body5, [i, num_child_sp, cell_state])\n",
    "    def body2(k, steps, ctl):\n",
    "        _, _, fj, _ = tf.while_loop(cond, body, [i, num_child, ft, U_ft[k+2]])\n",
    "        ctl += tf.sigmoid(fj) * tf.gather(ct, childs[k])\n",
    "        return k+1, steps, ctl\n",
    "\n",
    "    _, _, cds = tf.while_loop(cond, body2, [i, num_child, cell_state])\n",
    "    \n",
    "    hds = tf.expand_dims(output_gate * tf.tanh(cds), 0)\n",
    "    \n",
    "    cds = tf.expand_dims(cds, 0)\n",
    "    if(index==0):\n",
    "        hds_btup = hds\n",
    "        cds_btup = cds\n",
    "    else:\n",
    "        hds_btup = tf.concat([hds_btup, hds], 0)\n",
    "        cds_btup = tf.concat([cds_btup, cds], 0)\n",
    "    return index+1, steps, hds, cds, hds_btup, cds_btup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index, _, _, _, hidden_states_btup, cell_states_btup = tf.while_loop(\n",
    "    cond, body1, \n",
    "    [i, sp_length[0][0], hidden_state_tree, cell_state_tree, \n",
    "     hidden_states_btup, cell_states_btup], \n",
    "    shape_invariants=[i.get_shape(),i.get_shape(), \n",
    "    hidden_state_tree.get_shape(), hidden_state_tree.get_shape(), \n",
    "    tf.TensorShape([None, 1, state_size]), \n",
    "    tf.TensorShape([None, 1, state_size])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'while_14/Exit_4:0' shape=(?, 1, 100) dtype=float32>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states_btup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"hidden_layer\"):\n",
    "    W = tf.Variable(tf.truncated_normal([600, 100], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([100]), name=\"b\")\n",
    "    y_hidden_layer = tf.matmul(state_series, W) + b\n",
    "\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    y_hidden_layer_drop = tf.nn.dropout(y_hidden_layer, 0.3)\n",
    "\n",
    "with tf.name_scope(\"softmax_layer\"):\n",
    "    W = tf.Variable(tf.truncated_normal([100, relation_classes], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([relation_classes]), name=\"b\")\n",
    "    logits = tf.matmul(y_hidden_layer_drop, W) + b\n",
    "    predictions = tf.argmax(logits, 1)\n",
    "\n",
    "tv_all = tf.trainable_variables()\n",
    "tv_regu = []\n",
    "non_reg = [\"word_embedding/W:0\",\"pos_embedding/W:0\",'dep_embedding/W:0',\"global_step:0\",'hidden_layer/b:0','softmax_layer/b:0']\n",
    "for t in tv_all:\n",
    "    if t.name not in non_reg:\n",
    "        if(t.name.find('biases')==-1):\n",
    "            tv_regu.append(t)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    l2_loss = lambda_l2 * tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv_regu ])\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    total_loss = loss + l2_loss\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(total_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('data/vocab.pkl', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "word2id[unknown_token] = word_vocab_size -1\n",
    "id2word[word_vocab_size-1] = unknown_token\n",
    "\n",
    "pos_tags_vocab = []\n",
    "for line in open('data/pos_tags.txt'):\n",
    "        pos_tags_vocab.append(line.strip())\n",
    "\n",
    "dep_vocab = []\n",
    "for line in open('data/dependency_types.txt'):\n",
    "    dep_vocab.append(line.strip())\n",
    "\n",
    "relation_vocab = []\n",
    "for line in open('data/relation_types.txt'):\n",
    "    relation_vocab.append(line.strip())\n",
    "\n",
    "rel2id = dict((w, i) for i,w in enumerate(relation_vocab))\n",
    "id2rel = dict((i, w) for i,w in enumerate(relation_vocab))\n",
    "\n",
    "pos_tag2id = dict((w, i) for i,w in enumerate(pos_tags_vocab))\n",
    "id2pos_tag = dict((i, w) for i,w in enumerate(pos_tags_vocab))\n",
    "\n",
    "dep2id = dict((w, i) for i,w in enumerate(dep_vocab))\n",
    "id2dep = dict((i, w) for i,w in enumerate(dep_vocab))\n",
    "\n",
    "pos_tag2id['OTH'] = 9\n",
    "id2pos_tag[9] = 'OTH'\n",
    "\n",
    "dep2id['OTH'] = 20\n",
    "id2dep[20] = 'OTH'\n",
    "\n",
    "JJ_pos_tags = ['JJ', 'JJR', 'JJS']\n",
    "NN_pos_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "RB_pos_tags = ['RB', 'RBR', 'RBS']\n",
    "PRP_pos_tags = ['PRP', 'PRP$']\n",
    "VB_pos_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "_pos_tags = ['CC', 'CD', 'DT', 'IN']\n",
    "\n",
    "def pos_tag(x):\n",
    "    if x in JJ_pos_tags:\n",
    "        return pos_tag2id['JJ']\n",
    "    if x in NN_pos_tags:\n",
    "        return pos_tag2id['NN']\n",
    "    if x in RB_pos_tags:\n",
    "        return pos_tag2id['RB']\n",
    "    if x in PRP_pos_tags:\n",
    "        return pos_tag2id['PRP']\n",
    "    if x in VB_pos_tags:\n",
    "        return pos_tag2id['VB']\n",
    "    if x in _pos_tags:\n",
    "        return pos_tag2id[x]\n",
    "    else:\n",
    "        return 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f = open('data/word_embedding', 'rb')\n",
    "# word_embedding = pickle.load(f)\n",
    "# f.close()\n",
    "# sess.run(embedding_init, feed_dict={embedding_placeholder:word_embedding})\n",
    "# word_embedding_saver.save(sess, word_embd_dir + '/word_embd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = tf.train.latest_checkpoint(model_dir)\n",
    "# saver.restore(sess, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint/word_embd/word_embd\n"
     ]
    }
   ],
   "source": [
    "latest_embd = tf.train.latest_checkpoint(word_embd_dir)\n",
    "word_embedding_saver.restore(sess, latest_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('data/train_lca_paths', 'rb')\n",
    "word_p, dep_p, pos_p = pickle.load(f)\n",
    "f.close()\n",
    "relations = []\n",
    "for line in open('data/train_relations.txt'):\n",
    "    relations.append(line.strip().split()[1])\n",
    "\n",
    "length = len(word_p)\n",
    "num_batches = int(length/batch_size)\n",
    "\n",
    "for i in range(length):\n",
    "    for j, word in enumerate(word_p[i]):\n",
    "        word = word.lower()\n",
    "        word_p[i][j] = word if word in word2id else unknown_token \n",
    "    for l, d in enumerate(dep_p[i]):\n",
    "        dep_p[i][l] = d if d in dep2id else 'OTH'\n",
    "        \n",
    "word_p_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p_ids = np.ones([length, max_len_path],dtype=int)\n",
    "rel_ids = np.array([rel2id[rel] for rel in relations])\n",
    "path_len = np.array([len(w) for w in word_p], dtype=int)\n",
    "\n",
    "for i in range(length):\n",
    "    for j, w in enumerate(word_p[i]):\n",
    "        word_p_ids[i][j] = word2id[w]\n",
    "        \n",
    "    for j, w in enumerate(pos_p[i]):\n",
    "        pos_p_ids[i][j] = pos_tag(w)\n",
    "        \n",
    "    for j, w in enumerate(dep_p[i]):\n",
    "        dep_p_ids[i][j] = dep2id[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 800 loss: 2.85300489247\n",
      "Saved Model\n",
      "Epoch: 2 Step: 1600 loss: 2.73827668965\n",
      "Saved Model\n",
      "Epoch: 3 Step: 2400 loss: 2.70001435518\n",
      "Saved Model\n",
      "Epoch: 4 Step: 3200 loss: 2.68624746531\n",
      "Saved Model\n",
      "Epoch: 5 Step: 4000 loss: 2.68042603165\n",
      "Saved Model\n",
      "Epoch: 6 Step: 4800 loss: 2.67750604913\n",
      "Saved Model\n",
      "Epoch: 7 Step: 5600 loss: 2.67583220631\n",
      "Saved Model\n",
      "Epoch: 8 Step: 6400 loss: 2.67482194766\n",
      "Saved Model\n",
      "Epoch: 9 Step: 7200 loss: 2.67411908716\n",
      "Saved Model\n",
      "Epoch: 10 Step: 8000 loss: 2.67369878128\n",
      "Saved Model\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for i in range(num_epochs):\n",
    "    loss_per_epoch = 0\n",
    "    for j in range(num_batches):\n",
    "        feed_dict = {\n",
    "            path_length:path_len[j*batch_size:(j+1)*batch_size],\n",
    "            word_ids:word_p_ids[j*batch_size:(j+1)*batch_size],\n",
    "            pos_ids:pos_p_ids[j*batch_size:(j+1)*batch_size],\n",
    "            dep_ids:dep_p_ids[j*batch_size:(j+1)*batch_size],\n",
    "            y:rel_ids[j*batch_size:(j+1)*batch_size]}\n",
    "        _, _loss, step = sess.run([optimizer, total_loss, global_step], feed_dict)\n",
    "        loss_per_epoch +=_loss\n",
    "        if (j+1)%num_batches==0:\n",
    "            print(\"Epoch:\", i+1,\"Step:\", step, \"loss:\",loss_per_epoch/num_batches)\n",
    "    saver.save(sess, model_dir + '/model')\n",
    "    print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training accuracy\n",
    "all_predictions = []\n",
    "for j in range(num_batches):\n",
    "     feed_dict = {\n",
    "            path_length:path_len[j*batch_size:(j+1)*batch_size],\n",
    "            word_ids:word_p_ids[j*batch_size:(j+1)*batch_size],\n",
    "            pos_ids:pos_p_ids[j*batch_size:(j+1)*batch_size],\n",
    "            dep_ids:dep_p_ids[j*batch_size:(j+1)*batch_size],\n",
    "            y:rel_ids[j*batch_size:(j+1)*batch_size]}\n",
    "    batch_predictions = sess.run(predictions, feed_dict)\n",
    "    all_predictions.append(batch_predictions)\n",
    "\n",
    "y_pred = []\n",
    "for i in range(num_batches):\n",
    "    for pred in all_predictions[i]:\n",
    "        y_pred.append(pred)\n",
    "\n",
    "count = 0\n",
    "for i in range(batch_size*num_batches):\n",
    "    count += y_pred[i]==rel_ids[i]\n",
    "accuracy = count/(batch_size*num_batches) * 100\n",
    "\n",
    "print(\"training accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (2, 10) for Tensor 'input_1/path1_length:0', which has shape '(10,)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d81df71599e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdep_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdep_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         y:y_dict}\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mbatch_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mall_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shanu/virtualenv/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    962\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (2, 10) for Tensor 'input_1/path1_length:0', which has shape '(10,)'"
     ]
    }
   ],
   "source": [
    "f = open('data/test_lca_paths', 'rb')\n",
    "word_p, dep_p, pos_p = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "relations = []\n",
    "for line in open('data/test_relations.txt'):\n",
    "    relations.append(line.strip().split()[0])\n",
    "\n",
    "length = len(word_p1)\n",
    "num_batches = int(length/batch_size)\n",
    "\n",
    "for i in range(length):\n",
    "    for j, word in enumerate(word_p[i]):\n",
    "        word = word.lower()\n",
    "        word_p[i][j] = word if word in word2id else unknown_token \n",
    "    for l, d in enumerate(dep_p[i]):\n",
    "        dep_p[i][l] = d if d in dep2id else 'OTH'\n",
    "        \n",
    "word_p_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p_ids = np.ones([length, max_len_path],dtype=int)\n",
    "rel_ids = np.array([rel2id[rel] for rel in relations])\n",
    "path_len = np.array([len(w) for w in word_p], dtype=int)\n",
    "\n",
    "for i in range(length):\n",
    "    for j, w in enumerate(word_p[i]):\n",
    "        word_p_ids[i][j] = word2id[w]\n",
    "        \n",
    "    for j, w in enumerate(pos_p[i]):\n",
    "        pos_p_ids[i][j] = pos_tag(w)\n",
    "        \n",
    "    for j, w in enumerate(dep_p[i]):\n",
    "        dep_p_ids[i][j] = dep2id[w]\n",
    "\n",
    "# test predictions\n",
    "all_predictions = []\n",
    "for j in range(num_batches):\n",
    "     feed_dict = {\n",
    "            path_length:path_len[j*batch_size:(j+1)*batch_size],\n",
    "            word_ids:word_p_ids[j*batch_size:(j+1)*batch_size],\n",
    "            pos_ids:pos_p_ids[j*batch_size:(j+1)*batch_size],\n",
    "            dep_ids:dep_p_ids[j*batch_size:(j+1)*batch_size],\n",
    "            y:rel_ids[j*batch_size:(j+1)*batch_size]}\n",
    "    batch_predictions = sess.run(predictions, feed_dict)\n",
    "    all_predictions.append(batch_predictions)\n",
    "\n",
    "y_pred = []\n",
    "for i in range(num_batches):\n",
    "    for pred in all_predictions[i]:\n",
    "        y_pred.append(pred)\n",
    "\n",
    "count = 0\n",
    "for i in range(batch_size*num_batches):\n",
    "    count += y_pred[i]==rel_ids[i]\n",
    "accuracy = count/(batch_size*num_batches) * 100\n",
    "\n",
    "print(\"test accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
